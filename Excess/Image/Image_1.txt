Image and Vision Computing 40 (2015) 1–15

Contents lists available at ScienceDirect

Image and Vision Computing
journal homepage: www.elsevier.com/locate/imavis

Effects of texture addition on optical ﬂow performance in images with
poor texture☆
Mehran Andalibi ⁎, Lawrence.L. Hoberock, Hossein Mohamadipanah
Advanced Technology Research Center, Oklahoma State University, Stillwater, OK 74078, USA

a r t i c l e

i n f o

Article history:
Received 5 May 2014
Received in revised form 15 March 2015
Accepted 28 April 2015
Available online 27 May 2015
Keywords:
Optical ﬂow
Poor texture
Foreground detection
Laws' masks
F-measure
Boundary displacement error
Condition number

a b s t r a c t
This paper investigates the effects of adding texture to images with poorly-textured regions on optical ﬂow performance, namely the accuracy of foreground boundary detection and computation time. Despite signiﬁcant improvements in optical ﬂow computations, poor texture still remains a challenge to even the most accurate
methods. Accordingly, we explored the effects of simple modiﬁcation of images, rather than the algorithms. To
localize and add texture to poorly-textured regions in the background, which induce the propagation of foreground optical ﬂow, we ﬁrst perform a texture segmentation using Laws' masks and generate a texture map.
Next, using a binary frame difference, we constrain the poorly-textured regions to those with negligible motion.
Finally, we calculate the optical ﬂow for the modiﬁed images with added texture using the best optical ﬂow
methods available. It is shown that if the threshold used for binarizing the frame difference is in a speciﬁc
range determined empirically, variations in the ﬁnal foreground detection will be insigniﬁcant. Employing the
texture addition in conjunction with leading optical ﬂow methods on multiple real and animation sequences
with different texture distributions revealed considerable advantages, including improvement in the accuracy
of foreground boundary preservation, prevention of object merging, and reduction in the computation time.
The F-measure and the Boundary Displacement Error metrics were used to evaluate the similarity between detected and ground-truth foreground masks. Furthermore, preventing foreground optical ﬂow propagation and
reduction in the computation time are discussed using analysis of optical ﬂow convergence.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
Accurate optical ﬂow computation is crucial in many computer vision tasks, including motion estimation, object detection, and tracking.
Three decades after the seminal contribution by Horn and Schunck [1],
accuracy of optical ﬂow computation methods have been improved
signiﬁcantly. However, images with poor texture, especially in the background, which occur in many sequences, still remain a major challenge
in this ﬁeld [2]. Since solving for optical ﬂow components using the optical ﬂow constraint is an ill-posed problem with two unknowns and
one equation, there is a need for extra constraint(s). Spatial smoothness
of optical ﬂow components introduced by Horn and Schunck (HS) is one
of the most common constraints used in different publications with various modiﬁcations, such as in [3–6]. The smoothness constraint causes
the blurring of computed motion at the object boundaries, together
with spread of foreground non-zero ﬂow to the neighboring background pixels.

☆ This paper has been recommended for acceptance by Sinisa Todorovic.
⁎ Corresponding author.
E-mail addresses: mehran.andalibi@okstate.edu (M. Andalibi),
larry.hoberock@okstate.edu (L.L. Hoberock), hossein.mohamadipanah@okstate.edu
(H. Mohamadipanah).

http://dx.doi.org/10.1016/j.imavis.2015.04.008
0262-8856/© 2015 Elsevier B.V. All rights reserved.

As we will see in Section 2.1, while making optical ﬂow computation
possible, in images with poorly-textured regions, the smoothness constraint leads to some disadvantages, such as considerable deformations
in the size and the shape of the detected foreground objects, and accordingly in the position of the center area, which results in errors for foreground diagnosis and tracking. This is shown in the ﬁrst row of Fig. 1
for a sequence, where a wooden model (only the upper body) and its
cast shadows are moving against a background with poor texture. The
ﬁrst and second frames are shown in parts (a) and (b), respectively;
the magnitude of optical ﬂow calculated by the method in [4] is
shown in part (c), where propagation of the object ﬂow to the neighboring background pixels with poor texture has deformed the object shape
and lead to difﬁculty in foreground detection. In images with multiple
moving objects within a small region, smoothness of optical ﬂow can
lead to objects merging. This is illustrated in the second row of Fig. 1,
where multiple cars with cast shadows are moving close to each other
on a highway with insufﬁcient texture. The ﬁrst and second frames
are shown in parts (d) and (e), respectively; the magnitude of optical
ﬂow calculated by the method in [6] is shown in part (f), where object
merging is observable. The other negative effect of computing optical
ﬂow for poorly-textured regions is the considerable computation time
due to solving the time-consuming Laplace equation with boundary
conditions.

2

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

Fig. 1. Negative effects of the smoothness constraint on images with poorly-textured regions: First frame of the wooden model sequence (a); second frame of the wooden model sequence
(b); optical ﬂow magnitude computed according to [4] where the object shape is distorted (c); ﬁrst frame of the highway sequence (d); second frame of the highway Sequence (e); optical
ﬂow magnitude computed according to [6] where object merging has occurred (f).

Researchers have attempted to overcome the negative effects of the
smoothness term following the HS contribution. Nagel and Enkelmann
[7] employed oriented derivatives for the smoothness term, observing
that the motion boundaries coincide with the abrupt light intensity
transitions. Using heuristically determined smoothness across and
along the object boundaries, Alvarez et al. [8] proposed a modiﬁcation
for improving the method by Nagel and Enkelmann. A manuallydesigned probabilistic model using Markov Random Field (MRF) and
a statistical model using patch-based motion discontinuity were
used to relate the light intensity edges and motion boundaries by
Black [9] and Fleet et al. [10], respectively. Lei et al. [11] adopted a
variable weight for the effectiveness of the smoothness term in the HS
formulation. The variable weight coefﬁcient is adaptive through a
threshold function based on the detection of the gray boundaries and
on the real-time detection of the movement boundaries in the iterative
process. The method by Nir et al. [12] solves for six afﬁne parameters at
each pixel position instead of two ﬂow components. Sun as well as
Werlberger et al. [13,2] modiﬁed the total energy function by adding
non-local smoothness terms that employ adaptive weights for each

pixel, which is basically equivalent to using median ﬁltering after
every warping step.
The approach of anisotropic weighting of the smoothness term is a
breakthrough employed recently, including substitution of the standard
quadratic penalizing function by the anisotropic Huber-L1 Norm, ﬁrst
introduced in [14] and used in [15] and [16], applying smaller weights
along the intensity boundaries compared to the orthogonal direction
in [2]. A similar approach was proposed by Zimmer et al. [17] in which
the brightness constancy is used to determine the weights rather than
the intensity gradient. Harmonic constraint has been imposed on the
isotropic gradient vector ﬁeld to create the anisotropic diffusion in
[18] and [19], where the authors utilized divergence and curl of the vector ﬁeld. Aubert et al. [20] added an extra term, which penalizes computing motion in homogeneous blocks and only allows for large
values of optical ﬂow components in textured regions. Divergence controls the amount of diffusion, and the curl term controls the diffusion
direction.
Despite signiﬁcant improvements in the suppression of motion blurring at the object boundaries, even accurate and sophisticated leading

Fig. 2. First frame of the airplane sequence (a); second frame of the airplane sequence (b); ground-truth of the foreground (c); magnitude of optical ﬂow computed according to [4] (d);
magnitude of optical ﬂow computed according to [6] (e); magnitude of optical ﬂow computed according to [13] (f).

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

methods in the Middlebury,1 KITTI,2 and MPI Sintel3 rankings, such as
[4,6], and [13] fail to capture the proper size and contour of the foreground in images with poorly-textured background regions. Note that
employing more complicated cost functions leads to larger computation
times, which is detrimental in real-time tracking procedures. After numerous observations of optical ﬂow results using state-of-the-art
methods, we came to the conclusion that no matter how sophisticated
the algorithm is, performance could be undesirable if original frames
have poor texture. This encouraged us to explore the outcomes of modifying original images, rather than modifying the computing algorithms.
Regions with poor texture exist both within the background and the
foreground; however the regions in the background are the main reason
for propagation of foreground ﬂow to neighboring pixels, object shape
distortion, and even object merging. Furthermore, adding a static texture to these regions can be performed with sufﬁcient accuracy. However, to generate a moving texture for the foreground regions, we need to
know pixels' correspondence, which is not possible without calculating
the optical ﬂow. If we want to calculate optical ﬂow once and use it
again for generating a moving texture, even small inaccuracies in the
optical ﬂow vector ﬁeld leads to addition of the texture to wrong pixels
and hence induction of erroneous ﬂow. Moreover, any interpolation
using feature matching is prone to mismatching errors and it requires
knowledge about the type of object motion (rigid or ﬂexible as they
need different types of interpolations) which is not known a-priori;
thus it cannot be used to generate an accurate moving texture. Therefore, we treat only poorly-textured background regions and leave the
modiﬁcation of foreground regions to future investigations. It is important to mention that the camera is assumed to be stationary in this
paper. Therefore, we only add texture to the background pixels. Should
the camera be moving, egomotion estimation and compensation are required to be performed before texture addition.
To localize and add texture to poorly-textured regions in the background, we ﬁrst perform a texture segmentation using Laws' masks
and generate a texture map. Next, using a binary frame difference, we
constrain the poorly-textured regions to those with negligible motion.
Finally, we calculate the optical ﬂow for the modiﬁed images with
added texture. It is shown that if the threshold used for binarizing the
frame difference is in a speciﬁc range determined empirically, variations
in the ﬁnal foreground detection will be insigniﬁcant. Note that optical
ﬂow calculations suffer from poor texture, and image differencing cannot provide us with motion information, being signiﬁcantly sensitive
to illumination changes and the binarizing threshold; however, computing optical ﬂow while using image differencing and texture addition
as just described provides improved accuracy and robustness in foreground detection, as will be shown.
The main contributions of this study are: (1) creation of sharp motion boundaries and more accurate capture of the object size, position,
and contour; (2) avoiding or mitigating object merging in sequences
with multiple objects moving in a small area with poor texture; (3) reduction in computation time; and (4) mathematical analysis of the
effects of texture addition on the optical ﬂow convergence and computation time. This paper is organized as follows: In Section 2, we describe
the problem in more detail, together with the texture addition algorithm and effects accompanied by mathematical analysis of optical
ﬂow convergence. Section 3 demonstrates representative and quantitative results, and discussion. Section 4 provides limitations and future
work, while general conclusions are included in Section 5.
2. Algorithm and analysis
In this section, we ﬁrst discuss the problem in more detail in
Section 2.1. Then, we describe the texture addition algorithm and effects
1
2
3

http://vision.middlebury.edu/ﬂow/.
http://www.cvlibs.net/datasets/kitti/eval_stereo_ﬂow.php?benchmark=ﬂow.
http://ps.is.tue.mpg.de/project/MPI_Sintel_Flow.

3

Fig. 3. Histogram of texture energy values for a typical image.

in Section 2.2, and explain these effects from the mathematical perspective in 2.3.
2.1. Problem discussion
We will use the formulation of HS throughout this section for simpler explanations, while applying the accurate and leading methods in
[4,6], and [13] for demonstrations later. Denoting light intensity by I,
ﬁrst order spatial and temporal derivatives of light intensity by (Ix, Iy)
and It, respectively, optical ﬂow components (u,v) in [1] are computed
by minimizing the following total energy function:
Z Z
ϕðu; vÞ ¼

À

Á
ρ2 ϕd ðu; vÞ þ ϕc ðu; vÞ dxdy

ð1Þ

where the data error energy function is given by:
À
Á2
ϕd ðu; vÞ ¼ Ix u þ I y v þ It

ð2Þ

and smoothness energy function is deﬁned as:
ϕc ðu; vÞ ¼

 2  2  2  2
∂u
∂u
∂v
∂v
þ
þ
þ
:
∂x
∂y
∂x
∂y

ð3Þ

Parameter ρ determines the relative weight of the data term. Then,
the Euler–Lagrange equations yield:
I 2x u þ I x I y v−ρ2 ∇2 u ¼ −I x I t
I x I y u þ I 2y v−ρ2 ∇2 v ¼ −Iy It :

ð4Þ

In regions where spatial light intensity variations are negligible
(poor texture), including along the x and y axes (Ix ≈ 0,Iy ≈ 0), Eq. (4)
will be approximated by the Laplace equations with boundary conditions dictated by the neighboring windows:
∇2 u ≈ 0; ∇2 v ≈ 0:

ð5Þ

If Eq. (5) holds in the background regions (outside the boundaries of
the foreground objects), non-zero optical ﬂow of the object blocks will
affect the neighboring background pixels, where zero motion is expected. This effect will spread, and the level of inﬂuence depends on the
magnitude of light intensity variations around the object (as well as
the computing method), which can be a region of the image relatively
larger than the object size in images with considerably poor texture.
This is illustrated in Fig. 2 for a sequence in which an airplane4 is moving
against a uniform background sky, with the ﬁrst frame, second frame,
and the ground-truth of the foreground shown in parts (a), (b), and
4

http://www.youtube.com/watch?v=qF9VZSkVZI0.

4

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

Fig. 4. First frame of the laboratory sequence (a); second frame of the laboratory Sequence (b); binary texture energy map, TEB (c).

(c), respectively. The magnitude of optical ﬂow computed according to
methods in [4,6], and [13] can be seen in parts (d), (e), and (f) in the
second row, respectively. Comparing the magnitudes of optical ﬂow

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w ¼ u2 þ v2 , with the ground-truth of the foreground, we can see
that not only are the detected moving pixels signiﬁcantly larger than
the object's size, especially in part (d), but also the contour of the object
is not preserved.
2.2. Texture addition algorithm and effects
The ﬁrst step is to localize poorly-textured regions in a frame. We
use the Laws' masks introduced in [21] to measure the texture energy
in different regions of an image. Denoting a gray-scale frame by IL, we
apply Laws' 2-D convolution kernels on IL, which can be created using
the following set of 1-D kernels of length three: L3 = [1 2 1] (average
gray level), E3 = [1 0 − 1] (edge extractor), and S3 = [1 − 2 1] (spot
extractor).
Although nine 2-D kernels can be built using the outer product of
these ﬁlters, we did not use LT3L3, since it only measures the average
gray level value in a 3 × 3 window, while we look for pixel-wise light intensity variations. Accordingly, each frame is convolved with the following set of eight 2-D masks: [LT3E3,LT3S3,ET3L3,ET3E3,ET3S3,ST3L3,ST3E3,ST3S3]. This
set is capable of measuring light intensity variations in different patterns (i.e. texture) in a region. Denote this kernel set by [K1,K2,…,K8]
and show the convolution operation by (∗). Then texture energy (TE)
at a pixel in position (i, j) is given by:

TEði; jÞ ¼

8
X
j F n ði; jÞj;
n¼1

ð6Þ

F n ¼ K n Ã IL:
The next step is to create a binary texture energy map, denoted by
TEB, which requires a threshold on the values of the texture energy,
here denoted by γ. To avoid using an empirically-determined threshold
which can fail for sequences not studied, we use an adaptive method
based on the histogram of the texture energy values. Based on the
knowledge from images processing – the major portion of an image information lies in the low texture energy values – and after investigating
a large number of sequences, we acquired assurance that the histogram
of TE values for a typical image looks like what is shown in Fig. 3 (here
using 100 bins). It is a right-skewed distribution with mostly decreasing
frequencies as the texture energy increases (there could be sudden increases, but low very texture energy values tend to have signiﬁcantly
larger frequencies).
We call those regions “poorly-textured” for which the texture energy levels are sufﬁciently different, here smaller than the other values
(with higher frequencies). In other words, we look for those values
with such high frequency that are outliers in this histogram. Since the
histogram of texture energy levels for most of the images does not follow a normal distribution, determining outliers is performed using the

method introduced in [22] for skewed distributions. In this paper,
Vanderviere and Huber introduced an adjusted boxplot taking into
account the medcouple MC, a robust measure of skewness for a skewed
distribution, which for a data series with sorted entries (Xn =
{x1,x2,…,xn},x1 ≤ x2 ≤ … ≤ xn) is given by:
Á
À

x j −medk −ðmedk −xi Þ
MC ¼ med
x j −xi

ð7Þ

with med and medk be the median operator and the median of Xn, and xi
and xj have to satisfy xi ≤ medk ≤ xj and xi ≠ xj. Then, for right-skewed distributions like in Fig. 3 with MC ≥ 0, the boxplot limits given in Eq. (8)
can be used to determine the outliers:
xi b Q 1 −1:5e−3:5MC IQ R ;

xi NQ 3 þ 1:5e4MC IQ R

ð8Þ

where Q1 and Q 3 are the ﬁrst and third quantiles and IQR is the
interquartile range. Here Xn represents the sorted frequencies of TE
values acquired from the histogram. We use the upper limit in
(8) to determine those texture energy levels with frequencies in the histogram that are outliers from above. For instance, for the histogram
shown in Fig. 3, only the two ﬁrst bins were found to be outliers; so a
threshold of γ = 0.02 was used and multiplied by the maximum value
of TE, since 100 bins were used.Because the binary texture energy
maps for the ﬁrst and the second frames are not usually identical due
to different factors, such as lighting variations, we use the binary intersection operator to ensure that texture will only be added to identical
locations in both frames. Denoting the texture energy maps for the
ﬁrst and the second frame by TE1 and TE2, the ﬁnal binary texture
map is given by:
&
TEBði; jÞ ¼

1
0

if ðTE1 ði; jÞ≥γ Â maxðTE1 ÞÞ ∩ ðTE2 ði; jÞ≥γ Â maxðTE2 ÞÞ
otherwise:

ð9Þ
Fig. 4 shows the ﬁrst and second frames from a laboratory sequence,5
and the binary texture energy map in parts (a), (b), and (c), respectively.
TEB distinguishes only between regions with rich and poor textures.
So, to localize and add texture only to the poorly-textured regions in the
background, we must use a type of foreground detection algorithm. We
opted to use image differencing due to the small computation time required. Note that in addition to foreground detection (which is not accurate for poorly-textured images), optical ﬂow can also provide further
information, such as direction and magnitude of pixels' displacement
per frame. Furthermore, image differencing cannot be employed to detect the foreground with sufﬁcient accuracy due to lighting changes,
small capture rate, etc., and the shape of the resulting binary map will
depend on the threshold utilized. Therefore, image differencing cannot
replace optical ﬂow regarding accurate detection of motion; it is merely

5

http://arma.sourceforge.net/shadows/.

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

5

Fig. 5. First row: Binary frame difference (a); background regions with poor texture rendered in solid white (b); modiﬁed ﬁrst frame (c); modiﬁed second frame (d); second row: Magnitude of optical ﬂow for original frames computed according to [4,6], and [13] in (e), (f), and (g), respectively; second row: Magnitude of optical ﬂow for modiﬁed frames computed according to [4,6], and [13] in (h), (i), and (j), respectively.

used to ensure that the static texture is not added to the pixels with apparent motion, and that the added texture does not induce erroneous
ﬂow.
While the threshold used for binarizing the frame difference will
determine the shape of the binary map, FDB, later we show that the
ﬁnal optical ﬂow magnitude using the texture-added frames will not

signiﬁcantly vary provided that the threshold, β, is selected from
an empirically-determined range. Denoting the frame difference by
(FD = IL2 − IL1), we deﬁne FDB as:
&
FDBði; jÞ ¼

1
0

if FDði; jÞ≥β Â maxð FDÞ
otherwise:

ð10Þ

Fig. 6. First row: Magnitude of optical ﬂow using original images computed according to [13] (a); ground-truth for the foreground (b); second row: binary frame difference using different
thresholds, (c) to (g); third row: Magnitude of optical ﬂow using modiﬁed images computed according to [13] using different thresholds used for binarizing the frame difference, (h) to (l).

6

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

Fig. 7. Modiﬁed ﬁrst frame of the highway sequence (a); modiﬁed second frame of the highway sequence (b); magnitude of optical ﬂow using modiﬁed images computed according to [6] (c).

Fig. 8. Effect of texture magnitude on the erroneous background ﬂow suppression for the airplane sequence: First frame (a); second frame (b); modiﬁed second frame using SC = 5 (c);
modiﬁed second frame using SC = 40 (d); modiﬁed second frame using SC = 75 (e); magnitude of optical ﬂow using SC = 5 computed according to [6] (f); magnitude of optical ﬂow using
SC = 40 computed according to [6] (g); magnitude of optical ﬂow using SC = 75 computed according to [6] (h).

Note that in order to binarize the frame difference, typically a ﬁxed
threshold might be used, but we can improve the results by employing
an adaptive threshold. To avoid rendering small light intensity variations (due to illumination changes, camera inherent noise, etc.) as foreground pixels, we tried to only keep those pixels with the largest light
intensity variations, which can be determined by comparing their light
intensity variation magnitude to the maximum value of light intensity
change. Therefore, we used the max( ) function in Eq. (10).
We also perform binary image ﬁlling on FDB to avoid missing some
of the moving pixels within the FDB borders. To generate the texture
that will be added to the frames, we can select texture patches from
available texture images or simply generate a stochastic texture. Selection of the texture types depends on the optical ﬂow computation
method, which will be discussed in Section 3. To create the stochastic
texture for each pixel (i, j), ﬁrst three random numbers (RND) from a
normal distribution are generated with μ = 0 and σ = 1 for three RGB
channels (RND(i,j,k) ∈ N(0,1),k = 1,2,3). Then, they are multiplied by

a scalar, SC that determines the magnitude of the texture for each
pixel. The static stochastic texture image (STX) has the same size as
both sequence frames, and is given by:
STXði; j; kÞ ¼ SC Â RNDði; j; kÞ

; k ¼ 1; 2; 3:

ð11Þ

Finally, the modiﬁed frames (IN1, IN2) are created by adding texture
only to the regions in both original frames (here original color frames
are denoted by IM) that have poor texture and do not show apparent
motion:
&
INði; j; kÞ ¼

IMði; j; kÞ þ STXði; j; kÞ if
ðTEBði; jÞ ¼ 0 and
IMði; j; kÞ
otherwise:

FDBði; jÞ ¼ 0Þ

ð12Þ
In the ﬁrst row in Fig. 5, part (a) shows the binary frame difference
for the laboratory sequence, and part (b) shows the map that localizes

Fig. 9. By rows: modiﬁed ﬁrst frame, original second frame, ground-truth mask for the foreground, magnitude of optical ﬂow using according to [4] for original images, magnitude of optical
ﬂow according to [4] for modiﬁed images, magnitude of optical ﬂow according to [6] for original images, magnitude of optical ﬂow according to [6] for modiﬁed images, magnitude of
optical ﬂow according to [13] for original images, and magnitude of optical ﬂow according to [13] for modiﬁed images: ﬁrst column, laboratory sequence; second column, wooden
model sequence; third column, surveillance sequence; fourth column, indoor practice sequence; ﬁfth column, rolling balls sequence.

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

7

8

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

background regions with poor texture. Modiﬁed frames with texture
addition can be seen in parts (c) and (d), respectively. Here, for frames
with 8-bit unsigned integer values, SC = 40 was used for all pixel. Selection of SC will be discussed in Section 2.3. Magnitude of optical ﬂow calculated according to [4,6], and [13] for the original and modiﬁed frames
are shown in the second and third rows, respectively. Comparing the
optical ﬂow magnitude results for original and modiﬁed frames, we observe that the erroneous background ﬂow has been suppressed, and
thus the foreground boundaries have been preserved with higher
accuracy in all methods, despite differences in the approaches used to
calculate optical ﬂow components. Quantitative improvement in preservation of foreground boundary detection is discussed in Section 3.
We have illustrated the effect of β ∈ [0.002,0.1] on the binary frame
difference FDB and the optical ﬂow magnitude using the texture-added
frames in Fig. 6. In the ﬁrst row, part (a) shows the magnitude of optical
ﬂow using original images computed according to [13] and part
(b) shows the ground-truth for the foreground; in the second row, binary frame difference is shown for β = 0.002, β = 0.01, β = 0.025, β =
0.04, and β = 0.1; and the third row shows the magnitude of optical
ﬂow using modiﬁed frames computed according to [13]. Very small
thresholds as in part (c) lead to detection of the majority of pixels as belonging to the foreground, since light intensity of pixels do not remain
unchanged due to lighting changes, quantization effects, etc. Therefore,
texture will not be added to the poorly-textured regions as they are recognized to be foreground regions, and thus the magnitude of optical
ﬂow for the modiﬁed and original images will be similar without specific improvements. For large thresholds as in part (g), only a fraction of
foreground pixels will be correctly detected, and adding texture incorrectly to the moving pixels will result in erroneous ﬂow in the foreground regions (small ﬂow magnitude in part (l) for some pixels). Our
empirical results show that if the threshold is selected from the range
β ∈ [0.01,0.04], the magnitude of optical ﬂow remains approximately
the same, while the binary frame difference signiﬁcantly changes with
the threshold. Similar experiments with other sequences revealed that
the range β ∈ [0.01,0.04] (enclosed in a red rectangle in Fig. 6) can provide reasonable results.
In images with multiple objects moving close to each other, smoothness of optical ﬂow variations and propagation of foreground ﬂow into
neighboring background pixels lead to object merging, as observed in
the second row of Fig. 1 for the Highway Sequence. Modiﬁed frames
and the magnitude of optical ﬂow for these frames computed according
to [6] are shown in parts (a) to (c) in Fig. 7. (Here we use the method
proposed in [6] rather than in [13], since it is designed to handle large
displacements of the objects as they exist in this sequence). As compared to part (f) in Fig. 1, adding texture to background regions with
poor texture results in the suppression of erroneous ﬂow for the background pixels, thus reducing object merging, although not completely
eliminating the problem.
The other advantage of adding texture to the images with poorlytextured regions in the background is reduction in the computation
time, which is explained in 2.3. For the Laboratory Sequence shown in
Fig. 4, we noticed 20.1%, 7.8%, and 11.7% reduction in computation
time for methods in [4,6], and [13], respectively. Similar results
have been observed in other sequences, which will be discussed in
Section 3.
2.3. Analysis of convergence and computation time reduction
In this section, we explain why adding texture to poorly-textured
background regions suppresses erroneous ﬂow and why it reduces the
computation time. Rewriting Eq. (4) in matrix notation helps analyze

9

the convergence of optical ﬂow values for the background to very
small values, and reduced computation time. Mitchie and Mansouri
[23] used discretization of the Laplace operator and rewrote Eq. (4)
into a set of linear equations of the form Az = b, where A is a tridiagonal
matrix, vector z contains 2N2 unknown optical ﬂow components, u and
v, of an image with N × N pixels, and a constant vector b of the same size
as z. Proof of block-wise convergence for unknown values in vector z
using Gauss–Seidel iteration is provided in [23].
We can rewrite Eq. (4) for ρ = 1 as:
∇2

Àu Á
v

À Á
þ M uv ¼ R

ð13Þ

where M and R are deﬁned by:
"

I2
M¼− x
Ix Iy

#
!
Ix Iy
I I
;R ¼ x t
2
Iy It
Iy

ð14Þ

and the magnitudes of Ix and Iy depend on SC. For the background regions, where texture has been added, Ix and Iy will no longer be negligible terms, while temporal derivative It is close to zero (because
background pixels do not move and their light intensities do not change
signiﬁcantly with time). If we use ﬁnite difference formulas, then the
higher the scalar value SC is, the larger the spatial derivative terms become, and the larger the magnitudes of entries in M and R become.
Since the entries of the resulting matrix from the Laplacian operator
on the left-hand-side of Eq. (13) are not affected by the texture magnitude, and are calculated using only a weighted averaging of the neighboring ﬂow components (which are not large), the Laplacian operator
term would be signiﬁcantly smaller than the entries of M and R for relatively large values of SC (SC N 20). Therefore, Eq. (13) can be written as
M(uv ) ≈ R, which is approximately a set of linear homogeneous equations, where the solution must converge to zero due to invertibility of
the positive deﬁnite matrix M [23].
For the background blocks in the vicinity of the foreground
objects, due to the coupling of optical ﬂow components to the foreground values (as explained in [23]), values for u and v will not
converge to zero, but to small values. As we move further from the foreground object, background ﬂow would approximately vanish in a distance that depends on the magnitude of SC. The effect of SC on the
suppression the background ﬂow can be clearly seen in Fig. 8 for the Airplane Sequence in Fig. 2. First and second frames are shown in parts
(a) and (b); modiﬁed second frames are shown in parts (c) to (e) for different values of SC (higher values mean stronger texture intensity); and
corresponding magnitudes of optical ﬂow are displayed under each
modiﬁed frame in (f) to (h), respectively. As the value of SC increases,
the background ﬂow vanishes in shorter distances from the object.
Higher values of SC, however will lead to higher errors in the foreground object boundaries. Consider a background pixel and its immediate neighboring pixels in a poorly-textured background region that are
covered by a foreground object only in the ﬁrst frame (possibly pixels
near boundaries). Using I to show light intensity values and indices i, j,
and k for the x-axis, y-axis, and time, respectively, we can write the variations in the light intensity derivatives as:
Ii; j;kþ1 →Ii; j;kþ1 þ n1
Iiþ1; j;kþ1 →Iiþ1; j;kþ1 þ n2
Ii; jþ1;kþ1 →I i; jþ1;kþ1 þ n3
I iþ1; jþ1;kþ1 →Iiþ1; jþ1;kþ1 þ n4

ð15Þ

where “ → ” indicates “become” and n1 to n4 are random numbers due to
texture addition. If we employ the same discretizations used in [1] for

Fig. 10. By rows: modiﬁed ﬁrst frame, original second frame, ground-truth mask for the foreground, magnitude of optical ﬂow using according to [4] for original images, magnitude of
optical ﬂow according to [4] for modiﬁed images, magnitude of optical ﬂow according to [6] for original images, magnitude of optical ﬂow according to [6] for modiﬁed images, magnitude
of optical ﬂow according to [13] for original images, and magnitude of optical ﬂow according to [13] for modiﬁed images: ﬁrst column, highway sequence; second column, basketball sequence; third column, UAV sequence; fourth column, ofﬁce sequence; ﬁfth column, playground sequence.

10

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

numerical differentiations in Ix, Iy, and It, then the expected values of the
derivative terms can be given by Eq. (15), where E[n] is the expected
value of a random variable n:
E½ÂIx →E
Ã ½IÂx  Ãþ E½n3 þ n4 −n1 −n2 
E Iy →E Iy þ E½n2 þ n4 −n1 −n3 
E½It →E½I t  þ E½n1 þ n2 þ n3 þ n4 :

ð16Þ

Therefore, selection of random numbers in the synthetic texture
from a normal distribution with expected value of zero helps to maintain the expected values of spatial and temporal derivative terms unchanged. Note that our zero-mean normal distribution is more likely
to produce a larger number of random values close to zero than a
zero-mean uniform distribution. This will reduce the foreground errors
around the foreground object boundaries. Here, higher SC values would
magnify the standard deviation of the foreground ﬂow error, while suppressing the background ﬂow faster.
As we can see in Fig. 8(c), employing large values for SC does not
change the foreground size and shape considerably and provides negligible advantage. However, higher values of SC cause the optical ﬂow
error in the foreground boundary pixels to increase signiﬁcantly. For
the Airplane Sequence in Fig. 8, the object only undergoes a translation
of u = −5.11 and v = −2.28 pixels between two frames. The percentÀ
Á
age of average errors Δu; Δv in the u and v components are (0.40%,
0.32%), (1.16%, 0.91%), and (2.52%, 2.11%) for the object boundary pixels
using texture magnitudes of SC = 5, SC = 40, and SC = 75, respectively.
Experiments with multiple sequences have revealed that a texture magnitude of SC = 40 for images with 8-bit unsigned integer values (or
equivalently 15% of the maximum light intensity value) can provide a
reasonable compromise between suppression of erroneous background
ﬂow and the errors in u and v components for the object boundary pixels.
Eq. (13) can also help investigate the reduction in computation time.
In a linear system of equations, such as Az = b, (where A is a positive
deﬁnite matrix) the rate of convergence is directly related to the condition number (κ) of the coefﬁcient matrix A, which can be deﬁned as the
ratio of the largest eigenvalue of a matrix, λmax, to the smallest eigenvalue of the matrix, λmin, for any symmetric positive matrix [24]. The higher
the condition number, the slower the convergence would become. If we
use a 9-point discretization for the Laplace operator on an image of
N × N pixels, the condition number of the resulting positive deﬁnite matrix (denoted by L) is of order O(N2) [25], which could be very large
(≫1). In images with poorly-textured backgrounds, such as the UFO sequence, since the matrix M for these regions approximately vanishes in
Eq. (13), the condition number of the Laplace operator is the dominating term, and it leads to very slow convergence.
When synthetic texture is added, entries in matrix M are no longer
negligible, so we encounter the problem of eigenvalues of the sum of
two Hermitian (here positive deﬁnite) matrices. If we denote the eigenvalues of a matrix in a descending order by λ1 N λ2 N … N λSZ, where SZ is
the size of the matrix, then we can ﬁnd the upper and lower bounds for
the largest and smallest eigenvalues of the sum of two positive deﬁnite
matrices by referring to the Weyl inequality [26]:
λ1 ðL þ MÞ≤λ1 ðLÞ þ λ1 ðMÞ
λ1 ðL þ MÞ≥maxðλr ðLÞ þ λrþSZ−1 ðMÞÞ; r ¼ 1; 2; …; SZ
λSZ ðL þ MÞ≤minðλr ðLÞ þ λrþSZ−1 ðMÞÞ; r ¼ 1; 2; …; SZ
λSZ ðL þ MÞ≥λSZ ðLÞ þ λSZ ðMÞ:

ð17Þ

The condition number of L + M, denoted by κ(L + M) is the ratio
of λ1(L + M) to λSZ(L + M), which is bounded by the following
inequalities:
maxðλr ðLÞ þ λrþSZ−1 ðMÞÞ
λ1 ðLÞ þ λ1 ðMÞ
≤ κ ðL þ MÞ≤
minðλr ðLÞ þ λrþSZ−1 ðMÞÞ
λSZ ðLÞ þ λSZ ðMÞ

; r ¼ 1; 2; …; SZ: ð18Þ

Within a poorly-textured background region, if the values of Ix and Iy
using a very small scalar value of SC0 (which can correspond to the

original image without texture addition) are designated Ix0 and Iy0,
then amplifying the texture magnitude using a scalar value of η × SC0
would change these terms to η × Ix0 and η × Iy0 due to the linearity of
ﬁnite difference calculations. Furthermore, multiplication of scalar SC
by a factor of η would magnify all the entries and the eigenvalues of M
by a factor of η2, including the smallest and the largest eigenvalues. As
a result, all numerators and denominators of the upper bound and the
lower bound terms in Eq. (18) increase linearly proportional to η2. This
leads to the reduction of both upper and lower bounds, since
λ1(L) ≫ λSZ(L) and the bound terms are decreasing functions with respect to eigenvalues of M, or equivalently η2. Reduction in the upper
and the lower bounds will lead to the reduction in the condition number
of the equivalent matrix (M + L), such that the convergence rate would
increase.
3. Results and discussion
In this section, we will ﬁrst demonstrate representative results in
Section 3.1. Then, we evaluate the performance of optical ﬂow methods
used in this study with and without texture addition in terms of foreground boundary preservation and computation time in Section 3.2. Finally, we provide discussion in Section 3.3.
3.1. Representative results
As we have seen in the previous section, an important advantage of
the preprocessing stage is the suppression of the erroneous background
ﬂow yielding sharp motion boundaries and more accurate rendering of
the foreground size and shape. To illustrate this effect, we employed texture addition in conjunction with accurate and leading optical ﬂow
methods in [4,6], and [13] on ten sequences with different texture distributions and number of moving objects, eight of which are captured from
real videos and two are animations. The number of methods we used is
limited by the availability of the publication and the algorithm code, because many leading algorithms in databases are from anonymous subscribers without access to the publication or the algorithm. The method
in [4] is an accurate algorithm with similar performance over all sequences used in this study, so it can be considered as a reference for comparisons. The method in [6] is one of the leading methods in the KITTI
database, which can handle large displacements more efﬁciently. The
method in [13] uses median ﬁltering that leads to higher accuracy of object boundary preservations in many sequences. This method was ranked
1st in the Middlebury database in 2010, and is currently a leading method
in the KITTI and MPI Sintel databases.
Due to the large number of sequences and images, and to maintain
sufﬁcient space for each sequence, we have divided the results into
two separate ﬁgures, Figs. 9 and 10. For each sequence, the results are
displayed in one column, where the images from the top to the bottom
are, respectively: the modiﬁed ﬁrst frame, the original second frame,
ground-truth mask for the foreground, magnitude of optical ﬂow
using the method in [4] for original images, magnitude of optical ﬂow
using the method in [4] for modiﬁed images, magnitude of optical
ﬂow using the method in [6] for original images, magnitude of optical
ﬂow using the method in [6] for modiﬁed images, magnitude of optical
ﬂow using the method in [13] for original images, and magnitude of optical ﬂow using the method in [13] for modiﬁed images.
To generate ground-truth images, we used the edge maps of the
frames which were delineated by the Canny edge detector with manually selected parameters determined to render all edge pixels. Next, we
asked multiple volunteers to manually eliminate extra edge pixels and
connect non-connected edges. Finally, image ﬁlling was performed on
the accurate edge maps. In Fig. 9, the ﬁrst sequence (laboratory sequence) shows a person moving away from the camera in a laboratory,6
6

http://arma.sourceforge.net/shadows/.

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

where cabinet doors surrounding his body have poor texture. The second sequence shows a wooden model7 (wooden model sequence),
where the upper body and cast shadows move against a uniform background. The third sequence shows a personal vehicle captured by a surveillance camera8 (surveillance sequence), where the vehicle and the
cast shadow move against the street with poor texture. In these sequences, a single object (and corresponding cast shadows) is moving,
and the texture distribution varies signiﬁcantly across the sequences.
Texture addition in all methods helps suppress the erroneous ﬂow
around the objects (and shadows) and helps preserve the motion
boundaries more accurately. The fourth and the ﬁfth sequences demonstrate multiple objects moving, where two individuals are practicing in
an indoor environment9 (indoor practice sequence) with a poorlytextured background in the fourth sequence, and three rolling balls10
(and cast shadows) are animated on a curved surface with poor texture
in the ﬁfth sequence (rolling balls sequence). For these sequences,
texture addition and suppression of the background ﬂow also helps
prevent object merging, speciﬁcally for the method in [4]. Comparing
ground-truth for the foreground in the third row to the magnitude
of optical ﬂow in ﬁfth, seventh, and ninth rows, we notice that the combination of the optical ﬂow method in [13] with texture addition provides the most accurate foreground detection for the sequences in this
ﬁgure.
In Fig. 10, all sequences have multiple moving objects. The ﬁrst sequence (highway sequence) shows four vehicles moving toward the
camera on a highway11 with poor texture, where object merging can
be clearly observed. The second sequence shows an indoor sequence
(basketball sequence12), where two individuals are playing basketball
against a background with partial poor texture. The third sequence
shows two unmanned aerial vehicles ﬂying in a partially cloudy sky
(UAV sequence13). The fourth sequence is taken from an indoor ofﬁce
video, where two individuals are moving against a uniform background
(ofﬁce sequence14). The ﬁfth sequence is an animation in which three
children are moving against a background with poorly-textured regions
(playground sequence15). Similar to Fig. 9, texture addition in all
methods helps suppress the erroneous ﬂow around the objects (and
shadows), and helps preserve the motion boundaries more accurately.
Note that while combination of texture addition and the optical ﬂow
method in [13] provides the most accurate foreground detections for
most of the sequences in Fig. 10, for the highway and playground sequences, it does not show the best results and is not able to prevent
the object merging problem. This is due to large object displacement
in both sequences. As can be seen, the method in [6] demonstrates
higher accuracy and is the only method capable of preventing object
merging for the highway sequence, because of considering an extra
term in the error functional, which employs feature matching to handle
large displacements.

11

mask and the detected foreground mask, given by:
Fα ¼

ð1 þ α Þ Â Precision Â Recall
;
α Â Precision þ Recall

ð19Þ

where Precision/Recall are the ratios of the correctly detected foreground (overlapped area) to the detected/ground-truth foreground.
Denoting the area of detected foreground mask and ground-truth foreground mask by A(D) and A(G), respectively, Precision is given by:
Precision ¼

AðD ∩ GÞ
;
AðDÞ

ð20Þ

and Recall is given by:
Recall ¼

AðD ∩ GÞ
:
AðGÞ

ð21Þ

Clearly, larger values for Fα indicate higher overlap between the
ground-truth foreground and the detected foreground, with Fα = 1 indicating perfect overlap.The BDE measures the average displacement
error between the boundaries of two masks mentioned above. Let BD
and BG represent the boundary point set of the detected and groundtruth rectangles, respectively. The BDE from B D to BG , denoted as
E(D, G), is computed as the average of distances from every point p
in BD to its closest point in BG:
X
EðD; GÞ ¼

p∈BD

dðp; BG Þ

jB D j

:

ð22Þ

In Eq. (21), |BD| represents the number of points in set BD, and d(p, BG)
represents the minimum Euclidean distance from p to all points in BG:
dðp; BG Þ ¼ min
q∈BG

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðp1 −q1 Þ2 þ ðp2 −q2 Þ2 ;

ð23Þ

where (p1, p2) and (q1, q2) are coordinates of p and q, respectively. E(G, D),
is computed similarly. The ﬁnal BDE between BD and BG is computed as
the average of these two BDEs:
BDEðBD ; BG Þ ¼

1
½EðD; GÞ þ EðG; DÞ:
2

ð24Þ

Smaller values for BDE indicate lower average boundary displacement between the ground-truth foreground and the detected foreground, with BDE = 0 indicating perfect boundary match.
Table 1 summarizes the values for Fα calculated for the optical ﬂow
methods in [4,6], and [13] without and with texture addition as they
are employed on all sequences used in this study. Results indicate significant improvements in preservation of the foreground boundaries if the

3.2. Quantitative results
To quantify the effect of texture addition on the accuracy of foreground detection, we utilized two measures that are frequently used
in the literature, such as in [27] when two binary images are compared:
F-measure [28] and Boundary Displacement Error (BDE) [29]. Given a
speciﬁc weight α (here 0.5), we use Fα to designate F-measure, which
evaluates the amount of overlap between the ground-truth foreground

7
8
9
10
11
12
13
14
15

http://www.youtube.com/watch?v=eJlqQSMifqk.
http://www.youtube.com/watch?v=x6HRKncJuB0.
http://www.youtube.com/watch?v=APDmcwT1ii4.
http://visual.cs.ucl.ac.uk/pubs/ﬂowConﬁdence/supp/.
http://arma.sourceforge.net/shadows/.
http://vision.middlebury.edu/ﬂow/data/.
http://www.youtube.com/watch?v=fgHjVvqLXV8.
http://www.youtube.com/watch?v=cOyla67NMHk.
http://www.youtube.com/watch?v=GXmW6S1iVCI.

Table 1
Effect of the adding texture on the foreground boundary preservation (Fα) for different
methods using (SC = 40). Without texture addition: “wo tex add”, with texture addition:
“w tex add”. Fα values closer to unity are better.
Sequence

[4] wo
tex add

[4] w
tex add

[6] wo
tex add

[6] w
tex add

[13] wo
tex add

[13] w
tex add

Laboratory
Wooden model
Surveillance
Indoor practice
Rolling balls
Highway
Basketball
UAV
Ofﬁce
Playground
Average

0.395
0.079
0.664
0.370
0.189
0.327
0.634
0.082
0.259
0.327
0.332

0.686
0.434
0.831
0.736
0.541
0.396
0.808
0.487
0.808
0.614
0.634

0.792
0.290
0.737
0.724
0.706
0.402
0.660
0.253
0.547
0.553
0.567

0.864
0.772
0.870
0.794
0.836
0.714
0.863
0.530
0.869
0.683
0.779

0.672
0.120
0.988
0.541
0.809
0.330
0.602
0.382
0.288
0.338
0.504

0.927
0.865
0.990
0.813
0.958
0.399
0.898
0.552
0.930
0.476
0.781

12

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

optical ﬂow methods are accompanied by texture addition. Comparison
of Fα shows 30%, 21%, and 28% more overlap on average between detected and ground-truth foreground masks for the methods in [4,6], and
[13], respectively.
Table 2 summarizes the values for BDE calculated for the optical ﬂow
methods in [4,6], and [13] without and with texture addition as they are
employed on all sequences used in this study. Results indicate signiﬁcant improvements in preservation of the foreground boundaries
(displacement error) if the optical ﬂow methods are accompanied by
texture addition. BDE comparison shows an average reduction in the
Boundary Displacement Error between detected and ground-truth foreground masks by a factor of 5 to 6 for all methods.
Note that from the majority of representative results, it is expected
that the method in [13] with texture addition would provide the most
promising quantitative values. However Table 1 shows only marginal
performance increase for [13] with respect to the method in [6] with
texture addition. Moreover Table 2 shows higher BDE for the method
in [13] compared with the method in [6]. As explained earlier, the
reason for relatively poor performance of the method in [13] for the
highway sequence and the playground sequence is large object displacement as a result of low capture rate, which can be clearly observed
from both tables, given that the method in [6] is designed to handle
large displacements. To have a fair comparison between competing
methods with data that is not biased toward one method, we need to
compare Fα and BDE for the sequences in these tables other than the
highway and playground sequences. New calculations for the averages
over these remaining sequences show that average Fα for [13] with texture and [6] with texture are 0.853 and 0.799, respectively. Average BDE
for [13] with texture and [6] with texture become 0.470 and 0.623, respectively. Now, it can be seen that [13] with texture demonstrates
the highest performance provided that large object displacement does
not occur.
Another advantage of texture addition is the reduction in computation time. Table 3 displays the variation percentage (deﬁned as the difference between computation time with texture and computation time
without texture divided by computation time without texture) in the
running time for all three optical ﬂow methods as they are applied to
all sequences used in this study. Computations were performed using
Matlab 2011a on a processor of Intel(R) core TM i7-2670QM CPU@
2.20GHz with 6.00 GB of installed RAM. Here, two types of texture are
used: stochastic, which is generated according to (11) and regular,
which is taken from a texture image. The reason for considering regular
texture is that as can be seen in the fourth column of Table 3, computation time for the method in [6] has increased when stochastic texture is
added to sequences. This is due to considering an extra term for feature
matching in the energy functional in [6] for handling large displacements. When stochastic texture is added to frames, the number of
feature points detected and used in the calculations dramatically increases. Therefore, increase in the running time due to larger number

Table 3
Effect on variation percentage of computation time from adding texture for different
methods using regular texture: “r tex” and stochastic texture: “s tex” (SC = 40).
Sequence

[4] r tex

[4] s tex

[6] r tex

[6] s tex

[13] r tex

[13] s tex

Laboratory
Wooden model
Surveillance
Indoor Practice
Rolling balls
Highway
Basketball
UAV
Ofﬁce
Playground
Average

−20.1
−19.6
−24.6
−55.7
−54.0
−47.6
−57.8
−69.3
−60.8
−72.5
−48.2

−18.2
−64.4
−19.2
−42.7
−59.9
−27.4
−64.9
−74.9
−64.0
−70.1
−50.6

−7.8
−24.8
−14.8
−44.1
−33.2
−27.9
−15.3
−12.9
−21.8
−55.7
−25.8

+58.2
+198.3
+20.8
+38.9
+50.4
+5.1
+8.1
+270.4
+212.2
+18.9
+88.1

−3.7
−34.4
−2.2
−10.1
−18.6
−0.1
−7.8
−19.5
−11.2
−2.9
−11.1

−11.7
−33.1
−1.1
−10.3
−23.8
−0.2
−6.7
−27.4
−8.6
−0.8
−12.7

of feature point detection, description, and matching will be considerably larger than the decrease in the running time due to texture addition. Accordingly, we also employed a regular texture image for all
methods to investigate the difference between the effects of regular
and stochastic textures on the computation time. Comparing the fourth
and the ﬁfth columns, we can see that since a regular texture (with
moderate size of textons as shown in Fig. 11) does not add a very
large number of feature points to each frame, computation time of [6]
with texture shows reduction with respect to [6] without texture. Furthermore, reduction percentages using regular and stochastic textures
for other methods exhibit only negligible differences. As a result, the
texture type does not change the computation time signiﬁcantly for
other methods. We note that the effect of the texture type on Fα and
BDE variations are similarly small.
3.3. Discussion
In this section, we discuss two remaining issues related to the algorithm suggested. First, the answer to the following question: “what is
the difference between adding static texture in non-moving areas according to this algorithm with simply setting optical ﬂow component
to zero for those pixels?”; second, further discussion about the parameters used in the algorithm.
Since a major goal of our proposed algorithm is to reduce the optical
ﬂow of the background regions with poor texture, ideally to zero, one
might wonder why not setting the optical ﬂow values to zero for the
static poorly-textured regions after optical ﬂow using original frames
is calculated. To answer this question, we have to go back to the technique used for rough estimation of the moving pixels. On one hand, as
explained in 2.2, image differencing can provide a fast yet crude estimate of the moving pixels. On the other hand, while results of texture
segmentation are satisfying, there is potential for imperfection in the
ﬁnal binary mask. Therefore, when TEB and FDB are combined, the
mask of static poorly-texture regions is not very accurate, as can be
seen in Fig. 5(b). In fact, this mask contains some false alarms as well

Table 2
Effect of the adding texture on the foreground boundary preservation (BDE) for different
methods using (SC = 40). Without texture addition: “wo tex add”, with texture addition:
“w tex add”. BDE values closer to zero are better.
Sequence

[4] wo
tex add

[4] w
tex add

[6] wo
tex add

[6] w
tex add

[13] wo
tex add

[13] w
tex add

Laboratory
Wooden model
Surveillance
Indoor practice
Rolling balls
Highway
Basketball
UAV
Ofﬁce
Playground
Average

8.443
32.462
2.857
10.100
7.854
10.297
6.558
8.414
25.149
11.349
12.349

1.722
5.963
0.722
1.145
1.421
7.630
1.426
1.097
0.859
1.993
2.398

0.842
12.378
2.775
6.134
0.586
8.021
5.724
2.660
6.604
2.866
4.859

0.464
0.560
0.421
1.273
0.218
2.774
0.706
0.889
0.453
1.111
0.887

3.300
29.123
0.020
4.476
0.631
11.088
7.460
2.555
27.748
8.490
9.569

0.138
0.314
0.019
0.949
0.061
9.302
0.489
1.557
0.236
6.429
1.983

Fig. 11. Regular texture image used in Table 3.

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

as signiﬁcant number of disjoint pixels in the background due to camera
inherent noise, lighting changes, and so on. As a result, employing this
mask along with the optical ﬂow magnitude from original frames
leads to a foreground mask with remarkable boundary distortions.
This is shown in Fig. 12 for four sequences used in this study, including
the laboratory sequence in Fig. 5.
As observed in this ﬁgure, employing the ﬁnal mask for static poorlytextured regions in order to suppress optical ﬂow does not yield accurate object boundaries and produces erroneous foreground blobs due
to imperfection in this mask. This is while if this mask is only used to
add texture to these regions, should the synthetic texture be mistakenly
not added to some background pixels, due to calculation of zero ﬂow for
neighboring pixels and the effects of those pixels next to the pixel deprived of texture (as discussed in Section 2.3), optical ﬂow can still converge to zero for this pixel. This can be viewed as the healing effect of
optical ﬂow.
To quantitatively compare the effects of our proposed algorithm with
the idea of simply suppressing the optical ﬂow for pixels in the static
purely-textured regions on the foreground detection accuracy, we calculated Precision, Recall, Fα, and BDE for the sequences in Fig. 12, and
showed the results in Table 4. Since the crude frame difference map detects even the smallest variations in a pixel's brightness, the probability
of detecting all foreground pixels and subsequently the value for Recall
is expected to be higher if the latter method is used. This can be seen
in the fourth and ﬁfth columns of Table 4, where Recall values for the

13

alternative idea are either comparable or higher. This is while, our proposed algorithm shows higher Precision values due to suppressing the
erroneous background ﬂow, as discussed earlier (second and third columns). For Fα, which provides a good representation of both Precision
and Recall, and BDE, our proposed algorithm outperforms the alternative
idea, as can be seen by comparing sixth and seventh columns and eighth
and ninth columns of this table, respectively. Hence, we can claim that
our method provides higher foreground detection accuracy.
The next topic is about the effects of parameters used in the algorithm section: β, γ, and SC. Effects of β on the ﬁnal binary mask of
image difference were extensively discussed in 2.2 and Fig. 6. Investigating multiple sequences, it was empirically found that β ∈ [0.01,0.04] can
provide a safe range for accurate foreground detection; however, it
cannot guarantee perfect application for all sequences.
Threshold for texture segmentation (γ) was adaptively determined
based on the histogram of the texture energy values and was explained
in 2.2 using (8), and Fig. 3. Due to its adaptive nature, γ can be more
trusted over a large number of sequences.
Finally, effects of SC were studied extensively in 2.3. Since the effect
of SC on the shrinkage of the foreground blobs was only qualitatively
demonstrated in Fig. 8, we employed the quantitative measure of the
foreground detection accuracy introduced in 3.2 using Fα to further clarify selection of value SC = 40 for the synthetic texture intensity. Fig. 13
shows the effect of SC on Fα for the airplane sequenced used in Fig. 8. As
can be seen, beyond SC = 40, the accuracy of foreground detection

Fig. 12. By columns: optical ﬂow magnitude using original frames, mask of static non-textured pixels shared in both frames, optical ﬂow magnitude using original frames and set to zero
where TEB = FDB = 0, optical ﬂow magnitude using modiﬁed images: ﬁrst row, laboratory sequence; second row, basketball sequence; third row, indoor practice sequence; fourth row,
ofﬁce sequence.

14

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15

Table 4
Quantitative comparison of foreground detection accuracy using our proposed algorithm “OM” and the idea of simply suppressing the optical ﬂow for pixels in the static purely-textured
regions “AM” via Precision, Recall, Fα, and BDE (winner for each metric is shown in bold).
Sequence

Precision (OM)

Precision (AM)

Recall (OM)

Recall (AM)

Fα (OM)

Fα (AM)

BDE (OM)

BDE (AM)

Laboratory
Ofﬁce
Indoor practice
Basketball

0.940
0.926
0.899
0.961

0.835
0.858
0.780
0.758

0.902
0.938
0.834
0.795

0.933
0.941
0.882
0.922

0.927
0.930
0.813
0.898

0.865
0.884
0.811
0.770

0.138
0.236
0.949
0.489

0.319
0.383
1.165
1.506

increases negligibly, while disadvantages mentioned in 2.3 increases
more tangibly. Therefore, the value of SC = 40 can be justiﬁed
quantitatively.

effectiveness of combining texture addition with several leading optical
ﬂow methods on multiple real and animation sequences. Analysis of
optical ﬂow convergence supported the resulting advantages from a
mathematical perspective.

4. Limitations and future work
Acknowledgment
The algorithm proposed herein is the ﬁrst step toward solving
the problem of poor texture in optical ﬂow computation using a
novel and simple, yet effective approach. As mentioned earlier, our
proposed method has shortcomings, which include providing an adaptive threshold for binarizing the image differencing results and a moving synthetic texture to poorly-textured blocks within the foreground.
We have attempted to generate an accurate moving texture that
does not produce erroneous ﬂow and tested multiple strategies for
this problem, but they are far from a ﬁnal solution. For instance, adding
a moving texture to non-textured moving regions using displacement of
the feature points and interpolation for any arbitrary pixel within the
moving blocks cannot be accurately performed, since rigid and nonrigid motions require different types of texture interpolations and we
do not know the motion type a-priori.
The other option considered was to use the optical ﬂow vector ﬁeld
itself for ﬁnding the location of the corresponding pixels in the second
frame, which has two limitations. First, optical ﬂow does not only belong to one of the frames and magnitudes have sensible errors which
leads to production of the synthetic texture in the wrong location;
second, it requires optical ﬂow computation twice, which neutralizes
the advantage of reduction in the computation time provided by our
algorithm. We have considered these problems as future work.
5. Conclusions
In this study, we have investigated the effects of adding synthetic
texture to images with poorly-textured regions on the optical ﬂow
performance, namely the accuracy of foreground boundary detection
and computation time. It is demonstrated that texture addition leads
to important advantages, including creation of sharp motion boundaries, more accurate capture of object contour and size, avoidance or
mitigation of object merging, and reduction in the computation time.
Well-known quantitative metrics have been employed to evaluate the

Fig. 13. Effect of synthetic texture intensity, SC on the foreground detection accuracy, measured by Fα.

The authors would like to thank Dr. Yanqiu Wang with the Department of Mathematics, Oklahoma State University, for her assistance
with the analysis of the convergence for the optical ﬂow computation,
and Dr. Damon Chandler with the Department of Electrical and Computer Engineering, Oklahoma State University, for his assistance with the
quantitative measures for optical ﬂow performance.
References
[1] B.K.P. Horn, B.G. Schunck, Determining optical ﬂow, Artif. Intell. 17 (1981) 185–203.
[2] M. Werlberger, T. Pock, H. Bischof, Motion estimation with non-local total variation
regularization, PROC CVPR IEEE, 2010.
[3] H. Haussecker, D. Fleet, Computing optical ﬂow with physical models of brightness
variation, IEEE TPAMI, vol. 23 2001, pp. 661–673.
[4] T. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical ﬂow estimation
based on a theory for warping, in: T. Pajdla, J(.G.). Matas (Eds.), ECCV, LNCS, vol.
3024, Springer, Heidelberg, 2004.
[5] A. Bruhn, J. Weickert, C. Schnörr, Lucas/Kanade Meets Horn/Schunck: Combining
Local and Global Optic Flow Methods, IJCV, vol. 61 2005, pp. 211–231.
[6] T. Brox, J. Malik, Large displacement optical ﬂow: descriptor matching in variational
motion estimation, IEEE Trans. Pattern Anal. Mach. Intell. 33 (3) (2011) 500–513.
[7] H.-H. Nagel, W. Enkelmann, An investigation of smoothness constraints for the estimation of displacement vector ﬁelds from image sequences, PAMI 8 (5) (1986) 565–593.
[8] L. Alvarez, R. Deriche, T. Papadopoulo, J. Sanchez, IJCV, vol. 75 2007, pp. 371–385.
[9] M.J. Black, Intensity and motion for incremental segmentation and tracking over
long image sequences, in: G. Sandini (Ed.),ECCV, LNCS 1992, pp. 485–493.
[10] D.J. Fleet, M.J. Black, O. Nestares, Bayesian inference of visual motion boundaries,
Exploring Artiﬁcial Intelligence in the New Millennium, Morgan Kaufmann Pub.,
San Francisco, 2002. 139–174.
[11] Y. Lei, L. Jinzong, L. Dongdong, Discontinuity-preserving optical ﬂow algorithm,
Elsevier J. Syst. Eng. Electron. 18 (2) (2007) 347–354.
[12] T. Nir, A. Bruckstein, R. Kimmel, Overparameterized variational optical ﬂow, Int. J.
Comput. Vis. 76 (2) (2008) 205–216.
[13] D. Sun, S. Roth, M.J. Black, Secrets of optical ﬂow estimation and their principles,
Proc CVPR IEEE 2010, pp. 2432–2439.
[14] P.J. Huber, Robust regression: asymptotics, conjectures and Monte Carlo, Ann. Stat. 1
(5) (1973) 799–821.
[15] D. Shulman, J.Y. Hervé, Regularization of discontinuous ﬂow ﬁelds, Proc Workshop
on Vis Motion 1989, pp. 81–86.
[16] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers, H. Bischof, Anisotropic
Huber-L1 optical ﬂow, BMVC, 2009.
[17] H. Zimmer, A. Bruhn, J. Weickert, L. Valgaerts, A. Salgado, B. Rosenhahn, H.P. Seidel,
Complementary optic ﬂow, Proc Seventh Lect Notes Comput Sc, vol. 5681 of LNCS
2009, pp. 214–223.
[18] L. Kun, Y. Wang, Oriented smoothness aided harmonic gradient vector ﬂow for active contours, Proceedings of the 2nd International Cong Img, Signal 2009, pp. 1–5.
[19] J. Zhao, Y. Wang, H. Wang, Optical ﬂow with harmonic constraint and oriented
smoothness, Sixth International Conference on Image and Graphics 2011, pp. 94–99.
[20] G. Aubert, R. Deriche, P. Kornprobst, Computing optical ﬂow via variational techniques, SIAM J. Appl. Math. 60 (1999) 156–182.
[21] K. Laws, Textured Image Segmentation(Ph.D. Dissertation) University of Southern
California, 1980.
[22] M. Hubert, E. Vandervieren, An adjusted boxplot for skewed distributions, Comput.
Stat. Data Anal. 52 (12) (2008) 5186–5201.
[23] A. Mitiche, A. Mansouri, On convergence of the Horn and Schunck optical-ﬂow estimation method, IEEE Trans. Image Process. 13 (6) (2004) 848–852.
[24] L.N. TRef5then, D. Bau, Numerical Linear Algebra, SIAM, 1997.
[25] R.J. LeVeque, Finite Difference Methods for Ordinary and Partial Differential Equations, Steady State and Time Dependent Problems, SIAM, 2007.
[26] J.F. Queiró, Partial spectra of sums of Hermitian matrices, Mathematical papers in
honour of Eduardo Marques de SÃ¡ 392006.

M. Andalibi et al. / Image and Vision Computing 40 (2015) 1–15
[27] C. Vu, D. Chandler, Main subject detection via adaptive feature reﬁnement,
J. Electron. Imaging 20 (1) (2011) 013.
[28] T. Liu, J. Sun, N.N. Zheng, X. Tang, H.Y. Shum, Learning to detect a salient object,
Computer Vision and Pattern Recognition, CVPR 07, IEEE Conference, Minneapolis,
Minnesota, USA 2007, p. 18.

15

[29] J. Freixenet, X. Munoz, D. Raba, J. Marti, X. Cuﬁ, Yet another survey on image segmentation: region and boundary information integration, ECCV 02: Proceedings of
the 7th European Conference on Computer Vision—Part III, Springer-Verlag,
London 2002, p. 408422.

