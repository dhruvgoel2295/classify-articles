Image and Vision Computing 40 (2015) 28–37

Contents lists available at ScienceDirect

Image and Vision Computing
journal homepage: www.elsevier.com/locate/imavis

Fusion of a panoramic camera and 2D laser scanner data for constrained
bundle adjustment in GPS-denied environments☆
Yun Shi a,c, Shunping Ji b,c,⁎, Xiaowei Shao d, Peng Yang a, Wenbin Wu a, Zhongchao Shi e, Ryosuke Shibasaki c
a

Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, China
School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China
Center for Spatial Information Science, University of Tokyo, Chiba, Japan
d
Earth Observation Data Integration and Fusion Research Initiative, University of Tokyo, Tokyo 153-8505, Japan
e
Department of Restoration Ecology and Built Environment, Faculty of Environmental Studies, Tokyo City University, 3-3-1 Ushikubo-nishi, Tuzuki-ku, Yokohama, Kanagawa 224-8551, Japan
b
c

a r t i c l e

i n f o

Article history:
Received 15 November 2013
Received in revised form 24 April 2015
Accepted 10 June 2015
Available online 18 June 2015
Keywords:
Panoramic camera
Laser
Sensor fusion
Constrained bundle adjustment

a b s t r a c t
Pose estimation is a key concern in 3D urban surveying, mapping, and navigation. Although Global Positioning
System (GPS) technologies can be used to estimate a robot's or vehicle's pose, there are many urban environments in which GPS functions poorly or not at all. For these situations, we offer a novel approach based on a careful fusion of panoramic camera data and 2D laser scanner input. First, a Constrained Bundle Adjustment (CBA) is
introduced to handle scale and loop closure constraints. The fusion of a panoramic image series and laser data
then enables an accurate scale to be estimated and loop closures detected. Finally, the two geometric constraints
are enforced on the global CBA solution, which in turn produces a robust pose estimate. Experiments show that
the proposed method is practicable and more accurate than vision-only methods, with an average error of just
0.2 m in the horizontal plane over a 580 m trajectory.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction and related work
Localization and mapping, and especially their integration in
SLAM (Simultaneous Localization and Mapping) and SFM (Structure
from Motion), represent a core challenge for many applications in
robotics and computer vision. Related research dates back to the
early photogrammetry work of the last century [1]. In recent years,
the rapid growth in computing power and algorithmic sophistication
has yielded great progress, as well as a number of successful systems
[2,3]. At heart, the challenge consists of an estimation problem that
can be solved using two distinct approaches [4]: ﬁltering methods,
which marginalize past poses and summarize information gained
over time via a probability distribution [3,5,6], and bundle adjustment
(BA) methods, which retain a global optimization of the entire vision
sequence [7–11], and emphasize local optimization with respect to
both accuracy and efﬁciency [12,13].
Whereas ﬁltering methods have generally been used to estimate
robot motion in real time, BA methods have been used for the global
or local optimization of 3D bundles extracted from a sequence of images. Comparing the accuracy of the two methods, [4] concluded that

☆ This paper has been recommended for acceptance by Enrique Dunn.
⁎ Corresponding author. Tel.: +86 13554057323; fax: +86 68778086.
E-mail addresses: shiyun@caas.cn (Y. Shi), jishunping@whu.edu.cn (S. Ji),
shaoxw@iis.u-tokyo.ac.jp (X. Shao), Yangpeng@caas.cn (P. Yang), Wuwenbin@caas.cn
(W. Wu), shizc@tcu.ac.jp (Z. Shi), shiba@csis.u-tokyo.ac.jp (R. Shibasaki).

http://dx.doi.org/10.1016/j.imavis.2015.06.002
0262-8856/© 2015 Elsevier B.V. All rights reserved.

BA outperforms ﬁltering. In this paper, we focus on accurate localization
and mapping (full SLAM) rather than autonomous navigation. We adopt
BA methods to improve the performance of constrained bundle adjustment (CBA) with a scale factor and loop closure, thus achieving more accurate localization based on fused imagery and laser data.
In monocular SLAM/SFM applications, the translation between two
frames is only recoverable up to a scale factor. Thus, a more accurate estimate of the scale will yield better spatial alignment of consecutive data
frames with visual odometry [14]. Note that the scale factor can be observed/estimated for any two frames captured by a stereoscopic system;
in monoscopic systems, there is no direct way to measure feature
depths or odometry, resulting in compounded error drift [5]. To address
this, if we start from a physical object of known size, we can assign a
precise scale to the estimated map and motion, rather than considering
the scale as a completely unknown degree of freedom [15]. If the target
sizes are not available (as in many real-world applications, such as
vehicle-borne urban mobile mapping), GPS can be substituted as a constraint on the error drift, as it scales directly to the real world [14,16].
Although the spatial alignment of consecutive images can be calculated using scaled monoscopic odometry, camera pose error (and especially
attitude error) will still compound the drift in monoscopic systems. In
GPS-denied systems, a globally consistent alignment of the complete
data sequence under a closed loop constraint becomes a convenient
and efﬁcient solution to prevent drift [17]. However, the loop closure detection mechanism must be very robust, because false-positive loop closures will cause corruption in most mapping systems [3].

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

The state-of-the-art detection algorithm is FAB-MAP [18], a rigorous probabilistic approach to image matching based on a ‘visual
bag-of-words’ model. Unfortunately, as purely appearance-based
methods, they are unable to distinguish between different locations
or landmarks that have a similar appearance [3]. A local spatial geometric constraint can be added to FAB-MAP to give FAB-MAP 3D,
which uses 3D Delaunay tessellation to accelerate the comparison
of spatial layouts modeled as random graphs [19]. The combination
of a geometric comparison and appearance matching reduces the
likelihood of perceptual aliasing, though some (especially urban)
environments may exhibit repetitive locations that are identical in
both structure and appearance. Using camera motion sequences to
calculate the probability of loop closure through the location prior,
the CAT-SLAM [3] algorithm determines more correct loop closures
than the ﬁrst generation of FAB-MAP. Although such combination
methods signiﬁcantly reduce the ambiguity of loop closure detection, CAT-SLAM must revisit a previously traversed trajectory with
the same orientation to detect loops, and can seldom deal with
cases where the orientation is different [20]. Although FAB-MAP
can recognize loop closures in a new direction, the core of its
appearance-based matching system (the state-of-the-art SIFT descriptor) suffers a rapid decrease in matching rate owing to larger
viewpoint angles [21]. This may result in a low loop detection rate.
Laser scanning offers another means of improving loop closure
detection. In [17], point clouds from each end of the loop closure
were captured from short segments of a vehicle's motion. These
were used to determine the loop closure through iterative closest
point (ICP) approximation. Unfortunately, iterative approximation
based only on laser point clouds is not guaranteed to converge to a
solution. There are also practical limitations of using laser points
for loop closure detection, such as when dynamic objects such as
moving people cause incorrect point clouds to appear in the scene
[19]. To solve the spatial aliasing problem of laser-only methods
and detect more robust loop closures, the image texture can be
used to compensate the laser data.
In this paper, we propose a new method that fuses panoramic images and laser point cloud sensor input. Our approach uses motion
sequences for scale estimation and loop closure detection, and
these are in turn used by CBA for global optimization. Similar studies
on sensor fusion have been involved in visual-SLAM and aerial triangulation. Using a local BA strategy, [22] compared three weighting
methods for real-time visual-SLAM combined with an odometer or
gyroscope. In photogrammetry, GPS, inertial measurement unit
(IMU), and camera data are often integrated under the uniﬁed global
BA framework to obtain accurate georeferencing [23]. However, to
the best of our knowledge, there is no efﬁcient scale estimation

and loop closure detection method that fuses panoramic image sequences and laser point clouds. In our system, a panoramic camera
and slope-trawling laser scanner are integrated as shown in
Fig. 1(a); the estimated scale and the detected loop closures then
constrain the BA global optimization according to the ﬂowchart in
Fig. 1(b). Note that this proposal extends a previous work [16] involving data association and the spatial alignment of consecutive
image frames.
The remainder of this paper is structured as follows. Section 2
outlines the fundamental theory of BA, and then presents the CBA algorithm for use with the estimated scale factor and detected loop
closure. Section 3 describes our algorithm for scale estimation
based on sensor fusion, and discusses the advantages of this method
over camera-only methods. Our loop closure detection mechanism is
introduced in Section 4, emphasizing its insensitivity to changes in
camera view angle. Section 5 details our use of high-accuracy GPS/
IMU positioning to obtain the ground-truth for the experimental
data. Using this as a basis for comparison, we demonstrate that our
sensor fusion method is signiﬁcantly more accurate than techniques
that use only a panoramic camera, and gives results that are very
close to the ground-truth.
2. Constrained bundle adjustment
2.1. Pose estimation with bundle adjustment
In this section, we brieﬂy review a classic BA solution for nonlinear global pose estimation. Global BA is typically conditioned by all
ray observations in an ofﬂine procedure that emphasizes accuracy
over efﬁciency. In this form, BA has been applied to many fullSLAM problems.
The following notation and the graph shown in Fig. 2 are used to
represent the classical BA problem described in [24,25]. Here, the estimate of the sequence of 6D poses is denoted by X = [x1; … ; xn],
where xi is a 6 × 1 vector indicating the i-th pose. The vertical concatenation of these vectors forms the pose vector X6n × 1. Estimates of
3D landmarks are given by M = [m1; … ; mq], where M3q × 1 is the
landmark vector deﬁned in a similar way. When the i-th landmark
is observed from the j-th pose, the observation is denoted as zij .
The residual of observations is then deﬁned as
À
Á
li j ¼ zi j −h x j ; mi

Panoramic
C

ð1Þ

where h(∙) denotes the observation model. In this paper, h(∙) represents the rigorous sensor model of a panoramic camera described in
[16]. In general, the optimal value of the unknown parameters [X; M]

Camera
images
Slope 2D
L
S

29

Data
fusion

Data
calibration

Laser
data
Data
association

Laser-supported scale estimation
Spatial alignment of consecutive frames
Laser-supported loop closure detection
Global optimization of the complete frames
with the constrained bundle adjustment

(a)

(b)

Fig. 1. (a) Setup of integrated panoramic camera and 2D slope laser scanner. (b) Algorithm ﬂow of proposed localization system.

30

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

m4

m1

m3

m2

x2

x1

x1
x2
x3

m1
m2
m3
m4

distance between the i-th pose xi and j-th pose xj. Therefore, an additional constraint can be set in BA as follows:

x1x2 x3m1m2m3m4
AT PA

T

B PA

AT PB

1
1
min LT PL þ Ld T Pd Ld
X;M 2
2

T

B PB

ð6Þ

where Ld is the residual vector of scale constraints and Pd is the corresponding weight matrix. The problem can be converted as follows:

x3

1
1
min ðAδx þ Bδm−LÞT PðAδx þ Bδm−LÞ þ ðDδx−Ld ÞT Pd ðDδx−Ld Þ
2
2
ð7Þ

δx;δm

Fig. 2. Notation and information matrix.

can be achieved by minimizing the weighted sum of residuals, deﬁned as follows:
1
min LT PL
X;M 2

ð2Þ

where the residual vector Lr × 1 is the vertical concatenation of all
available lij and P is a symmetric positive-deﬁnite weight matrix.
It is challenging to directly solve the nonlinear problem in Eq. (1).
Hence, BA often exploits numerical optimization such as the Gauss–
Newton method. In this way, the problem in Eq. (1) can be approximately solved by iteratively calculating the increment in X and M as
follows:
1
min ðAδx þ Bδm−LÞT PðAδx þ Bδm−LÞ
δx;δm 2

AT PB
BT PB

!

!
!
T
δx
¼ AT PL :
δm
B PL

ð4Þ

In Eq. (4), the coefﬁcient matrix on the left-hand side is the information matrix (see Fig. 2). This contains two unknowns, δx and δm, and is of
very large dimension, (6n + 3q) × (6n + 3q). Fortunately, δm can be
eliminated by applying the Schur complement trick:



−1

−1
BT PA δx ¼ AT PL−AT PB BT PB
BT PL:
AT PA−AT PB BT PB

AT PA þ DT Pd D AT PB
BT PA
BT PB

!

!
!
T
T
δx
¼ A PL þT D Pd Ld
δm
B PL

ð8Þ

which can be reformulated as:



‐1
AT PA þ DT Pd D−AT PB BT PB BT PA δx

‐1
¼ AT PL þ CT Pd Ld −AT PB BT PB BT PL:

ð9Þ

ð3Þ

where δx, δm are the incremental updates of X, M, and A, B are the
corresponding Jacobian matrices, respectively.
The explicit solution is easily obtained by calculating the derivatives,
written as:
AT PA
BT PA

where D is the Jacobian matrix with respect to X. Note that M is not involved in this constraint, and the scale observation is considered independent of the ray observations.
In the information matrix form illustrated in Fig. 3, the measured distance d23 (red) between poses x2 and x3 only impacts on the red boxes,
whose values are directly added to the original values represented by
the corresponding grey boxes. Thus, the new information matrix can
be constructed as:

ð5Þ

The solution for δx can be obtained using a classic sparse Cholesky
solver [20], and δm is then given by substituting δx into Eq. (4).

2.3. Bundle adjustment with loop closure constraint
A loop closure indicates that the same scene is observable from two
corresponding poses, xi and xj. This implies that a geometric constraint
between xi and xj can be constructed:
xi ¼ x j Á Δxi j

ð10Þ

where Δxij represents the relative pose between xi and xj, which can be
calculated using a common 3D transformation based on the i-th and j-th
poses. In this paper, the observation of relative poses can be obtained
using our sensor fusion method (see Section 4). This forms an additional
constraint in our CBA model. Considering that the loop closure
constraint is independent of the ray observations, the new CBA can be
written as:
1
1
1
min LT PL þ Ld T Pd Ld þ Lc T Pc Lc
X;M 2
2
2

ð11Þ

where Lc is the residual vector of loop closure constraints and Pc is the
corresponding weight matrix. This loop closure constraint can be

2.2. Bundle adjustment with scale constraint
CBA is a modiﬁed form of the classic BA. Additional constraints, such
as scales from an odometer, locations from a GPS receiver, or attitudes
from an IMU measurement, can be introduced to further improve the
performance of BA.
In robotics and computer vision, scale usually means the ratio of a
ground-truth distance to a corresponding distance in camera-centered
space. For simplicity, we treat the distance between two camera poses
in the absolute world coordinate system as the scale observation, because the relative distance in camera-centered space is known. In this
paper, we obtain the scale observation dij based on our sensor fusion
method (see Section 3). Ideally, it should be equal to the 3D Euclidian

m1

m4

m3
m2

x1

x2

x
x21
x3

x1 x2 x3 m1m2m3 m4

m1

m2
m3
m4

x3

Fig. 3. Introduction of a scale observation to the information matrix.

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

m1

m4

m3

x
x21

x3
x4

m2

m5 x2

x1

x5

m1

x3

m2
m3
m4
m5

x4

x5

x1 x2 x3 x4 x5m1m2m3 m4 m5

ð14Þ

where H is the camera height. If (Xa Ya Za) are the coordinates of 3D object a on the road surface, Za can be expressed as:

incorporated into the information matrix. For example, Fig. 4 shows the
loop closure between x1 and x5 (sharing the same 3D landmark m5).
This closure contributes to the original information matrix through
the additive values represented by red boxes.
Finally, considering both scale and loop closure constraints, the information matrix and corresponding constant terms can be constructed
as:
!

correct scale), as shown in Fig. 5(a). There is a simple but nonrigorous method for estimating scale using only a panoramic camera.
If we assume that a road surface is perfectly ﬂat, and that a camera
moves across it from point A to point B, the scale can be estimated by
the known camera height above the road surface. In Fig. 5(a), pose B is
estimated with the correct scale using the relative height of road surface
point a with respect to A, which should be equal to the known camera
height:
Z a ¼ −H

Fig. 4. Introduction of a loop closure constraint to the information matrix.

AT PA þ DT Pd D þ CT Pc C AT PB
BT PA
BT PB

31

!
!
T
T
T
δx
¼ A PL þ D PTd Ld þ C Pc Lc
δm
B PL
ð12Þ

Za ¼

λðt x Z 2 −t z X 2 Þ
x1 Z 2 −z1 X 2

ð15Þ

where λ is a scale parameter, T = [tx ty tz]T is the translation vector between the two poses; (x1, y1, z1) and (x2, y2, z2) are the camera coordinates of point a with respect to A and B, respectively; and R is a
rotation matrix with (X2, Y2, Z2)T = R[x2, y2, z2]T.
Eq. (14) exhibits small model errors if the road surface is bumpy.
Real road surfaces are always irregular, and a given place (usually described by a feature) not on the road surface may be misused in a
scale calculation, particularly in complex dynamic environments. This
will cause the real road point a to be mistaken as a′, and pose B as B′,
leading to big discrepancies in scale estimation.

which can be reformulated as:


3.2. Accurate scale estimation using both laser and vision data



‐1
AT PA þ DT Pd D þ CT Pc C−AT PB BT PB BT PA δx

‐1
¼ AT PL þ DT Pd Ld þ CT Pc Lc −AT PB BT PB BT PL:

ð13Þ

3. Laser-supported scale estimation
It is clear that accurate scale constraints are very important to the robustness and convergence of the least-squares adjustment for the information matrix, especially those elements on the diagonal (see Fig. 3). In
this section, we present a method that improves on the common method of scale estimation by fusing panoramic image data and laser point
cloud data.
3.1. Scale estimation by panoramic camera alone
Data association [26–30] enables all relative pose components except the scale to be determined for an entire trajectory. This means
that, for a given scale, the otherwise correct pose B may lie anywhere
along the line AB (with A corresponding to the correct pose, at the

Pose with
incorrect scale
Camera
Pose

A

If road surface
is flat, and
camera height
is known, the
scale can be
calculated.

B

a

(a)

Our proposed method of scale estimation uses a laser point cloud to
calculate the height of point a relative to position A. Even when the road
surface is uneven, this is a rigorous computation, and a dense point
cloud further reduces the risk of taking the measurement from a false
road point a′. In Fig. 5(b), the road surface is represented by the red
3D laser points, whose coordinates are based on camera pose A. The intersection of this surface with ray Aa′ determines the correct road surface point a, and thus the correct scale for correct pose B.
Da ¼ d

In Eq. (15), d is the distance observation measured by laser; Da
is the distance between A and a, and is the square root of
(Xa ^2 + Y a^2 + Z a^2), where (X a Y a Z a) are functions of the scale
parameter λ and can be obtained by 3D intersection. Clearly, d is
an absolute observation, and is unaffected by the uneven surface
of the road.
According to Eq. (15), each road surface point determines one value
of λ. And redundant observations provided by many surface points can
contribute a more robust estimate. A histogram voting method is

Pose with
incorrect scale

Pose with
Correct scale

B

Camera
Pose

A

a

ð15Þ

m

Road
surface

B

B

Laser range
--Metric
--Scale invariant
points of laser scan
of road surface

a

Pose with
right scale

a

m

Road
surface

(b)

Fig. 5. Scale estimation methods under examination: (a) using only a panoramic camera; (b) using sensor fusion of a panoramic camera and a 2D laser scanner.

32

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

Fig. 6. Difﬁculty of loop closure detection by panoramic camera alone.

introduced: every observation votes once for a certain scale, and the histogram bin with maximum number of votes indicate the optimal
estimate.

4. Laser-supported loop closure detection
4.1. Problem of loop closure detection by panoramic camera alone
Using scaled monoscopic odometry, accurate camera pose estimates
can be achieved with GPS-supported BA [16]. In GPS-denied environments, however, pose estimation errors will become compounded and
drift away from the accurate state. To address this, it is possible to form
a globally consistent alignment (i.e., global optimization) of the entire
data sequence under a closed loop constraint [17].

Although recovering the relative pose of a loop from images alone can
yield excellent results, in certain loop closure cases—particularly those
involving cameras with very different view direction angles—proper
recovery may become infeasible. Variations in viewing angles cause
deformations that cannot be reﬂected by a single planar similarity
transformation, as the depth of landmarks in the scene is unknown
and many have strong 3D structures. Conventional visual-SLAM systems, such as FAB-MAP and CAT-SLAM, both employ SIFT descriptors
for loop detection, resulting in noticeable performance degradations
as the viewing angle increases in the 2D scene [21]. The left-hand
image in Fig. 6 was taken while traversing east-to-west through a region, whereas the right-hand image was captured while traversing
west-to-east. SIFT performs poorly in this case, because landmarks
at different distances from different viewing angles cause more complex geometric deformations.

Fig. 7. Proposed sensor fusion method for loop closure detection.

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

33

Fig. 8. Comparison of scale estimation results for the camera-only method (blue) and our sensor fusion method (red). The horizontal axis represents the image sequence, while the vertical axis
reﬂects the difference in height (in cm) from the ground-truth.

(a)

(b)

(c)

(d)

(e)
Fig. 9. BA results using estimated scale constraints: the green line indicates the ground-truth; the red lines in (a) indicate the route produced when no scale constraint was used; the blue
lines in (b) and (c) indicate the resulting routes using BA with scale constraints estimated by the camera-only approach and our fusion-based approach, respectively; (d) and (e) show the
horizontal accuracy and heading accuracy for bundle adjustment with scale constraints estimated by the camera-only approach and our fusion-based approach, respectively.

34

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

simple alignment of directions in the corresponding images. This simpliﬁcation is especially helpful in complicated scenes, for which the proposed method can detect many more loops than purely appearancebased methods.
The local ortho-images are generated as follows. First, local motion
sequences from data associations of panoramic images (i.e., local
odometry results) are used to generate the 3D laser point cloud
(shown as green dots in Fig. 7). Second, the 3D point cloud is used as
road surface elevation data to generate a local digital elevation model
(DEM) grid. Third, the local ortho-image is generated by mapping
color information from the panoramic images to the local DEM grid,
pixel by pixel (producing the images on the right in Fig. 7). These local
ortho-images with the same scale make loop closure detection much
more straightforward, and the detected features are then matched

4.2. Laser-supported loop closure detection
In [17], point clouds (trawled by slope lasers) were generated from
short segments of the vehicle's motion around each end of the loop closure. These could be used in an ICP algorithm to determine the loop closure. However, this iterative method based only on the point cloud is
not guaranteed to converge to a solution. For more robust and efﬁcient
loop closure detection, we propose a fusion method that integrates an
image sequence with point cloud data. The core concept of our approach
is to detect loop closure in images that have been orthographized for the
fused data, as opposed to the original, direction-sensitive images. The
main advantage of our “ortho-images” is that they are all top-down
views, thus obviating the complex and/or uncertain geometric transformations introduced by landmarks of unknown depth, and allowing for

(a)

(b)

(c)

(d)

(e)
Fig. 10. Results of bundle adjustment using scale and loop closure constraints. Green line indicates the ground-truth; the blue line in (a) indicates the result when only image-based scale
and loop closure constraints are used; the blue lines in (b) indicate the result when fusion-based scale and image-based loop closure are used; (c) represents the results of CBA using fusion-based scale and fusion-based loop closure; (d) and (e) show horizontal and heading accuracy for bundle adjustment using different constraints of (a)–(c), respectively.

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

35

accuracy of our fusion method in estimating scale, as an input to scaleconstrained CBA. We then tested the reliability of our method in detecting loop closures, and examined the overall accuracy of CBA using both
scale and loop closure constraints. Finally, we analyzed our resulting
pose estimations relative to ground-truth data.

using a rotation-invariant intensity correlation method [31] to determine whether the current observed image is new or simply a loop
closure.
5. Experiments
5.1. Test design

5.2. Sensor fusion for scale estimation and BA with scale constraint
To test our proposed sensor fusion-based method, we used the system setup shown in Fig. 1 to capture panoramic image sequences with a
Ladybug 3 camera and laser range data over a 580 m trajectory. Data
preprocessing, such as camera calibration and laser scanner setup, is described in [32–35]. The data association of panoramic image sequences
(tie-point extraction, sequential relative orientation, block bundle adjustment, and approximate alignment to real world coordinates) was
conducted as in [16]. Hence, our focus in this paper is on the sensor fusion performance, as opposed to visual-SLAM performance.
To obtain the ground-truth datum in our test case, highly accurate
(within several centimeters) pose data were acquired using the
Applanix POS/AV system. The GPS/IMU observations of the ﬁrst frame
were utilized to georeference the image sequence, while the remaining
data were used exclusively as a point of reference for measuring the localization accuracy of our proposed method. We began by testing the

Fig. 8 compares the scale estimation using only panoramic imagery to that using our sensor fusion approach. Our proposed method
was generally more accurate, with a root mean square error (RMSE)
of 1.6 cm, which should be sufﬁcient for robust bundle adjustment.
The RMSE of the camera-only method was 2.8 cm, although the
many errors of around ± 8 cm will reduce the convergence speed
of a BA solution.
Next, we examined the effectiveness of bundle adjustment using our
estimated scale constraints. Fig. 9(d) shows that the camera-only approach produced inferior localization results, with an average error of
3.99 m in the horizontal plane compared with 2.88 m for our fusionbased method. Fig. 9(e) shows similar results, but in this case the improvements are less obvious due to cumulative errors (indicating the
need for a loop closure constraint).

(a)

(b)

(c)
Fig. 11. CBA results for estimating elevation: in (a), the horizontal axis represents the image sequence, while the vertical axis reﬂects the difference in height (m) from the ground-truth;
the blue line indicates the result for the camera-only approach, the red line the results for the fusion approach, and the green line for fusion with elevation control; in (b), the green line
indicates the estimated route, and the blue the ground-truth, in 3D; in (c), the green line indicates the estimated route using the additional elevation control point constraint.

36

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

be improved using this data, reducing the error range from several
meters to a single meter.
The weights of the three terms (ray observations, scale estimations,
and loop closure) are very important for the correct convergence of a
global BA solution. Weights can be determined by the accuracy ratios
between the three observations. In our tests, the average reprojection
error was about 2 pixels (equivalent to 4 cm in a panoramic spherical
surface of 20 m radius), and the scale estimation error given by data fusion was about 2 cm with reference to a 1 m baseline. Setting the weight
of ray observations to 1.0, the weight of scale observations is set to 4.0
accordingly. For the loop closure, we regard the weight to be the same
as for the rays in the local area. However, we also test some larger
loop closure weights to examine whether a drifted trajectory can be
pulled back to the correct path. Fig. 12(a) shows that a scale weight of
4.0 gives the most accurate results. Fig. 12(b) shows that a loop closure
weight of 1.0 is slightly more accurate than weights of 10.0 and 100.0,
although there is considerably more volatility at this weight. In conclusion, CBA appears to be relatively insensitive to the loop closure weight,
whereas it needs a suitable scale weight to guarantee good localization
accuracy.

5.3. Sensor fusion for loop closure detection and bundle adjustment with
both constraints
Fig. 10 shows the BA results using different scale and loop closure constraints. Fig. 10(a) shows the results with image-based
scale and image-based loop closure, where the 2D localization accuracy reaches 0.7 m with accurate laser-supported scale and six loop
closure points. Fig. 10(b) illustrates the better accuracy obtained
using laser-supported scale estimation. Fig. 10(c) shows the localization results when both fusion-based scale and loop closure are
applied, giving a position accuracy that is slightly better than that
in Fig. 10(b). Note that the camera-only approach using similarity
measurements of SIFT features, as in FAB-MAP, detected only six
loop closures (Fig. 10(b)), whereas our image/laser fusion approach
detected 25 loop closures (Fig. 10(c)), four times more than the
appearance-based method. Further, note that Fig. 10(d) and
(e) illustrate that the greater abundance of loop closure constraints
improves the CBA accuracy by an average of 0.2 m in the horizontal
plane. Although the length of our dataset was too small to demonstrate the greater efﬁciency of our method, we believe that these results clearly indicate the superior robustness and accuracy of the
fusion-based approach.
Though our method can accurately constrain localization results
in 2D, further investigations indicate weaker performance in 3D estimation. Indeed, Fig. 11 suggests that the proposed method performs
no better than the camera-only method with respect to elevation estimates. This is hardly surprising, because scale and loop closure constraints are distributed according to horizontal routing, and provide
very weak sampling of elevation. For example, although △ x i,j in
Eq. (8) reﬂects incremental changes in elevation, these are approximately 0 because of the relative ﬂatness of the road surface, and
thus make no signiﬁcant contribution to the information matrix in
Eq. (11). To address this deﬁciency, we performed an additional experiment using elevation control point data, the acquisition of
which is considerably easier than that of true 3D control point data.
The green line in Fig. 11(a) illustrates how elevation accuracy can

6. Conclusions and future work
In this paper, we proposed a new method for urban mapping and
localization based on the fusion of image and depth data combined
with motion sequences. The resulting improvements in scale estimation and loop closure detection mean that we can constrain the bundle adjustment for the global optimization of sensor pose estimation
in GPS-denied urban environments. The resulting robust and accurate localization results are close to the ground-truth. Experiments
demonstrated that our method is practicable and feasible for most
urban localization or mapping projects, especially where elevation
accuracy is not a strict demand. The main innovation of our method
is the fusion of panoramic camera images and 2D laser scanner
input to constrain the bundle adjustment.

(a)

(b)
Fig. 12. (a) Scale weight of 4.0 (red line) is ideal and most accurate; weights of 40 (green line) and 0.4 (blue line) both cause bigger errors. (b) Loop closure weight of 1.0 is most accurate
(red line); weights of 10.0 (blue line) and 100.0 (green) give similar accuracy.

Y. Shi et al. / Image and Vision Computing 40 (2015) 28–37

In future work, we will focus on achieving greater accuracy and
robustness in elevation accuracy. To accomplish this, additional georeferenced data set, such as aerial images or digital maps, will be integrated into the current panoramic/laser data, acting as adequate inputs
for an accurate 6-D CBA solution.
Acknowledgments
This work was jointly supported by the National Key Basic Research
Program of China (2012CB719902), the National Natural Science Foundation of China (41301365, 41471288 and 61403285), and the National
High-Tech R&D Program of China (2015AA124001).
References
[1] E. Hammer, Historical development of photogrammetry, justiﬁcation of its usage in
measurement and construction, Petermanns Mitt. 59 (1913) 261, 261.
[2] M. Milford, G. Wyeth, D. Prasser, RatSLAM: a hippocampal model for simultaneous
localization and mapping, IEEE Int. Conf. Robot. Autom 2004, pp. 403–408.
[3] W. Maddern, M. Milford, G. Wyeth, CAT-SLAM: probabilistic localisation and mapping using a continuous appearance-based trajectory, Int. J. Robot. Res. 31 (2012)
429–451, http://dx.doi.org/10.1177/0278364912438273.
[4] H. Strasdat, J.M.M. Montiel, A.J. Davison, Visual SLAM: why ﬁlter? Image Vis.
Comput. 30 (2012) 65–77, http://dx.doi.org/10.1016/j.imavis.2012.02.009.
[5] A. Chiuso, P. Favaro, Structure from motion causally integrated over time, IEEE
Trans. Pattern Anal. Mach. Intell. 24 (2002) 523–535.
[6] J. Solà, T. Vidal-Calleja, J. Civera, J.M.M. Montiel, Impact of landmark parametrization
on monocular EKF-SLAM with points and lines, Int. J. Comput. Vis. 97 (2011)
339–368, http://dx.doi.org/10.1007/s11263-011-0492-5.
[7] K. Konolige, M. Agrawal, FrameSLAM: from bundle adjustment to real-time visual
mapping, Robot. IEEE Trans. 24 (2008) 1066–1077, http://dx.doi.org/10.1109/TRO.
2008.2004832.
[8] F. Dellaert, M. Kaess, Square root SAM: simultaneous localization and mapping via
square root information smoothing, Int. J. Robot. Res. 25 (2006) 1181–1203,
http://dx.doi.org/10.1177/0278364906072768.
[9] G. Sibley, C. Mei, I. Reid, P. Newman, Vast-scale outdoor navigation using adaptive
relative bundle adjustment, Int. J. Robot. Res. 29 (2010) 958–980, http://dx.doi.
org/10.1177/0278364910369268.
[10] S. Thrun, Simultaneous localization and mapping with sparse extended information ﬁlters, Int. J. Robot. Res. 23 (2004) 693–716, http://dx.doi.org/10.
1177/0278364904045479.
[11] S. Thrun, M. Montemerlo, The GraphSLAM algorithm with applications to largescale mapping of urban structures, Int. J. Robot. Res. 25 (2006) 403–429, http://dx.
doi.org/10.1177/0278364906065387.
[12] E. Mouragnon, M. Lhuillier, M. Dhome, F. Dekeyser, P. Sayd, Real time localization
and 3D reconstruction, 2006 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit,
1, 2006http://dx.doi.org/10.1109/CVPR.2006.236.
[13] G. Klein, D. Murray, Parallel tracking and mapping for small AR workspaces, 2007
6th IEEE ACM Int. Symp. Mix. Augment. Real, 2007.
[14] D. Dusha, L. Mejias, Error analysis and attitude observability of a monocular GPS/visual odometry integrated navigation ﬁlter, Int. J. Robot. Res. 31 (2012) 714–737,
http://dx.doi.org/10.1177/0278364911433777.

37

[15] A.J. Davison, I.D. Reid, N.D. Molton, O. Stasse, MonoSLAM: real-time single camera
SLAM, IEEE Trans. Pattern Anal. Mach. Intell. 29 (2007) 1052–1067, http://dx.doi.
org/10.1109/TPAMI.2007.1049.
[16] Y. Shi, S. Ji, Z. Shi, Y. Duan, R. Shibasaki, GPS-supported visual SLAM with a rigorous
sensor model for a panoramic camera in outdoor environments, Sensors 13 (2012)
119–136, http://dx.doi.org/10.3390/s130100119.
[17] P. Newman, G. Sibley, M. Smith, M. Cummins, A. Harrison, C. Mei, et al., Navigating,
recognizing and describing urban spaces with vision and lasers, Int. J. Robot. Res. 28
(2009) 1406–1433, http://dx.doi.org/10.1177/0278364909341483.
[18] M. Cummins, P. Newman, FAB-MAP: probabilistic localization and mapping in the
space of appearance, Int. J. Robot. Res. 27 (2008) 647–665, http://dx.doi.org/10.
1177/0278364908090961.
[19] R. Paul, P. Newman, FAB-MAP 3D: topological mapping with spatial and visual appearance, IEEE Int. Conf. Robot. Autom, IEEE 2010, pp. 2649–2656, http://dx.doi.
org/10.1109/ROBOT.2010.5509587.
[20] W. Maddern, M. Milford, G. Wyeth, Continuous appearance-based trajectory slam,
Proc. 2011 IEEE Int. Conf. Robot. Autom., Shanghai 2011, pp. 3595–3600.
[21] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, et al.,
A comparison of afﬁne region detectors, Int. J. Comput. Vis. 65 (2005) 43–72, http://
dx.doi.org/10.1007/s11263-005-3848-x.
[22] J. Michot, A. Bartoli, F. Gaspard, Bi-Objective Bundle Adjustment with Application to
Multi-Sensor Slam, 3DPVT'10. 3025, 2010.
[23] L. Hinsken, S. Miller, U. Tempelmann, R. Uebbing, A.S. Walker, Triangulation of LH
systems ADS40 imagery using Orima GPS/IMU, Int. Arch. Photogramm. Remote.
Sens. Spat. Inf. Sci. 34 (2002) 156–162.
[24] C. McGlone, E. Mikhail, J. Bethel, Manual of Photogrammetry, American Society for
Photogrammetry and Remote Sensing, Bethesda, MD, 2004.
[25] S. Agarwal, N. Snavely, S.M. Seitz, R. Szeliski, Bundle adjustment in the large,
Comput. Vision–ECCV 2010, 63122010. 29–42, http://dx.doi.org/10.1007/978-3642-15552-9.
[26] A. Ramisa, A. Tapus, D. Aldavert, R. Toledo, R. Lopez de Mantaras, Robust visionbased robot localization using combinations of local feature region detectors,
Auton. Robot. 27 (2009) 373–385, http://dx.doi.org/10.1007/s10514-009-9136-9.
[27] O. Booij, Z. Zivkovic, B. Kröse, Efﬁcient data association for view based SLAM using
connected dominating sets, Robot. Auton. Syst. 57 (2009) 1225–1234, http://dx.
doi.org/10.1016/j.robot.2009.06.006.
[28] M. Bosse, R. Zlot, Map matching and data association for large-scale twodimensional laser scan-based SLAM, Int. J. Robot. Res. 27 (2008) 667–691, http://
dx.doi.org/10.1177/0278364908091366.
[29] M. Chli, A.J. Davison, Active matching for visual tracking, Robot. Auton. Syst. 57
(2009) 1173–1187, http://dx.doi.org/10.1016/j.robot.2009.07.010.
[30] A. Diosi, L. Kleeman, Fast laser scan matching using polar coordinates, Int. J. Robot.
Res. 26 (2007) 1125–1153, http://dx.doi.org/10.1177/0278364907082042.
[31] M. Brown, R. Szeliski, Multi-image feature matching using multi-scale oriented
patches, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit 2005,
pp. 510–517, http://dx.doi.org/10.1109/CVPR.2005.235.
[32] J. Park, Y. Shin, K. Park, Extracting Extrinsic Parameters of a Laser Scanner and a
Camera Using EM, ICCAS-SICE, 2009, IEEE, 2009. 5269–5272.
[33] F. Vasconcelos, J.P. Barreto, U. Nunes, A minimal solution for the extrinsic calibration
of a camera and a laser-rangeﬁnder, IEEE Trans. Pattern Anal. Mach. Intell. 34 (2012)
2097–2107, http://dx.doi.org/10.1109/TPAMI.2012.18.
[34] G. Li, Y. Liu, L. Dong, An algorithm for extrinsic parameters calibration of a camera and
a laser range ﬁnder using line features, in: IEEE/RSJ Int. Conf. Intell. Robot. Syst., Ieee,
San Diego, CA, 2007, pp. 3854–3859. http://dx.doi.org/10.1109/IROS.2007.4399041.
[35] D. Sim, Y. Kim, Detection and compression of moving objects based on new panoramic image modeling, Image Vis. Comput. 27 (2009) 1527–1539, http://dx.doi.
org/10.1016/j.imavis.2009.02.009.

