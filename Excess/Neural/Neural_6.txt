Neural Networks 71 (2015) 112–123

Contents lists available at ScienceDirect

Neural Networks
journal homepage: www.elsevier.com/locate/neunet

A new computational account of cognitive control over
reinforcement-based decision-making: Modeling of a probabilistic
learning task
Sareh Zendehrouh
School of Cognitive Sciences, Institute for Research in Fundamental Sciences (IPM), P.O. Box 19395-5746, Tehran, Iran

highlights
•
•
•
•
•

Modeling decision-making from the perspectives of dual-system and cognitive control.
The model simulates human performance on a variant of probabilistic learning task.
The model addresses existing theories about the ERN and FRN components of ERP.
The results show that the ERN is best described by the RL-ERN theory.
The FRN is best described by a hypothetical cost-conflict signal.

article

info

Article history:
Received 13 September 2014
Received in revised form 17 May 2015
Accepted 13 August 2015
Available online 20 August 2015
Keywords:
Cognitive control
Reinforcement learning
Goal-directed behavior
Dual system theory
Cost function
Probabilistic learning task

abstract
Recent work on decision-making field offers an account of dual-system theory for decision-making
process. This theory holds that this process is conducted by two main controllers: a goal-directed system
and a habitual system. In the reinforcement learning (RL) domain, the habitual behaviors are connected
with model-free methods, in which appropriate actions are learned through trial-and-error experiences.
However, goal-directed behaviors are associated with model-based methods of RL, in which actions
are selected using a model of the environment. Studies on cognitive control also suggest that during
processes like decision-making, some cortical and subcortical structures work in concert to monitor the
consequences of decisions and to adjust control according to current task demands. Here a computational
model is presented based on dual system theory and cognitive control perspective of decision-making.
The proposed model is used to simulate human performance on a variant of probabilistic learning task.
The basic proposal is that the brain implements a dual controller, while an accompanying monitoring
system detects some kinds of conflict including a hypothetical cost-conflict one. The simulation results
address existing theories about two event-related potentials, namely error related negativity (ERN) and
feedback related negativity (FRN), and explore the best account of them. Based on the results, some
testable predictions are also presented.
© 2015 Elsevier Ltd. All rights reserved.

1. Introduction
The decision-making process, by which animals and humans
select action among several alternatives, has been the subject of
active research in many disciplines. Recent work in this field has
given rise to the dual-system theory of decision making, which
states that this process is conducted by two main systems: a deliberative ‘‘goal-directed’’ system controlled by response–outcome
associations (R–O) and a relatively automatic or ‘‘habitual’’ one

E-mail addresses: szendehrouh@ipm.ir, sareh.zendehrouh@gmail.com.
http://dx.doi.org/10.1016/j.neunet.2015.08.006
0893-6080/© 2015 Elsevier Ltd. All rights reserved.

controlled by stimulus–response (S–R) mappings (Balleine & Dickinson, 1998; Daw, Niv, & Dayan, 2005; Valentin, Dickinson, &
O’Doherty, 2007). The analysis of decision-making process from
the computational perspective of reinforcement learning (RL) has
given special attention in the past decade. In RL domain, the habitual behaviors are connected with model-free methods of RL,
in which appropriate actions are learned through trial-and-error
experiences. The idea behind the existence of a model-free system in the brain stems from the resemblance between phasic increases and decreases in firing rates of midbrain dopamine (DA)
neurons and a reward prediction error (RPE) in model-free algorithms of RL (Suri, 2002). The model-free algorithms that are
appropriate tools for modeling habit-driven stimulus–response

S. Zendehrouh / Neural Networks 71 (2015) 112–123

associations, cannot account for some goal-directed behavioral observations as devaluation sensitivity or latent learning (Bornstein
& Daw, 2011). The current literature claims that goal-directed behaviors are associated with model-based methods of RL, in which
actions are selected using a cognitive map or a model of the environment (Daw et al., 2005; Gläscher, Daw, Dayan, & O’Doherty,
2010; Simon & Daw, 2011). Work by some groups suggests that
goal-directed and habitual behaviors are conducted by separate
and parallel-operating model-free and model-based systems in the
brain (Bornstein & Daw, 2011). However, others propose that these
two systems may work together and are not completely separate (Daw, Gershman, Seymour, Dayan, & Dolan, 2011; Gutkin &
Ahmed, 2012; Simon & Daw, 2011).
From the perspective of cognitive control, successful decisionmaking requires the organism to monitor the consequences of
actions and decisions and to detect the failures in performance.
Decision-making changes the state of the environment or the
organism known as outcome. The differences between observed
outcomes and expected ones are monitored by performance monitoring systems and are sent to brain areas responsible for control
and optimization of current and/or future behaviors (Ullsperger,
Danielmeier, & Jocham, 2014). Thus, a monitor-controller system is activated in such processes. The computational model of
RL-ERN theory by Holroyd and Coles (Holroyd & Coles, 2002) is
one of the influential models for such systems. This model links
the impact of the observed outcome deviations from expected ones
on neurons in the anterior cingulate cortex (ACC) to changes in
the event-related brain potentials (ERPs), e.g. the ERN1 that follow
erroneous responses and the FRN2 that follows positive/negative
outcomes. The model of the RL-ERN as a model-free algorithm uses
the dopaminergic RPE signals conveyed by mesolimbic pathways
to optimize its performance and predictions. The RL-ERN theory
holds that phasic decreases in dopamine activity disinhibit motor
neurons in the ACC, thereby producing the ERN. In reinforcement
learning (RL) terms, the ERN is generated when a negative RPE is
sent to the ACC indicating the occurrence of an event that is worse
than expected. Similarly, an FRN is elicited when an unexpected
negative feedback occurs (Holroyd, Yeung, Coles, & Cohen, 2005).
Another model, the predicted response–outcome (PRO) model by
Alexander and Brown (Alexander & Brown, 2011) which is a generalization of RL algorithms, contains a model-based part that serves
as an immediate outcome predictor to control the behavior, and a
model-free part that learns a timed prediction of the expected time
of outcome occurrence. The PRO model, which has some parts that
operate like a model-based controller, holds that medial prefrontal
cortex (mPFC) signals differences between timed outcome predictions and the actual outcomes. Although, these difference signals
resemble RPEs in the RL-ERN theory, the PRO model suggests that
such difference signals (surprise signals) are computed internally
by the mPFC (Todd, Hills, & Robbins, 2012). Specially, the model
holds that the mPFC activity represents the amount of negative surprise signal (unexpected non-occurrence of a predicted outcome).
Therefore, the ERN and FRN are the manifestation of such signaling. However, such a description for ERN generation is inaccurate,
because the actual outcome may not be determined at the time of
response generation. This is in accordance with their research in
Alexander and Brown (2011) which does not contain direct simulations of ERN component (Zendehrouh, Gharibzadeh, & Towhidkhah, 2013, 2014a). Moreover, this model is not essentially based

113

on dopamine signals (Todd et al., 2012). Another prominent model,
which does not belong to the RL category, is the model of conflict
monitoring theory. This theory suggests the existence of a conflict
monitor-controller system in the brain that monitors for the occurrence of conflict at the response level and uses this information to adjust the performance of the controller (Botvinick, Braver,
Barch, Carter, & Cohen, 2001). In this model, simulated ERN is
defined based on conflict signals that the ACC detects through
monitoring the amount of energy in the motor cortex during action selection (Yeung, Cohen, & Botvinick, 2004). This theory in its
original form cannot account for the FRN (Holroyd et al., 2005; Yeung et al., 2004). Recently, a hypothetical cost-conflict monitor has
been proposed that extends this theory and can describe the FRN
based on the conflict between expected costs of the selected action
(Zendehrouh, Gharibzadeh, & Towhidkhah, 2014b).
In this paper, a new computational model for a variant of probabilistic learning task is given. While the proposed model, based on
dual system theory of decision-making, simulates behavioral data
from this experiment, it also explores the best description for ERN
and FRN components that matches the empirical data. The simulation results show that the simulated ERN is better matched with
the RPE signals in model-free part of the proposed model consistent with the RL-ERN description for this component. However, the
amplitudes of the simulated FRN based on RPEs at the time of feedback onset are not so close to the empirical FRN amplitudes. Results
also show that the FRN is better simulated by a hypothetical costconflict monitor-controller.
2. Materials and methods
2.1. Probabilistic learning task
The probabilistic learning task or the PLT is a trial-and-error
learning task where an arbitrary visual image is presented to
the participants on each trial. The participants press one of two
buttons in response to that image and receive a feedback indicating
receiving or missing a small amount of money. Here the data
of Morris et al. (Morris, Heerey, Gold, & Holroyd, 2008; Morris,
Holroyd, Mann-Wrobel, & Gold, 2011) are simulated, wherein
each stimulus is probabilistically associated with the proper
response on either 100%, 80%, or 50% of situations. Participants are
not provided with the appropriate stimulus–response mappings.
Instead, they have to deduce them by trial and error. Each block of
the experiment consists of a new set of six stimuli. In summary,
each stimulus and its accompanying outcome belongs to one of
the following categories. (1) 100% mappings condition: The left
button is the proper response for one of the six stimuli on 100%
of trials within a block. The right button is the proper response for
another stimulus in the same way. (2) 50% mappings condition: For
two other stimuli, a random feedback was delivered. Therefore, the
participants were rewarded on 50% of the trials and penalized on
50% of the trials. (3) 80% mappings condition: for one of the two
remaining stimuli, a left button is the proper response on 80% of
the trials (valid trials) and a right button is the proper response
on 20% of the trials (invalid trials). For the other stimulus, a right
button is the appropriate response on 80% of the trials (valid)
and a left button is the appropriate on 20% of the trials (invalid)
(Nieuwenhuis et al., 2002).
2.2. Proposed model

1 Error related negativity or error negativity (ERN/Ne) is an ERP component that
begins near the time of the erroneous response and peaks about 100 ms later in
speeded response time tasks (Gehring, Goss, Coles, Meyer, & Donchin, 1993).
2 The feedback related negativity (FRN) is a negative-going component observed
230 to 330 msec following outcome presentation (Miltner, Braun, & Coles, 1997) in
gambling and trial-and-error learning tasks (Holroyd, Hajcak, & Larsen, 2006).

As mentioned earlier, both model-based and model-free
methods are used in concert with each other to simulate human
performance in the task. The structure of the model is depicted in
Fig. 1.

114

S. Zendehrouh / Neural Networks 71 (2015) 112–123

Fig. 1. The structure of the proposed model.

2.2.1. Model-based module
In the model-based method of RL domain, the decision environment is formulated as a Markov decision process. In this paper the
value iteration method is used (Sutton & Barto, 1998).
As mentioned, there are a set of six stimuli and a set of two
responses in this task. Moreover, the response–outcome or R–O
mappings include Rre (right response, rewarded), Rnr (right response, non-rewarded), Lre (left response, rewarded), and Lnr (left
response, non-rewarded). The state transition matrix is updated after feedback presentation that obeys the following equation:
T (s, a, z ) =



(1 − ϕ) T (s, a, z ) + ϕ
(1 − ϕ) T (s, a, z )

if z = s′
otherwise

(1)

where s and a are the current stimulus and selected action (response), and s′ is the state after feedback presentation indicating
a rewarded reaching state or a non-rewarded one. T is the transition matrix and 0 < ϕ < 1 is its update rate.
The value of each stimulus–response or state-action pair for the
model-based module (Q mb (s, a)) is updated as:
Q mb (s, a) =

 

T s, a, s′

r (s, a, s′ ) + γ mb V s′



 

(2)

s′

where r (s, a, s′ ) represent the received reward at the final state
s′ , after taking action a at state s. The magnitude of this reward is
coded by 1 for a rewarding and 0 for a non-rewarding feedback.
γ mb
 is the discount factor for the model-based part of the model.
V s′ is the value of the final state coded by value of +1 and
−1 indicating a rewarded and a non-rewarded reaching state,
respectively.
2.2.2. Model-free module
The model-free module works based on QV -learning that
updates both the Q - and V -functions. In QV -learning, the temporal
difference methods are used to learn the state value function V. The
Q -values also learn through these V -values using the Q -learning
algorithm (Wiering & van Hasselt, 2009).

where rt is the reward magnitude at time t. Vt shows the states
V -values at time t. γ mf and α mf are discount factor and learning
rate for V -values of the model-free module, respectively. δt is the
reward prediction error or RPE signal.
The updates of Q -values (Q mf (s, a)) after receiving feedback
are:
Q mf (s, a) = Q mf (s, a) + α ′mf (r + γ ′mf Vtf − Q mf (s, a))

(5)

where r is the reward magnitude coded as 1 or 0 for a rewarding
or a non-rewarding feedback, respectively. γ ′mf and α ′mf are the
discount factor and learning rate for the Q -values of the modelfree module. Vtf is V -value at the time of feedback identification
and is computed using Eq. (4).
2.2.3. Combinatorial block
Finally, since researchers suggest that the brain employs both
approaches (Daw et al., 2005; Gläscher et al., 2010; Lee, Shimojo,
& O’Doherty, 2014), a combinatorial block has been implemented
that combines the Q -values using a weighted average of the values
from the model-based and model-free modules. The weighting
changes exponentially over time, which Gläscher (Gläscher et al.,
2010) showed that is best fitted to their data. In is also compatible
with this fact that habits form as the training increases (Yoshida &
Seymour, 2014).
Q T (s, a) = wn Q mb (s, a) + (1 − wn )Q mf (s, a)

(6)

wn = w0 × exp(−k n)
where Q T (s, a) is the total value of the state-action pair. wn is
the weight term for trial number n. w0 and k are initial weight
and decay rate of the exponentially decaying weight function,
respectively.
2.2.4. Response generation block
Action probabilities are computed according to total value
estimates and Softmax function:
eQ (s,ai )τ (s)
Pr i = 
T
eQ (s,aj )τ (s)
T

δt = rt + γ mf Vt +1 − Vt

(3)

Vt +1 = Vt + α

(4)

mf

δt

j=L,R

(7)

S. Zendehrouh / Neural Networks 71 (2015) 112–123

115

where Q T (s, ai ) is the total value of each state-action pair and τ (s)
is the inverse-temperature parameter. L and R denote the left and
right response, respectively. The trade-off between exploitation
and exploration is done using the method of Ishii et al. (Ishii,
Yoshida, & Yoshimoto, 2002) as follows:

τ (s) = τg · τl (s)

(8)

where τg and τl are the global and local coefficients of the inversetemperature parameter, respectively. τg recognizes the environmental change and globally controls the inverse-temperature
parameter.

τg =



αg + (1 − αg )τg
τg min

if z = zˆ
otherwise

(9)

where 0 < αg < 1 is the changing rate of τg . The minimum value
for τg is denoted by τg min . When the observed outcome (z) and the
expected one (zˆ ) are the same, it shows that the environment has
not changed. Therefore, the τg increases and this encourages the
exploitation.
However, the τl (s) uses the variation of the Q -value function to
locally control the inverse-temperature parameter.

τl (s) = 

1
E(

QT

(10)

(s, a) ) − (E (Q T (s, a)))2
2

E [ ] denotes the expectation term. τl (s) does not depend on action
a.
The activation function of the response units obeys the Eq. (11).
The action probabilities of Eq. (7) are used for response generation
using the mutual inhibition (Bogacz, Brown, Moehlis, Holmes, &
Cohen, 2006) or the leaky competing accumulator (LCA) (Bogacz,
Usher, Zhang, & McClelland, 2007) models of decision making in
the two-alternative forced-choice tasks. The LCA as a bio-inspired
model of choice simulates the leaky accumulation and competition
processes detected in neuronal populations in choice tasks (Bogacz
et al., 2007).
fi,t +1 = fi,t + Ei θt − Ei fi,t − Ii fj,t + 0.04 dt + N (0, 0.01)





fi,t +1 = max fi,t +1 , 0 .





(11)

In above equation, response units accumulate received information and act as leaky integrators with activity levels denoted by fi,t
and fj,t . Each response unit integrates information received from
an excitatory input denoted by Ei , mean activity of 0.04, and some
noise modeled by N (0, 0.01), which is a Gaussian noise with mean
0 and standard deviation 0.01. Therefore, the term Ei fi,t shows the
decay rate of information accumulation or leaky part. Response
units also inhibit each other. Ii shows the weight by which the competing response unit (j) inhibits response unit i. Consequently, Ii fj,t
denotes the amount of inhibition. When the activity of either response units exceeds a decision threshold, θt becomes zero (otherwise, it equals one). Moreover, the activity levels of response units
are restricted to decay below 0. dt (=0.01) is a time constant. The
relationships between different parts of response generation block
are plotted in Fig. 2.
2.2.5. Performance monitoring and control modules
The performance monitoring module monitors the performance of other modules to give feedback to a control mechanism
to adjust performance. Here the control mechanism regulates inhibitory or excitatory weights to response units based on the situation.
One of the functions of the monitoring module is to detect
the deviation of observed outcome from its expectation. Here the
outcome with the highest probability is assumed to reflect the
expected outcome of the selected response. When the observed

Fig. 2. Inside response generation block. Q T (s, aL ): Total value of stimulusleft response pair; Q T (s, aR ): Total value of stimulus-right response pair; Pr L :
probability associated with left response; Pr R : probability associated with right
response; fL : left response unit activity level; fR : right response unit activity level;
ω + µ: the excitatory weight to the response units; IL : the inhibitory weight to left
response unit; IR : the inhibitory weight to right response unit. L: left response unit;
R: right response unit.

outcome (z) is different from the expected one (zˆ ), the control
module updates the inhibitory weight to the selected response.
Therefore, if the outcome is rewarded, the inhibitory weight to
the selected response is reduced and if the outcome is nonrewarded, the inhibitory weight increases. The amount of change
is proportional to the occurrence probability of the observed
outcome and its relative difference to the non-observed outcome.
Ires,new = Ires,old + αI (1 − 2r ) |T (s, a, re) − T (s, a, nr )|
× T (s, a, sf ) if z ̸= zˆ
Ires,new = Ires,old Otherwise



(12)

where Ires shows the inhibitory weight to the selected response
unit. αI is the learning rate, and r is the amount of received
feedback and sf shows its state. ‘re’ and ‘nr’ stand for a rewarding and non-rewarding feedback state, respectively. The term
|T (s, a, re) − T (s, a, nr )| T (s, a, sf ) can be considered as a reliability measure of the estimated probability of the observed outcome.
Another function of the monitoring module is conflict detection
between costs of all likely outcomes of the selected response. Delay
is one of the cost sources that affects decision making process
(Floresco, St Onge, Ghods-Sharifi, & Winstanley, 2008). In fact
the subjective value of a reward is hyperbolically discounted as
the delay until receiving it increases (Green & Myerson, 2004).
This causes the cost to increase hyperbolically. Also, the delay
in receiving a negative feedback or punishment decreases the
amount of its aversiveness as a hyperbolic function of time and
consequently decreases its cost (Murphy, Vuchinich, & Simpson,
2001).
Moreover, there are two prominent time intervals after
selecting a response: before feedback identification by the subject
and after it. Since in the first time interval the actual outcome
has not been observed, one state can be expected for each of two
possible outcomes of the selected response (both outcomes are
non-observed). As mentioned, this time interval between response

116

S. Zendehrouh / Neural Networks 71 (2015) 112–123

selection and feedback identification discounts the value of the
rewarded outcome and increases its cost. However, this delay
decreases the aversiveness of the non-rewarded outcome and as
a result, decreases its cost (Zendehrouh et al., 2014b). Thus, the
cost functions of rewarded outcome and non-rewarded outcome
associated with the selected response follow Eqs. (13) and (14),
respectively:
costbf
r

(t ) = T (s, a, r ) Q (s, a)



costbf
nr (t ) = T (s, a, nr ) Q (s, a)
bf

1−



1
1 + kr (t − tr ) dt



1

(13)



1 + knr (t − tr ) dt

(14)

bf

where costr (t ) and costnr (t ) respectively show the cost functions
of selected response for rewarded and non-rewarded outcomes
before feedback identification. The selected response at response
time tr can result to a rewarded outcome with probability of
T (s, a, r ) and to a non-rewarded outcome with probability of
T (s, a, nr ). kr and knr are the temporal discounting rates. dt is the
duration of time steps in real world and equals 0.01.
The cost-conflict function before feedback identification is the
product of two simultaneous activated cost functions related to
the selected response. In this paper the conflict signal is computed
in a similar way that response conflict signal is computed in the
conflict monitoring theory (Yeung et al., 2004). This is inspired by
the fact that theoretically, conflict can occur at different levels of
information processing system (Carter & van Veen, 2007).
bf
Con (t ) = costbf
r (t ) · costnr (t )

t < tf

(15)

where tf shows the time of feedback identification. As will be
discussed in the following section, the required time for feedback
identification (in ms) is simulated by a Normal distribution
function with mean of 185 and standard deviation of 5.
In the second interval after feedback identification, the outcome
of the selected response is observed. Depending on the received
feedback, for each of two possible outcomes of the selected
response in this interval, two states can be expected (whether
the assumed outcome has been occurred or not). For an assumed
outcome, if it happens, its associated cost function will decrease.
Because, there is no further cost with that outcome. However, its
associated cost function will continue its past behavior if it does
not happen (Zendehrouh et al., 2014b). Thus, the cost function
associated with the observed outcome will obey Eq. (16) if it is
af
rewarded (costr (t )), and it will obey Eq. (17) if it is non-rewarded.
bf

costaf
r (t ) =

costr (tf − 1)

(16)

1 + k′r (t − tf )
bf

costaf
nr (t ) =

costnr (tf − 1)

(17)

1 + k′nr (t − tf )

where k′r and k′nr are the temporal discounting rates.
However, for the case of non-observed outcome if it is
rewarded, it will obey Eq. (18) and if it is non-rewarded, it will obey
Eq. (19).
bf

costaf
r (t ) =

costr (tf − 1) + T (s, a, r ) Q (s, a) k′r (t − tf )
1 + k′r (t − tf )

(18)

.

(19)

bf

costaf
nr (t ) =

costnr (tf − 1)
1 + k′nr (t − tf )

The cost-conflict function after feedback identification is:
af
Con (t ) = costaf
r (t ) · costnr (t )

t ≥ tf .

(20)

The monitoring module detects the cost conflict signal after
feedback identification and control module uses the accrued
conflict signal to modify the excitatory weights to the response
units for the following trial. Therefore, if the outcome is nonrewarded, the excitatory weight is decreased and if the outcome
is rewarded, the excitatory weight is increased.

µ = (2r − 1)



Con(t )

(21)

t > tf

Ei = (ω + µ) Pr i

(22)

where Pr i (i = L, R) is the corresponding response probability in
Eq. (11). ω is a constant weight and µ is updated by accrued conflict signal.
3. Results and discussions
In this paper, human performance in a probabilistic learning
task is simulated using the proposed model. In the PLT, which is a
trial-and-error learning task, participants press one of two buttons
in response to the presentation of a visual image and receive a
feedback that indicates whether they are rewarded or not. Here
the data of Morris et al. (Morris et al., 2008, 2011) have been
simulated wherein each stimulus is probabilistically associated
with the proper response on either 100%, 80%, or 50% of situations.
As mentioned, in each block, for 100% mappings, one of the six
stimuli needs pressing a left button, and another stimulus needs
pressing a right button on 100% of the trials to obtain a reward. In
80% mappings, one stimulus needs pressing a left button on 80%
of the trials (valid trials) and pressing a right button on 20% of the
trials (invalid trials) to obtain a reward. The other stimulus needs
pressing a right button on 80% of the trials (valid) and pressing a
left button on 20% of the trials (invalid) to obtain a reward. In 50%
mappings, the two remaining stimuli, receive feedback randomly
(Nieuwenhuis et al., 2002).
The results of the simulations reported here correspond to
1200 trials (four blocks of 300 trials) for each of 27 simulated
participants. Each simulation trial contains 230 time steps. A time
step shows 10 ms of the real world. In the first 130 steps, the
stimulus is displayed and the response is executed by the model.
Depending on the selected response, a feedback is presented at
step 131 that lasts for 50 steps. Free parameters of the proposed
model and their assigned values are given in Table 1. These
parameters were estimated by an improved harmony search (IHS)
algorithm (Mahdavi, Fesanghary, & Damangir, 2007) to optimize
model’s fit to the behavioral data.
Here the simulation results of the proposed model are
compared with the empirical data and simulation results of Morris
et al. (Morris et al., 2008, 2011) for the PLT, wherein they have
used a variant of the standard RL-ERN model for their simulations,
the details of which can be found in Holroyd and Coles (2002,
2008). Therefore, their model only uses a model-free algorithm
(temporal difference or TD), while the proposed model applies
both model-free and model-based algorithms of RL. This kind of
approach is more compatible with prevailing view that the modelfree and model-based systems interact with each other and are
not completely separate (Daw et al., 2011; Dolan & Dayan, 2013;
Gutkin & Ahmed, 2012; O’Doherty, Lee, & McNamee, 2015; Simon
& Daw, 2011).
Figs. 3 and 4 respectively compare response selection accuracy
and response times obtained by the empirical and simulation data
of Morris et al. (Morris et al., 2008, 2011) and the simulation
results of the proposed model. The results are averaged over all
participants in 50%, 80%, and 100% mapping conditions. In this
paper, the required time for perceptual processing and stimulus
identification has not been simulated by the model. Thorpe et al.

S. Zendehrouh / Neural Networks 71 (2015) 112–123

117

Table 1
Free parameters of the proposed model and their values.
Equation

Parameter

Value

Describing

(1)
(2)
(3)
(4)

ϕ
γ mb
γ mf
α mf
mf
γ′
′mf
α
w0

0.03
0.7
0.66
0.38

Update rate of the transition matrix
Discount factor of the model-based module
Discount factor for V -values of the model-free module
Learning rate for V -values of the model-free module

0.66

Discount factor for Q -values of the model-free module

0.98
0.94
0.0031
0.09
0.0052
0.54
0.01
0.08
0.08
0.08
2.6

Learning rate for Q -values of the model-free module
Initial weight in the weight function
Decay rate in the weight function
Minimum value of τg
Changing rate of τg
Learning rate for inhibitory weights to response units
Temporal discounting rate
Temporal discounting rate
Temporal discounting rate
Temporal discounting rate
Constant part of the excitatory weight to response units

(5)
(5)
(6)
(6)
(9)
(9)
(12)
(13)
(14)
(16), (18)
(17), (19)
(22)

k

τg min
αg
αI
kr
knr
k′r
k′nr

ω

Fig. 3. The percentage of response selection accuracy in the probabilistic learning
task. The bars show the average percentages (±standard deviation) for the
simulation results of the proposed model (Sim), the empirical (Exp) data, and the
simulation results of Morris et al. model (Morris et al., 2008, 2011). The results
are averaged over all participants in 50%, 80%, and 100% mapping conditions. The
standard deviations for the experimental data in 50% condition and the simulation
results of Morris et al. model (Morris et al., 2008, 2011) were not shown due to the
lack of data.

(Thorpe, Fize, & Marlot, 1996) show that visual perception on an
ultra-rapid time scale, for a stimulus presented for just 20 ms, is
performed in about 150 ms after stimulus onset. However, visual
system needs more processing time to identify objects (Hegdé,
2008). In another study, Grill-Spector and Kanwisher have shown
that the accuracy for all three tasks of detection, categorization,
and identification were close to 100% for a stimulus duration of
167 ms (Grill-Spector & Kanwisher, 2005). Thus, it seems that
the minimum required time for correct stimulus detection and
identification is about 170 ms. Therefore, a function with Normal
distribution with mean of 185 and standard deviation of 5 is used
to model the required time (in ms) for perceptual processing and
stimulus identification. The simulated performance of subjects on
the PLT shown in Figs. 3 and 4 are acceptably close to the empirical
data.
Fig. 5 illustrates the amplitudes of the ERN difference wave
across conditions for the empirical and simulation data of Morris
et al. (Morris et al., 2008, 2011) and simulation results of the
proposed model. In this figure, the simulated ERN difference
waves are computed based on the RL-ERN theory in which RPE

Fig. 4. Response times (RTs) in the probabilistic learning task. The bars show
average response times in ms (±standard deviation) for the simulation results of the
proposed model (Sim) and the empirical (Exp) data (Morris et al., 2008, 2011). The
results are averaged over all participants in 50%, 80%, and 100% mapping conditions.

signals are used for ERN generation. In each trial, the RPE signal
is obtained using Eq. (3). Then, response aligned RPE signals
are averaged across trials separately for trials with positive and
negative feedback. The simulated ERN difference wave signal
is then computed by subtracting averaged RPEs on trials with
positive feedback from averaged RPEs on trials with negative
feedback. In simulations, the amplitude of the simulated ERN
signal was determined as the maximum value of the difference
signal within 100 ms following the response. For the purpose
of comparison, all the empirical and simulation data have been
normalized to the range of [0, 1] using a min–max normalization.
The results show that RPEs obtained from the proposed model is
close to the empirical data. Therefore, the simulated ERN produced
by RPE signals in model-free part of the proposed model is
consistent with the RL-ERN description for this component.
Fig. 6 illustrates the empirical and the simulation data of
Morris et al. (Morris et al., 2008, 2011) and the results of the
proposed model for the amplitudes of the FRN difference waves
across conditions. The amplitude of the FRN difference signal
was determined as the maximum value of the difference signal
between 200 ms and 400 ms following feedback onset. The shown
data were also normalized to the [0, 1] range using a min–max
normalization. In this figure, two kinds of simulated FRN difference
wave have been illustrated, namely a cost-conflict based FRN

118

S. Zendehrouh / Neural Networks 71 (2015) 112–123

Fig. 5. Normalized amplitudes of the ERN difference wave across conditions for
the empirical (solid line) and simulation data of Morris et al. (Morris et al., 2008,
2011) (dotted line) and the results of the proposed model (dashed line) for the
probabilistic learning task. ‘‘80%v’’ and ‘‘80%i’’ indicate 80% condition trials with
valid and invalid feedback, respectively.

and an RPE-based FRN. We hypothesized that the cost-conflict
based FRN difference wave can be computed as the difference
in the cost-conflict signal of rewarded and non-rewarded trials.
To obtain this signal, cost-conflict signals computed by Eqs. (15)
and (20) are aligned to feedback onset. Then, these signals are
averaged across trials separately for rewarded and non-rewarded
ones. A cost-conflict based FRN difference wave is then computed
as the difference in the averaged cost-conflict signal of rewarded
and non-rewarded trials. The amplitude of this FRN difference
wave across conditions is illustrated by a dashed line with square
markers in Fig. 6. The FRN difference wave is also computed
according to the RL-ERN theory in which RPE signals obtained by
Eq. (3) were aligned to feedback onset. Then, the feedback aligned
RPE signals are averaged across trials separately for rewarded and
non-rewarded trials. The simulated FRN difference wave signal
is then computed by subtracting averaged RPEs on rewarded
trials from averaged RPEs on non-rewarded trials. The dash–dot
line with upward-pointing triangle markers in Fig. 6 shows the
amplitudes of these RPE-based FRN difference waves. The results
of the simulated RPE-based FRN difference wave obtained by the
proposed model is not close to the empirical data. The mean
squared error (MSE) of these amplitudes computed as the average
of the squares of the differences between these data and empirical
ones is 0.0115. The MSE for RPE-based FRN difference waves
obtained by Morris et al. is 0.0024 and is 0.0028 for cost-conflict
based FRN difference waves obtained by the proposed model.
However, the MSE for simulation results of Morris et al. is smaller
than that of the proposed model, but the difference is negligible.
Thus, a hypothetical conflict monitor-controller at the level of costs
may be considered as the cause of the FRN generation.
Moreover, the results are compatible with the findings that
the FRN amplitude is higher for more unexpected outcomes, and
the ERN amplitude is higher for more expected ones (Holroyd &
Coles, 2002; Nieuwenhuis et al., 2002). This issue is more evident
in Fig. 7 in which the dynamics of two simulated ERP components
are shown. This figure also depicts the dynamics of simulated FRN
difference wave based on PREs and cost-conflict signal.
In addition, for a better understanding of how the hypothetical
cost-conflict based FRN signal is computed, the dynamics of costconflict signal generation on rewarded and non-rewarded trials
are shown in Fig. 8(A). On rewarded trials (top panel), the cost

Fig. 6. Normalized amplitudes of the FRN difference wave across conditions for the
empirical (solid line) and simulation data of Morris et al. (Morris et al., 2008, 2011)
(dotted line), the FRN obtained by the hypothetical cost-conflict monitor (dashed
line), and the FRN obtained by RPE signals (dash–dot line) for the probabilistic
learning task. ‘‘80%v’’ and ‘‘80%i’’ indicate 80% condition trials with valid and invalid
feedback, respectively.

signal associated with the rewarded outcome obeys Eq. (13)
before feedback identification and obeys Eq. (16) after feedback
identification. The cost signal associated with the non-rewarded
outcome obeys Eqs. (14) and (19) before and after feedback
identification, respectively. Moreover, on non-rewarded trials
(lower panel), the cost signal associated the rewarded outcome
obeys Eqs. (13) and (18) before and after feedback identification,
respectively. The cost signal associated with the non-rewarded
outcome obeys Eq. (14) before feedback identification and obeys
Eq. (17) after feedback identification. On both rewarded and nonrewarded trials, the conflict signal obeys Eqs. (15) and (20) before
and after feedback identification, respectively. The conflict signal
for rewarded and non-rewarded trials is computed as the product
of its related cost signals. For each of rewarded and non-rewarded
trials, cost-conflict signal is first computed, and then it is averaged
across trials. Fig. 8(B) shows the simulated FRN difference wave
as the difference between conflict signals of rewarded and nonrewarded trials. As is evident from this figure, the conflict signal
replicates the main features of difference FRN wave. In this paper,
this difference signal in the period following feedback onset is
referred to as the simulated cost-conflict based FRN difference
wave.
It is also worth mentioning that in Morris et al.’ model, a trial
consists of 60 time steps. Response selection happens at time step
30 and the feedback is presented at final time step (60). Therefore,
their model cannot simulate the participants’ variabilities in response times. Moreover, the required time for feedback identification has not been considered. This also restricts the ability of their
model to generate the dynamics of the FRN that reaches its maximum 230–330 ms following feedback presentation. However, in
the proposed model, response time, feedback identification time,
and the FRN dynamics are simulated.
3.1. Model selection
Akaike information criterion (AIC) was used to compare
qualities of models with each other. AIC trades off the goodness
of a model’s fit against its complexity or more precisely it trades
off between underfitting and overfitting of a model (Busemeyer,
Wang, Townsend, & Eidels, 2015).

S. Zendehrouh / Neural Networks 71 (2015) 112–123

A

B

119

C

Fig. 7. Dynamics of the simulated ERP components across conditions obtained by the proposed model. (A) FRN difference wave based on cost-conflict signal. (B) FRN
difference wave based on RPEs. (C) ERN difference wave based on RPEs. Negative is plotted up. Zeros indicate time of feedback stimulus onset in figures (A) and (B) and time
of response generation in figure (C). ‘‘80%v’’ and ‘‘80%i’’ indicate 80% condition trials with valid and invalid feedback, respectively.

A

B

Fig. 8. (A) Dynamics of cost and conflict signals in different conditions averaged across rewarded trials (upper panel) and non-rewarded trials (lower panel) while aligned
to feedback onset. The figures in each row show the cost of the rewarded outcome, the cost of the non-rewarded outcome, and the cost-conflict signal associated with
the selected response, respectively. (B) Simulated FRN difference wave across conditions as the difference in the cost-conflict signal of rewarded and non-rewarded trials
(FRN based on cost-conflict signal). Zero on each abscissa indicates time of feedback stimulus onset. Negative is plotted up by convention. ‘‘80%v’’ and ‘‘80%i’’ indicate 80%
condition trials with valid and invalid feedback, respectively.

The important point in calculating AIC is the computation of
maximized value of the likelihood function. As is evident from
past section, free parameters of the proposed model have not been
obtained by a maximum likelihood estimator. The IHS method
(Mahdavi et al., 2007) was used to get best-fitting parameters
that minimized the sum of absolute percentage error between
predicted and actual accuracies and RTs across all trials. The other
model with which a comparison is performed is the model of

Morris et al. (Morris et al., 2011). In their paper, it is not obvious
how they optimized free parameters to fit the behavioral data
and how many free parameters exactly their model has. However,
it seems that they used a reinforcement learning approach as
in the (Holroyd & Coles, 2008). Moreover, it can be said with
certainty that their model at least has two free parameters (one is a
learning rate and the other is the value of epsilon in epsilon-greedy
approach of their response selection).

120

S. Zendehrouh / Neural Networks 71 (2015) 112–123

Table 2
Akaike information criterion (AIC) for the proposed model and Morris et al.’s model in three different conditions (with std (standard deviation of Morris et al.’ model) = stdpm
(std of the proposed model); std = 2 stdpm ; and std = stdexp (std of experimental data)). G2 : −2 log likelihood.

G2
Number of parameters
AIC

Proposed model

Morris model std = stdpm

Morris model std = 2 stdpm

Morris model std = stdexp

55.8
16
87.8

103.8
2
107.8

104.2
2
108.2

106.1
2
110.1

Another important issue is the level of data analysis. Due to the
lack of data at the individual level for the results of Morris et al.
(Morris et al., 2008, 2011), a group level analysis was incorporated.
Therefore, simulations are run for each participant separately and
all results are averaged afterward. Then, the currently applied
parameter estimates are used to produce simulations for the entire
sequence of choices for the next participant.
Now, if we observe a mean accuracy and a mean response time
across all trials for the person i (xAcc
and xRT
i
i , respectively), the
models will also predict those values for each person (P (xAcc
i ) and

ˆ
P (xRT
i ), respectively). Then given the model parameters (θ ), we can
compute the log-likelihood of the observed sequence of data (data)
as (Busemeyer et al., 2015) (assuming that the probabilities for
mean accuracy and mean response time are independent of each
other):
N

ln L(data|θˆ ) =



Acc
ln(P (xRT
i ) + P (xi ))

i =1

where N is the number of participants. AIC is then calculated as
G2 = −2 ln L(data|θˆ )
AIC = G2 + 2K
where K is the number of free parameters in the model. Since there
was no access to the details of experimental data except the mean
and standard deviation across all participants, it was assumed that
probability distribution functions of observed experimental data
for both accuracy and response time are Gaussian. Now assume
that we observed xAcc
and xRT
from individual i on the task. How
i
i
do the models predict the observed values? These predictions
RT
(P (xAcc
i ) and P (xi )) can be computed by first simulating mean
accuracy and mean response time for individual i on the task.
Then, the probabilities can be found by placing these estimated
values into the probability distribution function of experimental
data. Moreover, due to a lack of access to the simulation results of
Morris et al. and since they do not provide standard deviations, a
sequence of data for all participants was generated using a normal
distribution function with their reported mean value and different
standard deviations. The results of these computations are given
in Table 2. It is evident that the proposed model has the lowest AIC
value. The main cause of increase in Morris et al.’s AIC value arises
from a constant response time in their simulations as explained in
the previous section.
3.2. Model predictions
3.2.1. Maladaptive perfectionism
After learning the outcome of a decision, the individual may
imagine the possible outcomes of the non-selected option/options.
The decision maker can also compare these imagined outcomes
with the experienced or observed outcome. This appraisal of
imagined alternative outcomes as opposed to the experienced or
observed outcome and thinking about what would have happened
had another response been selected is referred to as counterfactual
thinking (Howlett & Paulus, 2013).
Distorted counterfactual thinking leads to some cognitive
and behavioral dysfunctions. Maladaptive perfectionism as a

tendency to achieve a perfect outcome is one of such behavioral
dysfunctions (Howlett & Paulus, 2013). This cognitive dysfunction
has been linked with depression (Flett, Besser, Davis, & Hewitt,
2003; Howlett & Paulus, 2013). In maladaptive perfectionism, the
individuals focus on best possible outcome that could have been
attained after a negative event (Sirois, Monforton, & Simpson,
2010). Here such kind of counterfactual thinking is simulated
after experiencing a negative outcome in the PLT. Therefore, after
experiencing a non-rewarded outcome, the individual imagines
that the non-selected response selection would have gained a
better outcome. In this case, when a person encounters a nonrewarded outcome, the occurrence probabilities of the nonselected response changes as follows:
T (s, anres , re) = 1 − ϕ ′ T (s, anres , re) + ϕ ′





T (s, anres , nr ) = 1 − ϕ ′ T (s, anres , nr )





(23)

where anres is the non-selected response. T (s, anres , re) and
T (s, anres , nr ) are the estimated occurrence probabilities for the rewarded and non-rewarded outcome, respectively. 0 < ϕ ′ < 1
is the update rate. ϕ ′ = 1 shows the absence of this kind of
counterfactual thinking and as it decreases the maladaptive perfectionism occurs. Fig. 9(a) shows the amplitudes of the simulated
cost-conflict based FRN across conditions for normal (ϕ ′ = 1) and
altered counterfactual thinking (ϕ ′ = 0.93) conditions. Fig. 9(b)
shows the amplitudes of the simulated RPE-based FRN across conditions for the mentioned situations. The results of this figure
indicate that as the ϕ ′ increases the amplitudes of cost-conflict
based FRNs also increases. This is in agreement with previous
studies showing increased FRNs for depressed individuals (Mies
et al., 2011; Santesso et al., 2008; Tucker, Luu, Frishkoff, Quiring,
& Poulsen, 2003). However, the simulated RPE-based FRN show
inferior results in 100% and 80% valid condition in comparison to
control state. These results also suggest that the cost-conflict monitoring can better describe the FRN.
3.2.2. Obsessive–compulsive disorder (OCD)
In this section, we simulate the empirical data of Nieuwenhuis
et al. (Nieuwenhuis, Nielen, Mol, Hajcak, & Veltman, 2005) in
which a similar PLT task has been conducted for 16 healthy
individuals and 16 patients with obsessive–compulsive disorder
(OCD). OCD is characterized by recurrent unwanted thoughts
known as obsessions and an irresistible need to perform repetitive
behavior known as compulsions (Fineberg, Marazziti, & Stein,
2001). OCD is also associated with maladaptive perfectionism
(Rhéaume, Freeston, Dugas, Letarte, & Ladouceur, 1995).
Fig. 10(a)–(c) respectively show response selection accuracy,
response times, and normalized amplitudes of simulated FRN
difference wave obtained by the empirical data of Nieuwenhuis
et al. (Nieuwenhuis et al., 2005) and the simulation results of
the proposed model for healthy individuals. Fig. 11(a)–(c) show
the same results for OCD patients. These results also confirm the
previous finding that a cost-conflict based FRN better matches the
empirical data in comparison with an RPE-based FRN.
Free parameters of the model fitted to the empirical data of
healthy individuals (controls) and OCDs are given in Table 3.
The changes in some parameter values support the proposal that
OCD is associated with hypoactivity in a system that actively

S. Zendehrouh / Neural Networks 71 (2015) 112–123

A

121

B

Fig. 9. Amplitudes of the FRN difference wave across conditions for normal (ϕ ′ = 1) and altered counterfactual thinking (ϕ ′ = 0.93) conditions. (A) The simulated FRN
based on cost-conflict signal. (B) The simulated FRN based on RPE signal.

A

B

C

Fig. 10. Simulation and empirical results for controls participated in a probabilistic learning task. (A) The percentage of response selection accuracy. The bars show average
percentages (±standard deviation) for simulation results of the proposed model (Sim), the empirical (Exp) data of (Nieuwenhuis et al., 2005). The results are averaged over
all participants in 50%, 80%, and 100% mapping conditions. (B) Response times (RTs). The bars show average response times in ms (±standard deviation) for simulation
results of the proposed model (Sim) and the empirical (Exp) data of (Nieuwenhuis et al., 2005). The results are averaged over all participants in 50%, 80%, and 100% mapping
conditions. (C) Normalized amplitudes of the FRN difference wave across conditions for the empirical data of (Nieuwenhuis et al., 2005), the FRN based on cost-conflict
signal, and the FRN obtained by RPE signals. ‘‘80%v’’ and ‘‘80%i’’ indicate 80% condition trials with valid and invalid feedback, respectively.

A

B

C

Fig. 11. Simulation and empirical results for OCDs participated in a probabilistic learning task. (A) The percentage of response selection accuracy. The bars show average
percentages (±standard deviation) for simulation results of the proposed model (Sim), the empirical (Exp) data of (Nieuwenhuis et al., 2005). The results are averaged over
all participants in 50%, 80%, and 100% mapping conditions. (B) Response times (RTs). The bars show average response times in ms (±standard deviation) for simulation
results of the proposed model (Sim) and the empirical (Exp) data of (Nieuwenhuis et al., 2005). The results are averaged over all participants in 50%, 80%, and 100% mapping
conditions. (C) Normalized amplitudes of the FRN difference wave across conditions for the empirical data of (Nieuwenhuis et al., 2005), the FRN based on cost-conflict
signal, and the FRN obtained by RPE signals. ‘‘80%v’’ and ‘‘80%i’’ indicate 80% condition trials with valid and invalid feedback, respectively.

122

S. Zendehrouh / Neural Networks 71 (2015) 112–123

Table 3
Free parameters of the proposed model fitted to the empirical data of OCDs and controls. In the first row of the table, ‘r’ stands for rewarded outcomes and ‘nr’ stands for
non-rewarded outcomes.
Equation

Parameter

Values for controls

Values for OCDs

Describing

(1)
(2)
(3)
(4)

ϕ
γ mb
γ mf
α mf
mf
γ′
′mf
α
w0

0.97
0.85
0.66
0.38

0.97(r)/0.93(nr)
0.7
0.97
0.58

Update rate of the transition matrix
Discount factor of the model-based module
Discount factor for V -values of the model-free module
Learning rate for V -values of the model-free module

(5)
(5)
(6)
(6)
(9)
(9)
(12)
(13)
(14)
(16), (18)
(17), (19)
(22)

k

τg min
αg
αI
kr
knr
k′r
k′nr

ω

0.66

0.97

Discount factor for Q -values of the model-free module

0.98
0.94
0.0031
0.089
0.0052
0.54
0.001
0.008
0.08
0.08
2.6

0.98
0.94
0.0031
0.096
0.0015
0.54
0.001
0.008
0.08
0.08
2.6

Learning rate for Q -values of the model-free module
Initial weight in the weight function
Decay rate in the weight function
Minimum value of τg
Changing rate of τg
Learning rate for inhibitory weights to response units
Temporal discounting rate
Temporal discounting rate
Temporal discounting rate
Temporal discounting rate
Constant part of the excitatory weight to response units

learns to avoid inappropriate choices and hyperactivity in another
system that somewhat passively produce behavior based on
learnt stimulus–response mappings (Endrass, Koehne, Riesel, &
Kathmann, 2013; Gründler, Cavanagh, Figueroa, Frank, & Allen,
2009; Nieuwenhuis et al., 2005). In other words, OCD may be
connected with hypoactivity in the system that controls goaldirected behavior and hyperactivity in the system that controls
habitual behavior. This is in line with the increment of γ mf and
α mf parameters of model-free module and the decrement of γ mb
parameter of model-based module that can respectively be an
indicator of model-free module hyperactivity and model-based
module hypoactivity.
4. Conclusions
In this paper, a computational model for a variant of probabilistic learning task was presented. While the proposed model, based
on dual system theory of decision- making, simulates behavioral
data from this experiment, it also explores the best definition for
ERN and FRN components that matches the empirical data.
In summary, the amplitudes of the simulated ERN and FRN difference waves and the simulated performance of subjects on the
PLT are compared against the empirical data. The results of the proposed model also found support for the RL-ERN view about ERN
generation. The simulated ERN is better defined by the RPE signals in model-free part of the proposed model consistent with the
RL-ERN description for this component. However, the amplitudes
of the simulated FRN based on RPE signals are not so close to the
empirical FRN amplitudes. In fact, we have presented simulation
evidences against the RL-ERN theory of the FRN. The results show
that the FRN is better simulated by a hypothetical cost-conflict
monitor-controller in the brain. However, future studies will be
necessary to define the plausibility of the hypothesis.
References
Alexander, W. H., & Brown, J. W. (2011). Medial prefrontal cortex as an actionoutcome predictor. Nature Neuroscience, 14(10), 1338–1344.
http://doi.org/10.1038/nn.2921.
Balleine, B. W., & Dickinson, A. (1998). Goal-directed instrumental action: contingency and incentive learning and their cortical substrates. Neuropharmacology,
37(4–5), 407–419. http://doi.org/10.1016/S0028-3908(98)00033-1.
Bogacz, R., Brown, E., Moehlis, J., Holmes, P., & Cohen, J. D. (2006). The physics
of optimal decision making: a formal analysis of models of performance in
two-alternative forced-choice tasks. Psychological Review, 113(4), 700–765.
http://doi.org/10.1037/0033-295X.113.4.700.
Bogacz, R., Usher, M., Zhang, J., & McClelland, J. L. (2007). Extending a biologically
inspired model of choice: multi-alternatives, nonlinearity and value-based
multidimensional choice. Philosophical Transactions of the Royal Society B: Biological Sciences, 362(1485), 1655–1670. http://doi.org/10.1098/rstb.2007.2059.

Bornstein, A. M., & Daw, N. D. (2011). Multiplicity of control in the basal ganglia:
computational roles of striatal subregions. Current Opinion in Neurobiology,
21(3), 374–380. http://doi.org/10.1016/j.conb.2011.02.009.
Botvinick, M. M., Braver, T. S., Barch, D. M., Carter, C. S., & Cohen, J. D. (2001). Conflict
monitoring and cognitive control. Psychological Review, 108(3), 624–652.
Busemeyer, J. R., Wang, Z., Townsend, J. T., & Eidels, A. (2015). The oxford handbook
of computational and mathematical psychology. USA: OUP.
Carter, C. S., & van Veen, V. (2007). Anterior cingulate cortex and conflict detection:
an update of theory and data. Cognitive Affective & Behavioral Neuroscience, 7(4),
367–379.
Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P., & Dolan, R. J. (2011). Model-based
influences on humans’ choices and striatal prediction errors. Neuron, 69(6),
1204–1215. http://doi.org/10.1016/j.neuron.2011.02.027.
Daw, N. D., Niv, Y., & Dayan, P. (2005). Uncertainty-based competition between
prefrontal and dorsolateral striatal systems for behavioral control. Nature
Neuroscience, 8(12), 1704–1711. http://doi.org/10.1038/nn1560.
Dolan, R. J., & Dayan, P. (2013). Goals and habits in the brain. Neuron, 80(2),
312–325. http://doi.org/10.1016/j.neuron.2013.09.007.
Endrass, T., Koehne, S., Riesel, A., & Kathmann, N. (2013). Neural correlates of
feedback processing in obsessive-compulsive disorder. Journal of Abnormal
Psychology, 122(2), 387–396. http://doi.org/10.1037/a0031496.
Fineberg, N., Marazziti, D., & Stein, D. (2001). Obsessive compulsive disorders: a
practical guide. Taylor & Francis.
Flett, G. L., Besser, A., Davis, R. A., & Hewitt, P. L. (2003). Dimensions of perfectionism, unconditional self-acceptance, and depression. Journal of Rational-Emotive
and Cognitive-Behavior Therapy, 21(2), 119–138.
http://doi.org/10.1023/A:1025051431957.
Floresco, S. B., St Onge, J. R., Ghods-Sharifi, S., & Winstanley, C. A. (2008). Corticolimbic-striatal circuits subserving different forms of cost-benefit decision
making. Cognitive Affective %26 Behavioral Neuroscience, 8(4), 375–389.
http://doi.org/10.3758/CABN.8.4.375.
Gehring, W. J., Goss, B., Coles, M. G. H., Meyer, D. E., & Donchin, E. (1993). A neural
system for error detection and compensation. Psychological Science, 4(6),
385–390. http://doi.org/10.1111/j.1467-9280.1993.tb00586.x.
Gläscher, J., Daw, N., Dayan, P., & O’Doherty, J. P. (2010). States versus rewards:
Dissociable neural prediction error signals underlying model-based and modelfree reinforcement learning. Neuron, 66(4), 585–595.
http://doi.org/10.1016/j.neuron.2010.04.016.
Green, L., & Myerson, J. (2004). A discounting framework for choice with delayed
and probabilistic rewards. Psychological Bulletin, 130(5), 769–792.
http://doi.org/10.1037/0033-2909.130.5.769.
Grill-Spector, K., & Kanwisher, N. (2005). Visual recognition: as soon as you
know it is there, you know what it is. Psychological Science, 16(2), 152–160.
http://doi.org/10.1111/j.0956-7976.2005.00796.x.
Gründler, T. O. J., Cavanagh, J. F., Figueroa, C. M., Frank, M. J., & Allen, J. J.
B. (2009). Task-related dissociation in ERN amplitude as a function of
obsessive-compulsive symptoms. Neuropsychologia, 47(8–9), 1978–1987.
http://doi.org/10.1016/j.neuropsychologia.2009.03.010.
Gutkin, B., & Ahmed, S. H. (2012). Computational neuroscience of drug addiction. New
York: Springer Science+Business Media, LLC.
Hegdé, J. (2008). Time course of visual perception: Coarse-to-fine processing and
beyond. Progress in Neurobiology, 84(4), 405–439.
http://doi.org/10.1016/j.pneurobio.2007.09.001.
Holroyd, C. B., & Coles, M. G. H. (2002). The neural basis of human error processing:
reinforcement learning, dopamine, and the error-related negativity. Psychological Review, 109(4), 679–709.
Holroyd, C. B., & Coles, M. G. H. (2008). Dorsal anterior cingulate cortex integrates reinforcement history to guide voluntary behavior. Cortex; a Journal
Devoted to the Study of the Nervous System and Behavior, 44(5), 548–559.
http://doi.org/10.1016/j.cortex.2007.08.013.

S. Zendehrouh / Neural Networks 71 (2015) 112–123
Holroyd, C. B., Hajcak, G., & Larsen, J. T. (2006). The good, the bad and the neutral:
electrophysiological responses to feedback stimuli. Brain Research, 1105(1),
93–101. http://doi.org/10.1016/j.brainres.2005.12.015.
Holroyd, C. B., Yeung, N., Coles, M. G. H., & Cohen, J. D. (2005). A mechanism for error
detection in speeded response time tasks. Journal of Experimental Psychology.
General, 134(2), 163–191. http://doi.org/10.1037/0096-3445.134.2.163.
Howlett, J. R., & Paulus, M. P. (2013). Decision-making dysfunctions of counterfactuals in depression: Who might i have been? Frontiers in Psychiatry, 4,
http://doi.org/10.3389/fpsyt.2013.00143.
Ishii, S., Yoshida, W., & Yoshimoto, J. (2002). Control of exploitation-exploration
meta-parameter in reinforcement learning. Neural Networks: The Official Journal
of the International Neural Network Society, 15(4–6), 665–687.
Lee, S. W., Shimojo, S., & O’Doherty, J. P. (2014). Neural computations underlying
arbitration between model-based and model-free learning. Neuron, 81(3),
687–699. http://doi.org/10.1016/j.neuron.2013.11.028.
Mahdavi, M., Fesanghary, M., & Damangir, E. (2007). An improved harmony
search algorithm for solving optimization problems. Applied Mathematics and
Computation, 188(2), 1567–1579. http://doi.org/10.1016/j.amc.2006.11.033.
Mies, G. W., van der Veen, F. M., Tulen, J. H. M., Birkenhäger, T. K., Hengeveld, M.
W., & van der Molen, M. W. (2011). Drug-free patients with major depression
show an increased electrophysiological response to valid and invalid feedback.
Psychological Medicine, 41(12), 2515–2525.
http://doi.org/10.1017/S0033291711000778.
Miltner, W. H. R., Braun, C. H., & Coles, M. G. H. (1997). Event-related brain potentials following incorrect feedback in a time-estimation task: Evidence for
a generic neural system for error detection. Journal of Cognitive Neuroscience,
9(6), 788–798. http://doi.org/10.1162/jocn.1997.9.6.788.
Morris, S. E., Heerey, E. A., Gold, J. M., & Holroyd, C. B. (2008). Learning-related
changes in brain activity following errors and performance feedback in
schizophrenia. Schizophrenia Research, 99(1–3), 274–285.
http://doi.org/10.1016/j.schres.2007.08.027.
Morris, S. E., Holroyd, C. B., Mann-Wrobel, M. C., & Gold, J. M. (2011). Dissociation
of response and feedback negativity in schizophrenia: electrophysiological and
computational evidence for a deficit in the representation of value. Frontiers in
Human Neuroscience, 5, 123. http://doi.org/10.3389/fnhum.2011.00123.
Murphy, J., Vuchinich, R., & Simpson, C. (2001). Delayed reward and cost
discounting. The Psychological Record, 51(4), 571–588.
Nieuwenhuis, S., Nielen, M. M., Mol, N., Hajcak, G., & Veltman, D. J. (2005). Performance monitoring in obsessive-compulsive disorder. Psychiatry Research,
134(2), 111–122. http://doi.org/10.1016/j.psychres.2005.02.005.
Nieuwenhuis, S., Ridderinkhof, K. R., Talsma, D., Coles, M. G. H., Holroyd, C. B., Kok,
A., & van der Molen, M. W. (2002). A computational account of altered error
processing in older age: dopamine and the error-related negativity. Cognitive
Affective & Behavioral Neuroscience, 2(1), 19–36.
O’Doherty, J. P., Lee, S. W., & McNamee, D. (2015). The structure of reinforcementlearning mechanisms in the human brain. Current Opinion in Behavioral Sciences,
1, 94–100. http://doi.org/10.1016/j.cobeha.2014.10.004.
Rhéaume, J., Freeston, M. H., Dugas, M. J., Letarte, H., & Ladouceur, R. (1995).
Perfectionism, responsibility and obsessive-compulsive symptoms. Behaviour
Research and Therapy, 33(7), 785–794.

123

Santesso, D. L., Steele, K. T., Bogdan, R., Holmes, A. J., Deveney, C. M., Meites, T. M.,
& Pizzagalli, D. A. (2008). Enhanced negative feedback responses in remitted
depression. Neuroreport, 19(10), 1045–1048.
http://doi.org/10.1097/WNR.0b013e3283036e73.
Simon, D. A., & Daw, N. D. (2011). Neural correlates of forward planning in a
spatial decision task in humans. The Journal of Neuroscience, 31(14), 5526–5539.
http://doi.org/10.1523/JNEUROSCI.4647-10.2011.
Sirois, F. M., Monforton, J., & Simpson, M. (2010). If only I had done better: Perfectionism and the functionality of counterfactual thinking. Personality %26 Social
Psychology Bulletin, 36(12), 1675–1692.
http://doi.org/10.1177/0146167210387614.
Suri, R. E. (2002). TD models of reward predictive responses in dopamine neurons. Neural Networks, 15(4–6), 523–533. http://doi.org/10.1016/S08936080(02)00046-1.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: an introduction.
Cambridge: MIT Press.
Thorpe, S., Fize, D., & Marlot, C. (1996). Speed of processing in the human visual
system. Nature, 381(6582), 520–522. http://doi.org/10.1038/381520a0.
Todd, P. M., Hills, T. T., & Robbins, T. W. (2012). Cognitive search: evolution,
algorithms, and the brain. Cambridge, MA: MIT Press.
Tucker, D. M., Luu, P., Frishkoff, G., Quiring, J., & Poulsen, C. (2003). Frontolimbic
response to negative feedback in clinical depression. Journal of Abnormal
Psychology, 112(4), 667–678. http://doi.org/10.1037/0021-843X.112.4.667.
Ullsperger, M., Danielmeier, C., & Jocham, G. (2014). Neurophysiology of performance monitoring and adaptive behavior. Physiological Reviews, 94(1), 35–79.
http://doi.org/10.1152/physrev.00041.2012.
Valentin, V. V., Dickinson, A., & O’Doherty, J. P. (2007). Determining the neural
substrates of goal-directed learning in the human brain. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 27(15), 4019–4026.
http://doi.org/10.1523/JNEUROSCI.0564-07.2007.
Wiering, M.A., & van Hasselt, H. (2009). The QV family compared to other reinforcement learning algorithms. In IEEE symposium on adaptive dynamic programming
and reinforcement learning, 2009. ADPRL’09 (pp. 101–108).
http://doi.org/10.1109/ADPRL.2009.4927532.
Yeung, N., Cohen, J. D., & Botvinick, M. M. (2004). The neural basis of error detection:
conflict monitoring and the error-related negativity. Psychological Review, 111,
931–959.
Yoshida, W., & Seymour, B. (2014). Decisions about decisions. Neuron, 81(3),
468–470. http://doi.org/10.1016/j.neuron.2014.01.030.
Zendehrouh, S., Gharibzadeh, S., & Towhidkhah, F. (2013). Modeling error detection
in human brain: A preliminary unification of reinforcement learning and
conflict monitoring theories. Neurocomputing, 103, 1–13.
http://doi.org/10.1016/j.neucom.2012.04.026.
Zendehrouh, S., Gharibzadeh, S., & Towhidkhah, F. (2014a). Reinforcement-conflict
based control: An integrative model of error detection in anterior cingulate
cortex. Neurocomputing, 123, 140–149.
http://doi.org/10.1016/j.neucom.2013.06.020.
Zendehrouh, S., Gharibzadeh, S., & Towhidkhah, F. (2014b). The hypothetical
cost-conflict monitor: is it a possible trigger for conflict-driven control mechanisms in the human brain? Frontiers in Computational Neuroscience, 8, 77.
http://doi.org/10.3389/fncom.2014.00077.

