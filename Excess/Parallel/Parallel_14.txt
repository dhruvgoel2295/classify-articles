Parallel Computing 49 (2015) 66–82

Contents lists available at ScienceDirect

Parallel Computing
journal homepage: www.elsevier.com/locate/parco

Time-domain BEM for the wave equation on
distributed-heterogeneous architectures: A blocking approach
Berenger Bramas a,∗, Olivier Coulaud a, Guillaume Sylvand b
a
b

Inria Bordeaux - Sud-Ouest, 200 Rue Vieille Tour, Talence 33405, France
Airbus Group Innovations, Toulouse, France

a r t i c l e

i n f o

Article history:
Received 8 October 2014
Revised 22 May 2015
Accepted 22 July 2015
Available online 31 July 2015
Keywords:
Parallel algorithms
Hybrid parallelization
CUDA
Multi-GPUs
Time-domain
BEM

a b s t r a c t
The problem of time-domain BEM for the wave equation in acoustics and electromagnetism
can be expressed as a sparse linear system composed of multiple interaction/convolution matrices. It can be solved by using sparse matrix-vector products which are ineﬃcient to achieve
high Flop-rate neither on CPUs nor GPUs. In this paper we extend the approach proposed in a
previous work [1] in which we re-order the computation to get a special matrix structure with
one dense vector per row. This new structure is called a slice matrix and is computed with a
custom matrix/vector product operator. In this study, we present an optimized implementation of this operator on Nvidia GPUs based on two blocking strategies. We explain how we
can obtain multiple block-values from a slice and how these can be computed eﬃciently on
GPUs since we target heterogeneous nodes composed of CPUs and GPUs. In order to deal with
different eﬃciencies of the processing units we use a greedy heuristic that dynamically balances work among the workers. We demonstrate the performance of our system by studying
the quality of the balancing heuristic and the sequential Flop-rate of the blocked implementations. Finally, we validate our implementation with an industrial test case on 8 heterogeneous
nodes, each composed of 12 CPUs and 3 GPUs.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
Airbus Group Innovations is an entity of Airbus Group devoted to research and development for the usage of Airbus Group
divisions (Airbus Civil Aircraft, Airbus Defence & Space, Airbus Helicopters). For more than 20 years, the numerical analysis
team has been working on integral equations and boundary element methods for wave propagation simulations. The resulting
software solutions are used on a daily basis in acoustics for installation effects computation, aeroacoustic simulations (in a
coupled scheme with other tools), and in electromagnetism for antenna siting, electromagnetic compatibility or stealth. Since
2000, these frequency-domain boundary element method (BEM) tools have been extended with a multipole algorithm (called
Fast Multipole Method) that allows us to solve very large problems, with tens of millions of unknowns, in reasonable time on
parallel machines. More recently, H-matrix techniques have enabled the design of fast direct solvers, able to solve problems
with millions of unknowns for a very high accuracy without the usual drawback associated with the iterative solvers: no control
on the number of iterations, diﬃculty to ﬁnd a good preconditioner, etc. At the same time, we work on the design and on the
optimization of the time domain BEM (TD-BEM).

∗

Corresponding author. Tel.: +33 524574076.
E-mail addresses: berenger.bramas@inria.fr (B. Bramas), Olivier.Coulaud@inria.fr (O. Coulaud), Guillaume.Sylvand@airbus.com (G. Sylvand).

http://dx.doi.org/10.1016/j.parco.2015.07.005
0167-8191/© 2015 Elsevier B.V. All rights reserved.

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

67

The frequency-domain BEM (FD-BEM) has been widely studied [2], compared to time-domain formulations. The TD-BEM
allows us to obtain results for a large number of frequency with only one calculation. Therefore, even if the TD-BEM appears
costly, it becomes competitive to study wide-band applications. Our TD-BEM formulation has been described in [3] and originally relies on sparse matrix-vector product (SpMV). However, it is well known that SpMV is memory bound and achieves a
small percentage of the peak performance on most architectures. In our previous work [1], we introduced a new computational
ordering and obtained special matrices called Slice matrices. We designed optimized CPU kernels that take advantage of the Slice
matrix structure and we proposed a parallelized algorithm on distributed and shared memory.
Even so, heterogeneous nodes composed of CPUs and accelerators are becoming a standard in modern clusters and it is
now widespread to provide industrial applications which support accelerators. This current study constitutes an extension of
our previous work in this direction. Here, we address two major problems connected to TD-BEM solvers. First, we propose an
eﬃcient computational kernel for our custom multi-vectors/vector product on NVidia GPUs. Second, with the help of a balancing
algorithm, we propose novel parallelization strategies for clusters of multicore nodes enhanced with multiple GPUs.
The rest of the paper is organized as follows: Section 2 provides the background and the mathematical formulation of the
problem and describes the usual algorithm. Then we introduce the slice matrix structure and the multi-vectors/vector product
in Section 3 and the related work in Section 4. Section 5 describes the blocking schemes and the GPU implementations. Section 6
details the parallelization strategies and the balancing algorithm. Finally, in Section 7 we provide an experimental performance
evaluation of our multi-vectors/vector operator on GPUs and CPUs and illustrate the parallel behavior with an industrial test
case.
2. Time-domain BEM for the wave equation
2.1. Formulation
In order to keep this paper self-explanatory we present the most relevant aspects of the TD-BEM, originally introduced in
[3]. An incident acoustic plane wave w is emitted on a boundary
and has a velocity c and a wavelength λ. The surface
is
discretized by a classical ﬁnite element method using triangular elements with the unknowns/degrees of freedom located at
the vertices. We denote by N the total number of unknowns in the system. The wave equation is also discretized in time with
a step t and the number of time steps T has to be chosen from the frequency range of the study and the size of the object
considered. The illumination vector ln contains the inﬂuence of the incident wave w over the unknowns of the mesh at iteration
time tn = n t. Once the wave illuminates the location where the unknowns are deﬁned it is then reﬂected by them over the
mesh. This complex behavior is represented numerically by a set of interaction/convolution matrices Mk , 0 ≤ k ≤ Kmax.
Using the convolution matrices Mk , and the vector ln which is the emitted by a source on the mesh, the aim is to compute the
state of the unknowns an at time n deﬁned by
Kmax

Mk · an−k = l n .

(1)

k≥0

The original Eq. (1) can be rewritten as in formula (2) where the left hand side is the state to compute and the right hand side
is known from the Kmax previous time steps and the test case deﬁnition

an = (M0 )−1 l n −

Kmax

Mk · an−k

.

(2)

k=1

2.2. Interaction/convolution matrices
The matrix Mk contains the interactions between unknowns that are separated by a distance around kc t and contains zero
for unknowns that are closer or further than this distance. These N × N matrices are positive deﬁnite and sparse in realistic
conﬁguration. They have the following properties:
•

•

The number of non-zero values for a given matrix Mk depends on the structure of the mesh (the distance between the
unknowns) and the physical properties of the system c, λ and t.
For k > Kmax = 2 + max /(c t ), with max the maximum distance between two unknowns, the matrices Mk are zero.

Fig. 1 illustrates the construction of these matrices and shows where the NNZ values are depending on the delay taken by a
wave emitted by an unknown to pass over another one.
The position of the non-zero values in the matrices is driven by the numbering of the unknowns. Two non-zero (NNZ) values
are contiguous in a row, for example Mk (i, j) and Mk (i, j + 1) if the unknowns j and j + 1 are both at a distance k.c. t from i. On
the other hand, two NNZ values are contiguous in a column, for example Mk (i, j) and Mk (i + 1, j), if the unknowns i and i + 1 are
both at a distance k.c. t from j. Therefore, numbering consecutively the unknowns that are spatially close is a way among others
to increase the chance to have contiguous values in the interaction matrices.

68

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

(a) M 0

(b) M 1

(c) M 2

(d) M 3

Fig. 1. Example of Mk matrices for three unknowns A, B, C in 2D A wave emitted from each unknown is represented at every time step. When a wave is around an
unknown, a value is added in the matrix which is symbolized by a gray square. All matrices Mk with k > 3 are zero since the highest distance between elements
is ≤ 3c t.

2.3. Resolution algorithm
The solution is computed in two steps. In the ﬁrst step, the past is taken into account using the previous values of ap with p <
n and the interaction matrices as shown in Eq. (3). The result sn is subtracted from the illumination vector, see Eq. (4),
Kmax

sn =

Mk · an−k ,

(3)

k=1

sn = l n − sn .

(4)

In the second step, the state of the system at time step n is obtained after solving the following linear system:

M0 an = sn .

(5)

The ﬁrst step is the most expensive part from a computational standpoint. The solution of Eq. (5) is extremely fast, since
matrix M0 is symmetric, positive deﬁnite, sparse and almost diagonal. We can solve it by using a sparse direct solver for example.
2.4. Context of the application
The application presented in this study is a layer of an industrial computational work-ﬂow. We delegate to some black-boxes
the generation of the values of the interaction matrices and the factorization of M0 and we concentrate our work on the solution
algorithm. The physical problems that are simulated are based on static conﬁgurations (no moving parts), and therefore all the
interaction matrices and the pre-computations needed by the direct solver − which includes analysis and factorization − are
performed once at the beginning. It makes the computation of the right-hand side sn the most costly part of our algorithm.
3. Re-ordering the summation computation
3.1. Ordering possibilities
We refer to the process of computing sn as the summation stage and it has to be done at each time step n from 1 to T.
This operation uses the interaction matrices Mk and the past values of the unknowns an−k with 1 ≤ k ≤ Min(n, Kmax ). A natural
implementation of this computation is to perform Kmax independent SpMVs using three nested loops. The ﬁrst loop is controlled
by index k in our formulation, it is over the interaction matrices and it goes from 1 to Kmax . The second and third loops are over
the rows and the columns of the matrices and are indexed by i and j respectively. The indexes i and j cover the unknowns and go
from 1 to N. The complete equation is written in Eq. (6) where all indexes n, k, i and j are visible

sn (i) =

Kmax N

Mk (i, j) × an−k ( j) , 1 ≤ i ≤ N .

(6)

k=1 j=1

In terms of algorithm, it is not mandatory to keep the outer loop on index k and two other orders of summation are possible
using i or j. The three possibilities are represented in Fig. 2 where all interaction matrices Mk are shown one behind another
and represented as a 3D block. This ﬁgure illustrates the three different ways to access the interaction matrices according to the
outer loop index. The natural approach using k is called by front and usually relies on SpMV (Fig. 2a). As we did in [1], we use the
approach called by slice using j as outer loop index (Fig. 2b).
3.2. Slice properties
We denote by slice the data when the outer loop index of the summation is j in Eq. (6). A Slicej is composed by the concatenation of each column j of the interaction matrices [M1 (∗, j) M2 (∗, j) . . . MKmax (∗, j)] as illustrated in Fig. 2b. This deﬁnition

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

69

Fig. 2. Three ways to re-order the computation of sn with current time step n = 6, number of unknowns N = 8 and Kmax = 6. (a) The outer loop is in the different
Mk matrices. (b) The outer loop is over the row index of Mk and sn . (c) The outer loop is over the column index of Mk .

Fig. 3. An example of Slice where the non-zero values are symbolized by gray squares and the zero values by white squares. The vector a∗ < n (j) contains the past
values of the unknown j. The vector sn will contain the current result of the summation stage for t = n.

induced the relation Mk (i, j) = Slice j (i, k) and thus a slice is a sparse matrix of dimension (N × Kmax ). It has a non-zero value at
line i and column k if d(i, j) ≈ k · c · t, where d(i, j) is the distance between the unknowns i and j. While an interaction matrix Mk
represents the interaction between the unknowns for a given time/distance with coeﬃcient k, a Slicej represents the interaction
that one unknown j has with all others over the time. This provides the main property of the sparse structure of a slice: the
non-zero values are contiguous on each line. As illustrated by Fig. 1, it takes several iterations for the wave emitted by an unknown to cross over another. In other words, for a given row i and column j all the interaction matrices Mk that have a non-zero
value at this position are consecutive according to index k. In the slice format, it means that each slice has one vector of NNZ
per line, but each of this vector may start at a different column k which correspond to the delay between the emission and the
reception of a wave. If it takes p time steps for the wave from j to cross over i, then Slice j (i, k) = Mk (i, j) = 0 for ks ≤ k ≤ ks + p
where ks = d(i, j)/(c t ). We refer to these dense vectors in each row of a slice as the row-vectors. Using the interaction matrices,
we multiply a matrix Mk by the values of the unknown at time n − k (i.e. an−k ( ∗ )) to obtain sn . When we work with slices, we
multiply each slice Slicej by the past value of the unknown j (i.e. a∗ < n (j)). An example of a slice is presented in Fig. 3.
When computing the summation vector sn , we can perform one scalar product per row-vector. Then, sn can be obtained with
N × N scalar products (there are N slices and N rows per slice) instead of Kmax SpMVs.
3.3. Slice computational algorithm with multiple steps
The scalar product, which is a level 1 BLAS, has a low ratio of ﬂoating point operations (Flop) against loaded data. In fact, one
needs to load one value from both vectors in order to perform one multiplication and one addition. More precisely, by calling d
the dimension of the vector, we need to load 2 d + 1 values to perform 2d Flop. Fig. 4a shows how we compute a slice using one
scalar product per row. In [1], we proposed two optimizations to increase this ratio.
The ﬁrst optimization is to work with multiple summation vectors at a time. At time step n, we try out to compute the current
time step summation vector sn and the next step summation vector sn+1 together. When computing a summation vector, we
use the slice matrices which remain constant and the past values of the unknowns relatively to a given time step. The vector sn
requires the past values ap , with p < n, whereas the vector sn+1 needs the past values a p with p < n + 1. In other words, sn+1
needs ap plus the present state an which has not been computed yet. If we replace an by zero we are able to compute sn and a part
of sn+1 together but sn+1 is incomplete. Therefore, we perform a matrix-vector product instead of a scalar product. The vectors
are the non-zero row-vectors of the slices and the matrices are the past values which match the summation vectors s. We denote
by ng the number of summation vectors that are grouped together and the Fig. 4b shows the different computation possibilities
with ng = 3. Once the summation is done, if ng = 2, we end up with sn and sn+1 and we continue the usual algorithm with sn and

70

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

Fig. 4. Summation stage with Slicej and 3 possibilities. (a) With ng = 1 using scalar product. (b) With ng = 3 using matrix/vector product (X and Y are the values
of a, not yet available and replaced by zero). (c) With ng = 3 using the multi-vectors/vector product.

Fig. 5. Computing one slice-row with 3 vectors (ng = 3): (a) using 3 scalar products and (b) using the multi-vectors/vector product.

obtain an after the resolution step (Eq. (5)). Then, this result an is projected to sn+1 , using a SpMV and M1 , and allows us to obtain
the complete summation vector sn+1 . We refer to this projection as the radiation stage. It is possible to have ng greater than 2,
but the higher ng , the more important the radiation stage. In this conﬁguration, we load d + ng + d × ng data to perform ng × 2d
Flop.
The second optimization takes into account the properties of the past values. When working with the Slicej , we need the past
values of the unknown j: sn needs ap < n (j) and sn+1 needs a p <n+1 ( j). The vector a p <n+1 ( j) is equal to the vector ap < n (j) where
each value is shifted by one position and the ﬁrst value is an (j) (or zero if this value does not exist at that time). In order to
increase the data reuse, we consider only one past values vector for the ng summation vectors involved in the process. We take
the existing values ap < n (j) and append one zero to each ng > 1 as it is shown in Fig. 4c. In this case, we load d + ng + (d + ng − 1)
data to perform ng × 2d Flop. This operator is called multi-vectors/vector product and is detailed in Fig. 5.

4. Related work
The advent of many-core era has pushed the High Performance Computing community to develop methods and tools to take
advantage of heterogeneous nodes. Apart from numerous challenges in this ﬁeld, our work relates to two mains topics. The ﬁrst
important topic is dedicated to the development of eﬃcient kernels on existing architectures. The second issue includes the
distribution of workload, the scheduling of tasks and the pipelining to ﬁll up the processing units.
The linear algebra community addresses both problems and proposes very advanced solutions. Intrinsic kernels have been
developed for decades and there are eﬃcient reference implementations on almost all architectures. We refer to the following
non-exhaustive studies on CPU [16,17] and GPU [7,8], but we also include the vendors libraries which are used by many scientiﬁc
applications: the Intel MKL on CPU and the NVidia cuBLAS on GPU. In order to deal with different kinds of processing units inside
a node, many state-of-the-art solvers successfully pipelined the work using runtime systems [18–21]. However, in all these cases
the runtime systems help to deal with various kinds of tasks and to manage the dependencies between them. It is inapplicable
in the case of our problem because we are optimizing a stage composed of a single type of operations.

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

71

The formulation we use from [3] relies on the SpMV which has been widely studied on CPUs and GPUs because it is an
essential operation in many scientiﬁc applications. The SpMV is a bandwidth/memory bound operation which hardly achieves
20% of the peak performance on CPU [9] and 5% on GPU [10]. Our work is not an optimization or an improvement of the general
SpMV on GPU because we use a custom operator that matches our needs and which is between the SpMV and the dense general
matrix-matrix multiplication (GEMM). The optimizations of our implementation on GPU have been inspired by recent works
that include eﬃcient data structures, memory access pattern, global/shared/local memory usage and auto-tunning, see [11–14].
These studies show that the SpMV on GPU has a very low performance against the hardware capacity and motivate the use of
blocking which is crucial to improve performance. The method to compute multiple small matrix/matrix products from [15] has
many similarities with our implementation (e.g. the use of templates).
Whether or not we use the SpMV or our custom operation the workload has to be balanced between the CPUs and the GPUs.
To solve this issue we rely on a mix between static and dynamic balancing. Pure static balancing can be appropriate as in [22]
where authors show interesting results on Sparse Grid Interpolation. However, one must be able to estimate the performance
that each processing unit will achieve for a given problem or a subdivision of it in order to balance the work statically. In [23],
the authors propose an approach which is close to ours. They divide the work between the CPUs and the GPUs for the Linpack
benchmark and they use the result of an iteration to balance the next one better. However, they have more constrains than us
because their problem is composed of several different kinds of tasks and they need to move data continuously between hosts
and devices.
There are concurrent implementations and studies on TD-BEM solvers. In [4], the authors have implemented a TD-BEM application and their formulation is similar to the one we use. They show results up to 48 CPUs and rely on sparse matrix-vector
product without giving details on the performance. In [5], the author uses either multi-GPUs or multi-CPUs parallelization and
accelerates the TD-BEM by splitting near ﬁeld and far ﬁeld. The study shows an improvement of the GPUs against the CPUs and
focuses on the formulation of the near/far computation. The proposed method is a competitive implementation, but it is diﬃcult
to compare to ours because we consider a different formulation. In [6], the authors give an overview of an accelerated TD-BEM
by using Fast Multipole Method. However, their study does not focus on performance, but rather on a new formulation and its
numerical study which makes it diﬃcult to compare to ours in term of performance.
5. Slice computation on GPU
On CPU it is possible to achieve performance by processing the row-vectors individually, but this kind of approach is no longer
eﬃcient on GPU because of its hardware particularities that we resume brieﬂy before introducing two methods to compute a
slice on this device. One can see a GPU as a many-threads device or a large SIMD processor unit. It executes several teams of
threads, usually called blocks. In this paper, we explicitly refer to this set of threads as a thread-block to avoid the confusion with
a block which is a matrix in our study. The number of thread-blocks represent the dimension of the GPU grid. When running a
GPU kernel, one has to choose the number of thread-block and the number of threads inside a thread-block. There are different
levels of memory in most GPUs. The global memory can be accessed and shared by all the threads even from different threadblocks, but it is slow and even slower if accesses are unaligned/uncoalesced inside a thread-block. The shared memory is fast
and shared among the threads of a thread-block, but it is very limited in size and can be slowed down because of bank conﬂicts.
Finally, each thread has dedicated registers/local memory.
To provide a many-thread approach we work with a block of values instead of vectors. We present two cut-out strategies
that transform a slice in blocks, the Full-Blocking and the Contiguous-Blocking approaches. We call these pieces blocks and their
dimension br (the number of rows) and bc (the number of columns). In both cases bc should be at least equal to dmax which is the
longest row-vector in all the slices of the input simulation.
5.1. Full-blocking approach
In this approach we extract and copy parts of the slices into blocks and leave the original structure of the values unchanged.
A block contains the row-vectors that are inside a 2D interval of dimension br per bc . If two row-vectors are separated by more
than br rows or, if they have some values separated by more than bc columns, they cannot be in the same block. There are several
algorithms to cut out a slice matrix into blocks. Our implementation is a greedy heuristic which has a linear complexity with
respect to the number of rows in the slices. The algorithm starts by creating a block at the ﬁrst row (for the ﬁrst row-vector).
Then, it progresses row by row and starts a new block whenever the current row-vector does not ﬁt in the current block. Fig. 6
shows an example of a cut-out using this algorithm and the resulting blocks. The generated blocks remain sparse in most cases.
The algorithm needs a pair of integers for each block which corresponds to the position of the upper-left values inside the
original source slice as shown in Fig. 6c. We have compared this algorithm to a dynamic block height br by increasing the blocks
until a row-vector is out or even by using a dynamic programming approach to have as few blocks as possible (results are not
included in this study). However, the extra-costs of seeking the best block conﬁguration or having unpredictable block sizes are
too signiﬁcant.
The computation we have to perform is identical to the multi-vectors/vector product introduced in Section 3.3 and is called
a multi-vectors/matrix product. It is also similar to a matrix/matrix product with a leading dimension of one in the past values
(which is not a matrix, but a special vector).

72

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

Fig. 6. Example of a slice cut-out into blocks with the Full-blocking approach: (a) the original slice, (b) the blocks found by the greedy algorithm and (c) the
blocks as they are going to be computed with their corner positions in the original slice. The block dimension is bc = 7 × br = 7.

Fig. 7. Example of the computation of a block from Full-blocking on GPU with ng = 3, bc = 16 and br = 9: (a) the original slice is split into blocks during the
pre-computation stage, (b) the blocks are moved to the device memory for the summation stage and (c) a group of threads is in charge of several blocks and
compute several summation vectors at the same time by performing a multi-vectors/matrix product. In c − I) the threads copy the associate past values, in c − II)
each thread computes a row and in c − III) threads add the results to the main memory.

In this paragraph and in Fig. 7 we give some implementation details of this operator on GPU. A thread-block of br threads is in
charge of several blocks (all of dimension br × bc ) from a slice interval. First, the threads copy the past values needed by a block in
a shared memory array of size bc + ng − 1. Each thread treats one row per block and computes ng results. The outer loop iterates
bc times over the columns of the block. The inner loop iterates ng times to allow the threads to compute the result by loading a
past value and using the block values. The ng results are stored into local/register variables which are written back to the main
memory once a thread has computed its entire row. In this approach the threads read the block values from the global memory
once and in a coalesced scheme column by column (the blocks are stored in column major). Also, ng and bc are known at compile
time, thus the loops over the columns and the results can be unrolled. This is possible using C + + templates. We compile lots
of different kernels and select the appropriate one at runtime. There is no bank conﬂict because all the threads read the same
values at the same time from the shared memory. Therefore, there are only broadcast operations.
The drawback of this method is the number of generated blocks and thus the extra-zeros padding. In the worst case this
method can generate one block of dimension br × bc per row. Such conﬁgurations occur if br is equal to one or if each row-vector
starts at a very different column compared to its neighbors. So bc should be large enough to reduce the number of blocks, but the
larger bc is, the more zeros are used to pad. The numbering of the unknowns is also an important criterion, because the positions
of the NNZ values, and thus the number of blocks, depend on it. In Section 7.4.1 we study the number of blocks generated for
different numbering on a realistic test case. However, the main advantage of this method is that all rows in a block depend on
the same past values.

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

73

Fig. 8. Example of a slice cutting-out into blocks with the Contiguous-Blocking approach: (a) the original slice, (b) the block build one row after the other and
(c) the blocks as they are stored and computed with the starting point of each row in the original slice. The block dimension is bc = 7 and br = N.

Fig. 9. Example of the computation of a block from Contiguous-Blocking on GPU with ng = 3, bc = 11 and nb − threads = 9: (a) the original slice is transformed
in a block during the pre-computation stage, (b) the blocks are moved to the device memory for the summation stage, (c) a thread-block is in charge of the blocks
from a slice interval and computes several summation vectors at the same time by performing a multi-vectors/matrix product. In c − I) the threads copy the past
values associated with a slice, in c − II) each thread computes a row and read the past values that match its own row and in c − III) threads add the results to the
main memory.

5.2. Contiguous-blocking approach
In this approach we do not keep the original structure of the values from a slice in the output blocks. We copy all the rowvectors into a block no matter where they start and where their values are positioned. In the Full-Blocking, all rows from a block
have been copied from the same columns in the original slice. However, this is not guaranteed in the Contiguous-Blocking and
each row of a block may come from different columns of the slices as shown in Fig. 8. That is why we need to store the origin of
the rows in the slices inside a vector to be able to compute them with the correct past values, see Fig. 8c. It is possible to create
several blocks per slice, but here we consider that br = N and we create one block per slice.
The kernel implementation of the Contiguous-Blocking is very similar to the Full-Blocking, except that it must take into
account the column differences as shown in Fig. 9. Instead of copying the past values needed for a block in the shared memory,
the threads copy all the past values needed for a slice which is a vector of length K max + ng − 1. The threads of a thread-block
do not access the same past values, but each thread accesses the past values that match the starting point of the row it has to
compute. The threads continue to read the block values as they do in the Full-Blocking with a regular pattern. We may not be
able to create a thread-block with N threads in it and in this case some threads will be in charge of the computation of several
rows of a block. With this implementation there might be some bank conﬂicts because the shared memory accesses are driven

74

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

Fig. 10. Example of workers in a node composed of 12 CPU and 3 GPUs: (a) the CPU-worker composed of all the CPUs that are not in charge of a GPU, (b) three
GPU-workers each composed of a couple of CPU/GPU.

by the original column positions of the values: the threads might need the same values or different values in the same banks or
different values in different banks.
The Contiguous-Blocking generates one block per slice. The number of columns in a block bc must be at least equal to the
longest row-vector dmax and there is no gain to have bc greater than dmax . Knowing the block size and the number of unknowns,
we know the total number of values in the system generated by the Contiguous-Blocking: N × N × bc . By calling dav the average
length of the row-vectors in the simulation, there are N × N × dav NNZ values and the blocks are padded with bc − dav zeros per
row in average. The memory cost of this approach is N × bc ﬂoating values and N integers per block generated from one slice,
plus a copy of Kmax + ng − 1 ﬂoating values from the global to the shared memory and one matrix of N × ng ﬂoating values for
the results. The total number of Flop is N × br × 2, but the effective/real number of Flop is N × dav × 2 per block.
6. Parallel heterogeneous algorithm
6.1. Distributed memory algorithm
Parallelization. We consider the parallelization approach proposed in [1] for CPU where each node has an interval of the slices
and computes an amount of the summation sn . Then all nodes are involved in the factorization/resolution and compute the
current time step an , Eq. (5). Communications between nodes occur only during the solve steps of the direct solver.
Inter-nodes balancing. In our conﬁguration, we consider that the nodes are similar and have the same computational capacity.
We try to make each node have an interval of the slices that contains the same amount of data in order to balance the summation
work. Since the resolution is a critical operation that involves all the nodes, balancing the summation work between nodes is
crucial for the application to reduce the synchronization time between iterations.
6.2. Shared memory algorithm
Parallelization. A worker deﬁnes a processing unit or a group of processing units on the same node. We dedicate one CPU core
to manage one GPU and to be in charge of the kernel calls and the data transfers between the host and the device. Thus, a
GPU-worker is a couple of CPU/GPU. All the cores from a node that are not in charge of a GPU are seen as a single entity called
CPU-worker. So one node is composed of one GPU-worker per GPU and one single CPU-worker as shown in Fig. 10. All the workers
take part in the summation step, but only the CPUs are involved during the factorization of M0 and the T resolutions.
Inside a CPU-worker we balance the work between threads as we do with nodes by assigning the same amount of data to
each thread.
Dynamic balancing between workers. A node is in charge of an interval of the slices [Ai ; Bi ] and computes a part of the summation
sn , Eq. (3). However, this interval has to be balanced between the different workers within the node. We constrain each worker
to have a contiguous interval of data/slices which enables copying and moving in one call. Also, workers can have different
computational capacities and the slices can have different costs. Therefore, the problem is to ﬁnd the optimal interval for each
worker that cover the node slices and with the minimum wall time. The wall time is the maximum time taken by a worker to
compute its interval.
One possibility to solve this problem is to perform a calibration and to estimate the speed of each worker. But such an approach
takes a non-negligible time and it is diﬃcult to consider all the possible conﬁgurations and data transfers. We could also perform
a warm-up stage and have each worker computes the full slice interval to know the computation time taken for each worker for
each slice. Not only this process can be extremely slow but the fastest worker for each slice individually may not be the fastest
to compute a given interval. In fact, GPUs are much more limited by their memory than CPUs, and if we assign a slice interval
to a GPU that does not ﬁt in its memory, it will induce very slow copies (from CPU to GPU or even from hard-drive to GPU). We
propose an heuristic to balance the work after each iteration in order to improve the next ones. There are plenty of methods that
can perform such an operation and we propose a greedy algorithm with a (W) complexity, where W is the number of workers.
The algorithm we propose considers that the average time ta of all workers in the previous iteration is the ideal time and
the objective for the next iteration. For the ﬁrst iteration we assign the same number of slices to each worker. At the end of the

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

75

Fig. 11. Illustration of one iteration of the balancing algorithm with 4 workers. Workers 1 and 3 are above the average and should drop 3 blocks and 1 block, respectively (red blocks). Workers 0 and 2 are under the average and should acquire 1/4 and 3/4, respectively of the dropped blocks (blue blocks). (For interpretation
of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)

iteration, each worker wi has computed its complete interval of slices [ai ; bi ] in time ti . We do not measure the time taken for
each individual slice, but we have an estimation of the cost ci = ti /si , with si = bi − ai + 1 as the number of elements computed
by the worker of index i.
Workers that were slower than average (if ta < ti ) should reduce their intervals. However, the faster workers (if ti < ta )
should increase their intervals and compute more slices. We consider that each slice on a given worker has the same cost.
Therefore, for a slow worker wi we remove ri = (ti − ta )/ci slices from its interval. We would like to do the same for a faster
worker and add oi = (ta − ti )/ci to its intervals. But in most cases the number of elements to remove from the slower workers
is not equal to the number of elements we want to give to the faster workers. For example, in a system with two workers and
the following properties in the previous iteration: worker w1 has computed s1 = 10 elements in t1 = 10s and worker w2 has
computed s2 = 3 elements in t2 = 4s. The average execution time is ta = (10 + 6)/2 = 8s and the ﬁrst worker should remove r1 =
(10 − 8)/(10/10) = 2/1 = 2 elements, whereas the second worker should increase its interval by o2 = (8 − 4)/(4/3) = 4/(4/3) =
3 elements.
Thus, the faster workers have to share the slices that have been removed. We sum the number of available slices Sremoved =
ri as per the number of required slices Sgiven = oi and distribute them using a weight coeﬃcient. A faster worker will have
its interval increased by oi ∗ Sremoved /Sgiven which guarantees an equality between the slices removed and given. The number of
slices to compute is updated for each worker (si = si − ri or si = si + oi ∗ Sremoved /Sgiven ) and then a new contiguous slice interval
is assigned to each worker. An example of this heuristic is presented in Fig. 11.
This algorithm should ﬁnd an optimal solution in one iteration if there is the same amount of work per element. However,
there is no guarantee that the algorithm can ﬁnd the optimal solution or even that it improves as it iterates. Theoretically, we
can stop the algorithm if a conﬁguration does not give a better result than the previous one, but in practice some latency or
unpredictable events make this statement unsafe. Therefore, in practice we stop the algorithm after a given number of iterations
and rollback to the best conﬁguration that was generated.

7. Performance and numerical study
7.1. Experimental setup
Hardware conﬁguration. We use up to 8 nodes and each node has the following conﬁguration: 2 Hexa-core Westmere Intel Xeon
X5650 at 2.67GHz and 36GB (DDR3) of shared memory and 3 NVIDIA Tesla M2070 GPU (1.15GHz), 448 Cores, 6GB (GDDR5) of
dedicated memory. Peak performances are 21.36 GFlop/s in single and 10.68 GFlop/s in double for one CPU core and 1.03 TFlop/s
in single and 515 GFlop/s in double for one GPU. The nodes are connected by an Inﬁniband QDR 40 Gb/s network.

Compiler and libraries. We use the GCC 4.7.2 compiler, OpenMPI 1.6.5 library and CUDA SDK 5.5. The compilation ﬂags for GCC
are -m64 -march=native -O3 -msse3 -mfpmath=sse and for NVCC -arch=sm_20 -use_fast_math. The direct solver is the stateof-the-art Mumps 4.10.0 solver [27], and relies on Scotch 5.1.12b. The calculations are performed in Single (32-bit arithmetic)
or Double (64-bit arithmetic). Parallelization over nodes is supported by MPI [28] and parallelization inside nodes over shared
memory by OpenMP [29].

76

B. Bramas et al. / Parallel Computing 49 (2015) 66–82
Table 1
Balancing Algorithm vs. Optimal Choice. Extra-cost of the dynamic balancing algorithm against the optimal
choice after 40 iterations with 6 workers and 10, 000 elements. A zero score means that the optimal choice
has been achieved.

Worker heterogeneity

Work distribution

Up-Down

0.0%

0.1%

0.1%

0.0%

Up-Up

0.0%

0.0%

0.1%

0.1%

Up

0.1%

0.1%

0.2%

0.2%

Random

0.0%

0.0%

0.2%

0.0%

Stable

0.0%

0.0%

0.1%

0.1%

Table 2
Performance of computing the blocks from Full-Blocking. Performance in GFlop/s for 420 slices composed of 6400 rows and bc columns and ng = 8. % percentage of the Peak performance against single|double: GPU 1.03 TFlop/s|515 GFlop/s, CPU 21.36 GFlop/s|10.68 GFlop/s.
Width (bc )

Single

Double

16
GPU

32

64

Height (br )
32
64
128
32
64
128

53
124
138
39
68
73

64
176
196
51
95
100

105
230
244
67
128
128

128

16
CPU

139
270
280 (27%)
68
136
141 (27%)

2.9
8.8
8.2
2.5
3.4
3.8

32

8.4
7.5
7
3.6
3.1
3.1

64

8.8
6.7
6.4
3.4
3
3

128

8.2
6.3
6 (28%)
3.5
2.8
2.9 (27%)

7.2. Balancing quality
In Section 6.2 we described the heuristic we use to balance the summation work among the workers inside a node. In Table 1
we test this balancing method mathematically to know how far from the optimal distribution it can be. We do not perform real
simulations, but we rather generate various conﬁgurations composed of two arrays. The ﬁrst array represents the workloads,
which is the cost per element, and we refer to it as the Work Distribution (an element can be seen as a slice in our study). The
second array contains the performance of the workers, which is the time taken per a worker to compute a unit of work, and we
refer to it as Worker Heterogeneity. To get a worker virtual execution time we multiply the sum of the costs from the interval that
is assigned to this worker per its performance factor. The virtual wall time of a distribution is obtained by taking the maximum
virtual execution time from all workers involved. The balancing algorithm is executed during a given number of iterations and
compared to the optimal choice. We ﬁnd the optimal balance using dynamic programming, that is ﬁnding the contiguous interval
for each worker that has the minimum wall time. The results show that the balancing algorithm is close to the optimal even when
the cost per element or the worker performance factors are very heterogeneous.

7.3. Sequential Flop-rate
7.3.1. Full-blocking kernel Flop-rate
Table 2 presents the Flop-rate of the GPUs and the CPU implementations for the blocks generated by the Full-Blocking method
described in Section 5.1. We look at the Flop-rate achieved by one GPU or one CPU core for different sizes of dense blocks. We
create thread-blocks of br threads and the best dimension of the grid (the number of thread-blocks) depends on the size of
the block and on the number of blocks. The performance increases with the block size on GPU because increasing br increases
the number of threads in a thread-block and increasing bc provides more work to each thread and allows to unroll larger loops.
The CPU implementation beneﬁts also from the instruction pipelining when we increase br or bc , but its performance decrease
when the block overtakes a size (bc > 16 and br > 32) due to cache effect. We remind that there are zero values in the blocks and
that increasing the size of the blocks should reduce the number of generated blocks, but it also increases the zero padding. So
ﬁnding the correct dimension is a matter of ﬁnding the fastest kernel for the generated number of blocks (% of NNZ).

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

77

Table 3
Performance of computing the blocks from Contiguous-Blocking. Performance in GFlop/s
for 420 slices composed of 6400 rows and bc columns and with 14 GPU thread-blocks of
512 threads and ng = 8. % percentage of the Peak performance against single|double: GPU
1.03 TFlop/s|515 GFlop/s, CPU 21.36 GFlop/s|10.68 GFlop/s.
Width (bc )

Single
Double

16
GPU

32

64

128

16
CPU

146
82

203
113

251
139

285 (28%)
158 (31%)

2.7
2

32

3
1.6

64

2.3
1.6

128

2.3 (11%)
1.4 (13%)

7.3.2. Contiguous-blocking kernel Flop-rate
Table 3 presents the Flop-rate of the GPU and CPU implementations for the blocks generated by the Contiguous-Blocking
method described in Section 5.2. We have no choice as far as block size parameters are concerned: br is set to N and bc to dmax .
This is a drawback for the CPU version because it leads to a large block and a high leading dimension between columns but since
our goal is to concentrate on the GPU we agree to pay this extra cost. This method has better performance for the GPU against
Full-Blocking which means that it does not pay any extra-cost of its irregular/uncoalesced shared memory accesses. Moreover,
this implementation copies all the past values needed by an entire slice in the shared memory which is an advantage compared
to the GPU Full-Blocking because it copies past values for each block (smaller copies but more frequently).
Warp occupancy and performance of the Contiguous-Blocking kernel. In Section 5 we give an overview of the GPU speciﬁcations,
but to achieve performance with this architecture we also need to focus on hardware details. Under the M2070 a thread-block
can use a maximum of 32768 registers and 49152 bytes of shared memory and supports 1536 threads. This GPU has 14 multiprocessors each with a warp of size wp = 32 and one can see this GPU as 448 CUDA Cores. However, there is no concurrency inside
a warp. Using this information we have to ﬁll all the multiprocessors by choosing the number of thread-blocks and the number
of threads.
The CUDA compiler (NVCC) gives us the memory occupancy of our kernel. Each thread uses 62 registers and one threadblocks needs 6104 bytes of shared memory to store the past vector of length K max + ng − 1 (with K max = 756 and ng = 8). The
size of the memory occupancy in the shared-memory is not related to the number of threads and the available shared-memory
does not appear as a limitation. On the other hand, the number of available registers limits the number of threads to maxth =
32768 div 62 = 528 per thread-block. If we use more threads than maxth , the registers become insuﬃcient and the kernel
will use the local memory as a replacement which is drastically slower. It is recommended that the number of threads nbth
in a thread-block is a multiple of the warp size wp; thus, for our conﬁguration nbth = 512 ≤ 528 threads. Choosing nbth not to
be a multiple of wp means that some multiprocessors will be under-exploited (some CUDA cores will remain idle). We have
implemented our kernel in order to have low registers and shared memory usage but also low divergence between threads. In
fact divergence between threads is a costly operation since it requires to perform all the different code paths of the threads
inside a warp separately. We have two divergences that may happen during the copy of the past vector into the shared memory
or during the loop over the rows. When copying the past vector, the threads with id < (K max + ng − 1)modnbt perform one extra
copy compared to the others. Since the number of threads may not be a multiple of the number of rows N, some threads could
perform more computation. Finally, all accesses to the global memory are coalesced or similar inside a thread-block.
The GPU hides the global memory latency by swapping threads on the multiprocessors, while a warp of threads is requesting
a memory access. Since we have 512 threads per thread-block, we just need to have 14 thread-blocks in order to ensure that
the 14 GPU multiprocessors are ﬁlled. Having more than 14 thread-blocks is not needed as it could deliver worse performance
because of the costs for the GPU to manage thread-blocks and increase the reduction work of the result matrices. The source
code of the Contiguous-Blocking kernel implementation is given in Appendix A.1.
7.4. Test case
We now consider a real simulation to study the parallel behavior of the application. Our test case is an airplane composed of
N = 23, 962 unknowns shown in Fig. 12. The simulation has to perform 10 823 time iterations. There are K max = 341 interaction
matrices. The total number of non-zero values in the interaction matrices, except M0 , is 5.5 × 109 . The longest row-vector dmax
is 15 and the row-vectors have 9.5 values in average. For one iteration, the total amount of Flop to compute the summation sn is
about 11 GFlop. If we consider that the solution step of the system associated with M0 has the cost of a matrix-vector product,
the total amount of Flop for the entire simulation is 130, 651 GFlop. We compute the test case in single precision. In this format
we need 50 GB to store all the data of the simulation. Our application can execute out-of-core simulations, but we concentrate
our study on in-core executions. We need at least 2 nodes to have the entire test case ﬁtting in main memory.
7.4.1. Number of blocks from the full-blocking
The Full-Blocking generates blocks from a given slice depending on the block size (br and bc ) and the position of the NNZ values
inside the slices. Therefore, the numbering of the unknowns is crucial to decrease the number of blocks. We tested different
ordering methods, but the spatial ordering gave better results (the comparison is not given in the current study). For example, we

78

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

Fig. 12. Illustration of the Airplane test case.
Table 4
Number of blocks for the Full-Blocking and the airplane test case. Number of blocks generated by the Full-Blocking method
in the airplane test case for different block sizes (br × bc ) and different orderings. For each size and ordering we show the
number of blocks (NBB), the percentage of non-zeros in the blocks (NNZ%) and the estimated time to compute the blocks
with one GPU when using the GPU kernel performance measures (E ≈ ). Best times for each ordering are highlighted in the
table.
br × bc

No ordering
NBB

32 × 16
32 × 32
32 × 64
32 × 128
64 × 16
64 × 32
64 × 64
64 × 128
128 × 16
128 × 32
128 × 64
128 × 128

121 × 10
115 × 106
114 × 106
114 × 106
74 × 106
63 × 106
59 × 106
58 × 106
48 × 106
32 × 106
25 × 106
22 × 106
6

Morton indexing
NNZ%
8.8%
4.6%
2.3
1.1
7.2
4.2
2.2
1.1
5.6
4.1
2.6
1.4

E≈

NBB

3.1
4.6
7
13.7
2.25
2.75
3.82
7
2.71
2.68
3.30
5.27

36 × 10
18 × 106
9 × 106
5 × 106
36 × 106
18 × 106
9 × 106
4 × 106
35 × 106
17 × 106
9 × 106
4 × 106
6

NNZ%
29.6
29.1
27.5
23.8
14.9
14.9
14.8
14.4
7.5
7.4
7.4
7.4

Hilbert indexing
E≈
0.95
0.74
0.59
0.68
1.08
0.78
0.58
0.56
2.02
1.47
1.15
1.06

NBB
36 × 10
18 × 106
9 × 106
6 × 106
36 × 106
18 × 106
9 × 106
4 × 106
35 × 106
17 × 106
9 × 106
4 × 106
6

NNZ%
29.6
28.9
27.3
22.4
14.9
14.9
14.7
14.3
7.5
7.5
7.4
7.4

E≈
0.95
0.75
0.6
0.72
1.08
0.78
0.58
0.56
2.02
1.47
1.15
1.05

tried to number the unknowns by solving a Traveling Salesman Problem, using one or several interaction matrices as proposed in
[26]. Here we present the results for the Morton indexing [24] and the Hilbert indexing [25]. In both cases we compute a unique
index for each unknown and sort them accordingly. It is possible to score the quality of the ordering by looking at the contiguous
j
j
j
values between the row-vectors. If two consecutive row-vectors vi and vi+1 from Slicej have qi,i+1 values on the same columns,
we describe the quality between these two rows as (qi,i+1 )2 . The quality of a slice is the average quality between its rows and the
quality of the entire system is the average quality of the slices. Using the original ordering provided by the mesh generation, the
ordering score for all the slices is 33. By ordering the unknowns using Morton and Hilbert index, we obtain the scores 54 and 55,
respectively. This means that using these spatial ordering increases the quality and makes most of the consecutive row-vectors
having contiguous NNZ values in the same columns.
Table 4 shows the number of blocks depending on the type of ordering and the size of the blocks when we process the slices
of the test case using the Full-Blocking from Section 5.1. It is clear that numbering the unknowns with a space ﬁlling curve
drastically reduces the number of blocks (NBB in the table). The table also contains an estimation of the computation time E ≈ to
process the generated blocks with one GPU, using the performance measures from Table 2. We can see that increasing the size
of the blocks does not always reduce the number of blocks. For example, with the Morton Indexing the br parameter does not
improve the ﬁlling of the blocks signiﬁcantly.
These results show the limits of the Full-Blocking approach. The best estimated time E ≈ which is obtain using Morton
Indexing have only 14.4% of NNZ values. That means that the memory requirement is multiplied by more than 6 and that the
real kernels performance is divided by 6. Moreover, in order to ﬁnd out the best block size, we need to do a complete study (at
least try different blocks size for a given ordering) which cannot be carried out in a real simulation. The Contiguous-Blocking
approach, on the other hand, only needs to know the longest row-vector dmax which is 15 in the airplane test case and can be
deduced from the simulation properties. With an average length of 9.5 for the row-vectors and 23, 962 unknowns we have 63%
of NNZ using Contiguous-Blocking. Moreover, Contiguous-Blocking is faster on GPU than Full-Blocking. Therefore, we do not use
the Full-Blocking method in the rest of the paper to run the simulation.
j

7.4.2. Parallel study
We now study the parallel behavior of the application. Fig. 13a shows the wall time to compute the simulation using 0–3 GPUs
and 2–8 nodes. Based on these results, Fig. 13b shows the speed-up of the GPU versions against the CPU only version. We recall

CPU-Only

1GPU

2GPU

3GPU

1GPU

3,000

79

2GPU

3GPU

4.0
Speedup

Time (seconds)

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

1,000

3.0
2.0
1.0

300
2

3 4 5 6 7
Number of nodes

8

(a) Execution time

2

3 4 5 6 7 8
Number of nodes

(b) Speedup against CPU-Only

Percentage (%)

Fig. 13. Parallel study of the airplane test case from 0 to 3 GPUs and 2 to 8 nodes.

100
80
60
40
20
0

2

5
Number of nodes

8

100
80
60
40
20
0

2

Percentage (%)

(a) CP U − Only

100
80
60
40
20
0

2

5
Number of nodes
(c) 2GP U

5
Number of nodes

8

(b) 1GP U

8

100
80
60
40
20
0

2

5
Number of nodes

8

(d) 3GP U

Fig. 14. Percentage of time of the different stages of the airplane simulation for 2–8 nodes.

that the GPU versions use all the CPUs as explained in Section 6.1. For a small number of nodes the executions with GPUs do not
provide a signiﬁcant improvement against the CPU only case. This is because GPUs are limited by their memory capacities and
they cannot hold an important proportion of the work when the number of nodes is low. Since the data are almost divided by
the number of nodes, a small number of nodes means that each of them will need to store a large amount of data. When a GPU is
in charge of an interval that exceeds its memory capacity, it will need to perform host to device copies during the computation.
Such copies are slow and drastically decrease the eﬃciency of the GPUs. However, our application must be able to support outof-core executions where an entire simulation cannot ﬁt inside the main memory. It is then required to perform host to device
or hard-drive to device copies. The balancing algorithm is in charge of the attributions of the intervals as detailed at the end of
this section. The parallel eﬃciency of the CPU only version for 8 nodes is 0.78.
Fig. 14 presents the details of the main operations of the simulation by giving the percentages of the operations against the
complete wall time. We see that the idle time (red) remains low in most of the cases. It is clear, however, that the solution stage
becomes more and more dominant as the summation is improved. That is due to the low number of unknowns compared to the
number of nodes. In fact, there is no improvement in the time spent in the solution step of the direct solver as we increase the
number of nodes and MUMPS is taking the same time with 2 or 8 nodes to solve the system for all the time steps.
We can draw the work balance between workers as shown in Fig. 15. It shows how the work is balanced for different work
distributions and different numbers of nodes in a single node for several iterations. We measure the balance by studying the

10,000
5,000
0

0 5 10 15
Number of Iteration
(a) 2 Nodes, 1 GPU, Node no 1

50
0
0 5 10
Number of Iteration
3,000
2,000
1,000
0

0 5 10
Number of Iteration
(b) 8 Nodes, 2 GPUs, Node no 6

Percentage of time (%)

0
0 5 10 15
Number of Iteration

100

Interval of work

50

Interval of work

Interval of work

100

Percentage of time (%)

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

Percentage of time (%)

80

100
50
0
0 5 10 15
Number of Iteration
6,000
4,000
2,000
0
0 5 10 15
Number of Iteration

(c) 4 Nodes, 3 GPUs, Node no 0

Fig. 15. Illustration of the work balance for the airplane simulation and three conﬁgurations. The objective is to have equal percentage of time per worker. The
CPU-worker is always represented by green color (always at the top of the plots). GPU-workers are plotted in blue. (For interpretation of the references to color
in this ﬁgure legend, the reader is referred to the web version of this article.)

time taken per each worker in term of percentage of the total time (which is the sum of the time taken by all the workers). In
a perfectly balanced conﬁguration all the workers take 100/W percent of the time, with W the number of workers. We see that
the balance is improved after each iteration. The second part of the ﬁgure shows the interval of work per worker. We see the
speed-up of the GPU-workers against the CPU-worker when the work is balanced and what percent of the node interval each
worker is in charge of.
We remind that for the ﬁrst iteration the balancing algorithm gives the same number of elements to each worker. Fig. 15 a
which shows the balancing for 2 nodes also point out a problem in the limitation of the GPU memory. In fact, at the ﬁrst iteration
the GPU-worker and the CPU-worker are in charge of the same number of slices (as shown by the interval of work). However, this
amount of slices cannot ﬁt in GPU memory; thus, the ﬁrst iteration is very slow for the GPU-worker and it takes 90% of the total
time. The GPU-worker needs to load and copy the slices into its memory. The balancing algorithm tried to balance the second
iteration, but this time the GPU-worker has few slices and can store them in its memory. Therefore, the GPU-worker, compared
to the CPU-worker, computes its interval extremely fast. Such performance differences as in-GPU and out-of-GPU can lead to
unbalanced or at least not optimally balanced conﬁgurations.

8. Conclusions
We have proposed an eﬃcient data decomposition and load balancing algorithm to compute the TD-BEM on heterogeneous
nodes. We point out that advanced blocking techniques are required in order to eﬃciently compute slice matrices on GPU. Two
methods are presented, but only the Contiguous-Blocking can be used in realistic simulations. In fact, this method generates
blocks with few extra-zero padding and requires only the choice of the br parameter. Moreover the Contiguous-Blocking GPU
kernel achieves a high Flop-rate even for small block widths. Independently, a new heuristic is proposed to balance the work
over workers with a low-cost but still eﬃcient results. It successfully balances the summation inside the node and, due to its
simplicity and degree of abstraction, it could be used in many other areas.
We also show the limits of the current implementation which is mainly the inappropriate use of a direct solver for the matrix
M0 . In our case it is clear that as the number of nodes becomes large the resolution stage is no longer improved and becomes a
bottleneck. In our case, we have a small number of unknowns, an important number of nodes and we solve the same resolution
several times. Such a conﬁguration is not usual for most of the available solvers; thus, we have to investigate and to compare
existing solvers in order to ﬁnd the most appropriate one.
We will also investigate shortly how to improve the computation of the Contiguous-Blocking kernel on CPU. We will certainly
need to implement the kernel using SIMD intrinsics (SSE, AVX) to overtake the compiler. We can expect that improving the CPU
kernel will not require any other changes in the application thanks to the balancing algorithm. Finally, we plan to perform larger
simulations using more nodes later on.

Acknowledgments
Experiments presented in this paper were carried out using the PLAFRIM experimental test bed. This work is supported by
the Airbus Group, Inria and Conseil Régional d’Aquitaine initiative.

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

81

Appendix
A.1. CUDA-C Code of Contiguous-Blocking Computation Kernel

References
[1] B. Bramas, O. Coulaud, G. Sylvand, Time-domain BEM for the wave equation: optimization and hybrid parallelization, Euro-Par, Lecture Notes in Computer
Science, 8632, Springer International Publishing, 2014, p. 511.
[2] Y.J. Liu, S. Mukherjee, N. Nishimura, M. Schanz, W. Ye, A. Sutradhar, E. Pan, N.A. Dumont, A. Frangi, A. Saez, Recent advances and emerging applications of
the boundary element method, ASME Appl. Mech. Rev. 64 (5) (2011) 1–38.
[3] I. Terrasse, Résolution mathématique et numérique des équations de Maxwell instationnaires par une méthode de potentiels retardés, 1993 Ph.D. dissertation. Ecole Polytechnique Palaiseau, France.

82

B. Bramas et al. / Parallel Computing 49 (2015) 66–82

[4] T. Abboud, M. Pallud, C. Teissedre, Sonate: a parallel code for acoustics nonlinear oscillations and boundary-value problems for hamiltonian systems, Technical report, 1982. http://imacs.xtec.polytechnique.fr/Reports/sonate-parallel.pdf
[5] F.Q. Hu, An eﬃcient solution of time domain boundary integral equations for acoustic scattering and its acceleration by graphics processing units, in: 19TH
AIAA/CEAS AEROACOUSTICS CONFERENCE, 2013, doi:10.2514/6.2013-2018.
[6] T. Takahashi, A time-domain BIEM for wave equation accelerated by fast multipole method using interpolation, 2013, 191–192, doi:10.1115/1.400549.
[7] R. Nath, S. Tomov, T.T. Dong, J. Dongarra, Optimizing symmetric dense matrix-vector multiplication on GPUs, Proceedings of 2011 International Conference
for High Performance Computing, Networking, Storage and Analysis, ACM, 2011, p. 6.
[8] Y. Li, J. Dongarra, S. Tomov, A note on auto-tuning GEMM for GPUs, Computational Science ICCS, Springer Berlin Heidelberg, 2009, pp. 884–892.
[9] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, J. Demmel, Optimization of sparse matrixvector multiplication on emerging multicore platforms, Parallel
Comput. 35 (3) (2009) 178–194.
[10] N. Bell, M. Garland, Implementing a sparse matrix-vector multiplication on throughput-oriented processors, in: Proceedings of ACM/IEEE Conf. Supercomputing (SC), Portland, 2009.
[11] M.M. Baskaran, Optimizing sparse matrix-vector multiplication on GPUS, 2009, IBM Research Report, RC24704 (W0812-047) December 8, 2008 − REVISED
April 2.
[12] M. Garland, Sparse matrix computations on manycore GPU’s, Proceedings of the 45th Annual Design Automation Conference (DAC ’08), ACM, New York, NY,
USA, 2008, pp. 2–6. http://doi.acm.org/10.1145/1391469.1391473, doi:10.1145/1391469.1391473.
[13] N. Bell, M. Garland, Implementing sparse matrix-vector multiplication on throughput-oriented processors, Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis (SC ’09), ACM, New York, NY, USA, 2009, doi:10.1145/1654059.1654078. Article 18, 11 pages.
[14] A. Monakov, A. Lokhmotov, A. Avetisyan, Automatically tuning sparse matrix-vector multiplication for GPU architectures, in: High Performance Embedded
Architectures and Compilers, 2010, pp. 111–125.
[15] C. Jhurani, P. Mullowney, A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices, 2013, arXiv:1304.7053J.
[16] K. Goto, R. Van De Geijn, High-performance implementation of the level-3 BLAS, ACM Transactions on Mathematical Software (TOMS) 35 (1) (2008) 4.
[17] M.Q. Jiang, Y.C. Zhang, G. Cong, Y.C. Li, Research on high performance implementation mechanism of GotoBLAS general matrix-matrix multiplication,
Comput. Eng. 34 (7) (2008) 84.
[18] E. Agullo, J. Demmel, J. Dongarra, B. Hadri, J. Kurzak, J. Langou, S. Tomov, Numerical linear algebra on emerging architectures: the plasma and magma
projects, J. Phys. Conf. Ser. 180 (1) (2009) 012037.
[19] F.G.V. Zee, E. Chan, R.A.V. de Geijn, E.S. Quintana-Ort, G. Quintana-Ort, The libﬂame library for dense matrix computations, Comput. Sci. Eng. 11 (6) (2009)
56–63.
[20] X. Lacoste, M. Faverge, G. Bosilca, P. Ramet, S. Thibault, Taking advantage of hybrid systems for sparse direct solvers via task-based runtimes, Parallel and
Distributed Processing Symposium Workshops (IPDPSW), IEEE International, 2014, pp. 29–38.
[21] E. Agullo, A. Buttari, A. Guermouche, F. Lopez, Multifrontal QR factorization for multicore architectures over runtime systems, in: Euro-Par 2013 Parallel
Processing, 2013, pp. 521–532.
[22] A. Murarau, J. Weidendorfer, A. Bode, Workload balancing on heterogeneous systems: a case study of sparse grid interpolation, Euro-Par 2011: Parallel
Processing Workshops, Springer Berlin Heidelberg, 2012, pp. 345–354.
[23] C. Yang, F. Wang, Y. Du, J. Chen, J. Liu, H. Yi, K. Lu, Adaptive optimization for petascale heterogeneous CPU/GPU computing, Cluster Computing (CLUSTER),
IEEE International Conference on, IEEE, 2010, pp. 19–28.
[24] G.M. Morton, A Computer Oriented Geodetic Data Base and a New Technique in File Sequencing, International Business Machines Company, 1966.
[25] D. Hilbert, über die stetige abbildung einer linie auf ein ﬂchenstück, Math. Annal. 38 (1891) 459–460.
[26] A. Pinar, M.T. Heath, Improving performance of sparse matrix-vector multiplication, Proceedings of the 1999 ACM/IEEE Conference on Supercomputing,
ACM, 1999.
[27] P.R. Amestoy, I.S. Duff, J.-Y. L’Excellent, Mumps multifrontal massively parallel solver version 2.0, 1998.
[28] M. Snir, Steve Otto & All: The MPI core, second ed., MIT press, 1998.
[29] OpenMP speciﬁcations, Version 3.1, 2011, http://www.openmp.org.

