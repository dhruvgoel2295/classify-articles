Parallel Computing 48 (2015) 59–80

Contents lists available at ScienceDirect

Parallel Computing
journal homepage: www.elsevier.com/locate/parco

Power consumption management in fat-tree
interconnection networks
M. Alonso a,∗, S. Coll b, J.M. Martínez a, V. Santonja a, P. López a
a
b

Department of Computer Engineering, Universitat Politècnica de València, Valencia 46022, Spain
Department of Electronic Engineering, Universitat Politècnica de València, Valencia 46022, Spain

a r t i c l e

i n f o

Article history:
Received 9 June 2014
Revised 13 March 2015
Accepted 19 March 2015
Available online 9 April 2015
Keywords:
Power consumption
Interconnection network
Fat-tree

a b s t r a c t
As higher communication bandwidth is required in current designs of high performance parallel computers, the amount of power consumed by the interconnection network also increases.
Fat-tree is one of the most popular topologies in high performance interconnection networks
aiming at low latency, eﬃcient collective communication and scalability. We present a new
methodology for managing power consumption of fat-tree interconnection networks. Our
proposal is based on dynamically adjusting available network bandwidth according to traﬃc
requirements. To meet this goal, we deﬁne a mechanism for managing the operating status of
network links as a function of network load. Our main contributions include a complete deﬁnition of the mechanism and a tuning methodology based on its sensitivity and aggressiveness
in terms of potential power savings. Results show that our proposal can provide signiﬁcant
power savings (up to 67% in a 4-ary 4-tree) with no changes in the underlying routing algorithm, with minimal impact on network performance. Experiments conducted on a 16-ary
3-tree topology provide up to 36% energy savings with performance degradation below 1.10%.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
For some time, the cluster architecture is becoming increasingly popular in the supercomputing arena. The list of the most
powerful computers (Top500) shows that cluster-based machines represent more than 80% and the size of the clusters is
progressively growing. The cluster architecture is also common in large Internet servers, from web servers to the latest cloud
systems.
Traditionally, computer architects have made an important effort in order to obtain high computing power. However, for
quite some time, it is equally important to consider energy consumption because power saving is becoming an important issue
in the design of any device that uses electricity as power supply. It is also the case of clusters. As an example, the Top500 list was
proposed in 1993 to classify machines based only on computational power. In 2003, the Green500 list highlights the importance
of energy eﬃciency with the introduction of the MFLOPS/watt ﬁgure of merit, that measures the rate of computation that can be
delivered by a computer for every watt of consumed power.
Power saving is a very important aspect in cluster design due to different reasons: the problems generated by high power
dissipation, such as high temperatures, that can damage the machine, plus the additional complexity involved in heat removal.
Moreover, the cost of the energy bill is getting higher and any strategy to reduce it is welcomed. Finally, all computer architects

∗

Corresponding author. Tel.: +34 963877007.
E-mail address: malonso@disca.upv.es (M. Alonso).

http://dx.doi.org/10.1016/j.parco.2015.03.007
0167-8191/© 2015 Elsevier B.V. All rights reserved.

60

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

must have a commitment to the environment, making their designs more sustainable. Following a holistic approach, power
saving techniques are applied in different aspects of computer design. With energy optimization techniques being applied in
processors and memories, attention is moving towards the interconnection network.
Interconnects are signiﬁcant contributors to the power consumed by the whole system. For example, the routers and links
in a Mellanox server blade, consume about 37% of the total power budget [1]. Greenberg et al. [2] showed that the network
consumes 10–20% of a data center total power. Abts et al. [3] indicate that a typical interconnect consumes 12% of the total
system power at full load, and up to 50% when the system is not fully utilized. They also state that a typical Google cluster spends
most of its time within the 10–50% utilization range. Furthermore, increasing need for higher bisection bandwidth and faster,
more power-consuming links, will make the network power become a ﬁrst-order contributor to the overall power budget [3].
1.1. Related work
Different power reduction techniques for interconnection networks have been proposed, either based on varying the bandwidth; e.g. DVS, or through on/off links.
Most of them are based on dynamic voltage scaling (DVS). DVS was originally proposed for microprocessors and now is
widely deployed. When applied to networks, this approach allows DVS links to work in a discrete range of frequencies and
supply voltages, which leads to different levels of power consumption in response to their traﬃc utilization. The history-based
DVS policy proposes to use past network utilization to predict future traﬃc, therefore dynamically tuning link frequency and
voltage [4]. Stine and Carter compare DVS with the use of adaptive routing in non DVS links, showing that, as long as the
network provides enough bandwidth to meet the needs of the application, an adaptively-routed network can improve latency
with the same power consumption [5]. However, Abts et al. [3] propose dynamically adjusting link data rates on data center
networks using the ﬂattened butterﬂy topology. They set a target utilization for each link and set the link rate (by halving or
doubling the current rate) according to the actual utilization. This work, however, does not allow tuning the responsiveness of
the mechanism. Situations were low link utilization is due to network congestion are not considered by this proposal. In addition,
DVS has signiﬁcant drawbacks: it requires a sophisticated hardware mechanism to ensure correct link operation during scaling,
it consumes signiﬁcant CMOS area, and links continue to consume power even while idle.
Another area of research is based on the use of on/off links that are selectively switched on and off according to their
utilization [1,6,7]. These proposals require the interconnection network to use complex adaptive routing algorithms in order to
avoid deadlocks. Koibuchi et al. apply the same strategy to Ethernet-based clusters where link deactivation require the paths that
go through it must be changed to avoid the set of deactivated links [8]. Their proposal needs a network reconﬁguration every
time a link is activated or deactivated. Kim et al. also investigate hybrid techniques based on both DVS and on/off links. The idea
is to shut down DVS links when traﬃc drops to very low levels [1]. However, their work requires modiﬁcations in the routing
algorithm.
Fat-tree interconnection networks have become one of the most used topologies in supercomputers due to their high bisection
bandwidth and ease of application mapping for arbitrary communication topologies [9]. But most applications have communication topology requirements that are far less than the total connectivity provided by fat-trees. Vetter and Mueller show that
applications that scale most eﬃciently to large numbers of processors use point-to-point communications patterns where the
average number of distinct destinations is relatively small [10]. This provides strong evidence that many application communication topologies exercise a small fraction of the resources provided by fat-trees [11]. Moreover, traﬃc in an interconnection
network exhibits large spatial and temporal variance, leading to inactivity periods at several links in the network [7]. On the other
hand, fat-trees are particularly well-suited for applying power consumption reduction techniques since they provide multiple
alternative paths for each source/destination pair. This paper shows that there is a chance to reduce power consumption by
dynamically switching on/off links based on the traﬃc while running a set of applications. Signiﬁcant advantages of the proposed
method are that it works in a distributed way, with no need for a central controller, it keeps network connectivity and does not
require changes in the routing algorithm.
ElasticTree [12] is a network-wide power manager for fat-tree networks which dynamically adjusts the set of active switches
and links devised for satisfying changing data center loads. Their proposal is based on a central network controller that predicts
the incoming traﬃc matrix based on historical traﬃc. Yi and Singh [13] add a merge network to a fat-tree for pushing traﬃc to
the ”left” so additional switches can be put into low power modes. Their proposal requires additional hardware to be deployed
in the network and they do not provide any performance results.
In 2010, the IEEE 802.3az Energy Eﬃcient Ethernet (EEE) standard was published [14]. After analyzing various alternatives,
the task force deﬁned a framework for dynamic on/off link management. The standard allows network links to switch between
“sleep” and “wake” modes on demand, to save energy. Switches that support EEE, targeting data centers, are already commercially
available.
In this paper, we present a method to reduce power consumption in fat-tree networks based by dynamically managing on/off
links. This method can be easily applied in EEE networks, as the additional functionality required to turn links on and off is
already deﬁned. Our proposal deﬁnes a strategy for deciding which links are turned on/off and when. We propose a distributed
mechanism based on two link utilization thresholds that control link, powering up and down according to network traﬃc. This
mechanism can be easily set to provide different levels of sensitivity to traﬃc changes (responsiveness) and diverse power
saving strategies (aggressiveness). Another advantage of our proposal is it does not require changes in the routing algorithm nor
additional switching elements.

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

column
0,0
0,1

0,2

0,3

1,0

1,2

1,3

2,0

2,2

2,3

3,0

3,2

3,3

0,0,0

0,1,0

0,2,0

0,3,0

1,0,0

1,1,0

1,2,0

1,3,0

2,0,0

2,1,0

2,2,0

2,3,0

3,0,0

3,1,0

3,2,0

3,3,0

0,0,1

0,1,1

0,2,1

0,3,1

1,0,1

1,1,1

1,2,1

1,3,1

2,0,1

2,1,1

2,2,1

2,3,1

3,0,1

3,1,1

3,2,1

3,3,1

0,1,2

0,2,2

0,3,2

1,0,2

1,1,2

1,2,2

1,3,2

2,0,2

2,1,2

2,2,2

2,3,2

3,0,2

3,1,2

3,2,2

3,3,2

1,1

2,1

3,1

level

0

61

1

0 1 2

2

0,0,2

3

3,0,0
3,0,1
3,0,2
3,0,3
3,1,0
3,1,1
3,1,2
3,1,3
3,2,0
3,2,1
3,2,2
3,2,3
3,3,0
3,3,1
3,3,2
3,3,3

2,0,0
2,0,1
2,0,2
2,0,3
2,1,0
2,1,1
2,1,2
2,1,3
2,2,0
2,2,1
2,2,2
2,2,3
2,3,0
2,3,1
2,3,2
2,3,3

1,0,0
1,0,1
1,0,2
1,0,3
1,1,0
1,1,1
1,1,2
1,1,3
1,2,0
1,2,1
1,2,2
1,2,3
1,3,0
1,3,1
1,3,2
1,3,3

0,0,0
0,0,1
0,0,2
0,0,3
0,1,0
0,1,1
0,1,2
0,1,3
0,2,0
0,2,1
0,2,2
0,2,3
0,3,0
0,3,1
0,3,2
0,3,3

01 23

Fig. 1. 4-ary 3-tree node, switch and edge labels.

The rest of the paper is organized as follows. Section 2 formalizes k-ary n-tree network topology considering it as a particular
class of fat-tree, including a description of packet routing. Section 3 describes the proposed power saving mechanism, evaluating
it by simulation in Section 4. Finally, some conclusions are drawn.
2. Fat-trees (k-ary n-tree)
A k-ary n-tree is composed of N = kn processing nodes and S = nkn − 1 switches. Every switch has 2k ports, k “down” links,
labeled from 0(down) to k − 1(down), and k “up” links, labeled from 0(up) to k − 1(up). Each processing node is identiﬁed by an
n-tuple p ࢠ {0, 1, . . . , k − 1}n , while each switch is deﬁned as an ordered pair (w, l), where w ࢠ {0, 1, . . . , k − 1}n − 1 and l ࢠ {0,
1, . . . , n − 1} is the level of the switch (0 is the root or upmost level).
•

•

Two switches (w0 , w1 , . . . , wn − 2 , l) and (w0 , w1 , . . . , wn−2 , l ) are connected by an edge if and only if l = l + 1 and wi = wi for
all i ࣔ l. The edge is labelled with wl (down) on the level l switch and with wl (up) on the level l switch.
There is an edge between the switch (w0 , w1 , . . . , wn − 2 , n − 1) and the processing node p0 , p1 , . . . , pn − 1 if and only if wi = pi
for all i ࢠ {0, 1, . . . , n − 2}. This edge is labelled with pn − 1 (down) on the level n − 1 switch.

This labeling scheme makes the k-ary n-tree a delta network [15,16]: any path starting from a level 0 switch and leading to
a given node p0 , p1 , . . . , pn − 1 traverses the same sequence of edge labels (p0 (down), p1 (down), . . . , pn − 1 (down)). An example of
such labeling is shown in Fig. 1, for a 64-node fat-tree network, which is a 4-ary 3-tree.
Minimal routing between any pair of processing nodes can be accomplished by sending the message to one of the nearest
common ancestor switches and from there to the destination. Hence, each message experiences two routing phases: an ascending
phase, from the processing node to a nearest common ancestor, followed by a descending phase. While the descending phase
is necessarily deterministic, since there is a single path from a nearest common ancestor switch to the destination, there could
be alternative routes to reach a nearest common ancestor. The availability of alternative routes makes it possible to randomly
choose ascending links or even implementing an adaptive algorithm that makes a decision according to the local state of the
switch, avoiding congested links.
3. Power saving in fat-trees
The proposed power saving mechanism is based on dynamically switching links on and off as a function of the required network
traﬃc. For doing so, every switch in the network periodically measures outgoing traﬃc and sets the number of operating outgoing
links depending on traﬃc variations. A subset of network links, which is deﬁned as the Minimal Tree, cannot be switched off
in order to maintain the network connectivity. Although this restriction bounds the maximum power saving level, it avoids
introducing complex modiﬁcations in the routing algorithm.
3.1. Minimal Tree
Given a k-ary n-tree, the Minimal Tree (MT) is the subset of the tree composed of all the processing nodes, a subset of the
communication switches, and the edges between them. A switch (w0 , w1 , . . . , wn − 2 , l) belongs to the MT if one of the following
properties holds:
1. l < n − 1 and wi = 0 ∀i ∈ {l, . . . , n − 2}.
2. l = n − 1 (all the switches in level n − 1 belong to the MT).

62

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

0,0

0,2

0,3

1,0

1,1

1,2

1,3

2,0

2,1

2,2

2,3

3,0

3,1

3,2

3,3

3,2,2

3,3,2

0,0,0

level

0

column
0,1

1

2

0,0,1

0,0,2

1,0,1

0,1,2

0,2,2

0,3,2

2,0,1

1,0,2

1,1,2

1,2,2

1,3,2

2,0,2

3,0,1

2,1,2

2,2,2

2,3,2

3,0,2

3,1,2

3,0,0
3,0,1
3,0,2
3,0,3
3,1,0
3,1,1
3,1,2
3,1,3
3,2,0
3,2,1
3,2,2
3,2,3
3,3,0
3,3,1
3,3,2
3,3,3

2,0,0
2,0,1
2,0,2
2,0,3
2,1,0
2,1,1
2,1,2
2,1,3
2,2,0
2,2,1
2,2,2
2,2,3
2,3,0
2,3,1
2,3,2
2,3,3

1,0,0
1,0,1
1,0,2
1,0,3
1,1,0
1,1,1
1,1,2
1,1,3
1,2,0
1,2,1
1,2,2
1,2,3
1,3,0
1,3,1
1,3,2
1,3,3

0,0,0
0,0,1
0,0,2
0,0,3
0,1,0
0,1,1
0,1,2
0,1,3
0,2,0
0,2,1
0,2,2
0,2,3
0,3,0
0,3,1
0,3,2
0,3,3

01 2 3

Fig. 2. Minimal Tree of a 4-ary 3-tree.
Table 1
Minimum relative power for different fat-trees.
n

k

2
4
8
16

2

3

4

5

6

0.75000
0.62500
0.56250
0.53125

0.58333
0.43750
0.38021
0.35547

0.46875
0.33203
0.28564
0.26666

0.38750
0.26641
0.22856
0.21333

0.32813
0.22217
0.19048
0.17778

The Minimal Tree of a quaternary fat tree of dimension 3 (4-ary 3-tree) is shown in Fig. 2.
The MT is composed of the switches that cannot be turned off, as they provide the minimum paths needed to maintain all the
processing nodes connected.
The number of switches in the MT is:

|MTS | = kn−1 + kn−2 + · · · + k + 1 =

1 − kn
1−k

Within these switches, all the down links, and the up link with index 0(up) also belong to the Minimal Tree. Thus, there are
k + 1 links in the MT per switch in the MT (except the root switch, with only its k down links in the MT). The links that connect
the N = kn processing nodes with the switches also belong to the MT.
The number of unidirectional links in the MT is:

|MTL | = (k + 1)|MTS | − 1 + kn = 2k |MTS |
Considering only the link power consumption, the minimum relative power consumption is given by the ratio between the
number of links in the Minimal Tree and the total number or links in the fat-tree:

pmin =

|MTL |
2kS

=

|MTS |
nkn−1

Table 1 shows the minimum relative power consumption for different combinations of the number of stages n and switch
arity k of a fat-tree. As it can be seen, the mechanism potential for saving power increases with the number of stages and switch
arity.
It must be noticed that, for some traﬃc patterns, some of the links of the Minimal Tree may be inactive. This is the case
of traﬃc patterns communicating with switch local neighbors. For those cases, if more links were turned off, an even lower
network power could be achieved. Therefore, the results shown in Table 1 are not a lower bound of the power but the minimum
one achieved by the power saving mechanism. Remember that the proposed mechanism keeps the links of the switches of the
Minimal Tree turned on to maintain an always connected path among all processing nodes.
3.2. A mechanism for power saving in fat-trees
The proposed power saving mechanism is based on dynamically switching links on and off as a function of the required
network traﬃc. We consider bidirectional links that are composed of a pair of unidirectional channels that can be turned on/off
in a given direction, either ascending or descending. As stated in [3], there is signiﬁcant advantage to having independent control
of each unidirectional channel comprising a network link, since these channels typically see asymmetric use.
In the mechanism description that follows, link direction is based on the network layout shown in Figs. 1 and 2:

M. Alonso et al. / Parallel Computing 48 (2015) 59–80
•

•

63

Ascending links: the utilization of the ascending links in a switch is used to decide whether to turn on/turn off an ascending
link.
Descending links: all the descending links of a switch are turned on/off at the same time. These links are turned off when the
switch cannot receive descending traﬃc, that is, when all the input links (ascending and descending) have been turned off.
They are turned on again when some input link is turned on, because the arriving messages could require them for proceeding
to the destination.

A switch could be moved to a standby state (providing additional power saving) in the case that all the incoming links, both
from the upper and lower levels, were inactive. We do not evaluate the impact of this strategy that could lead to additional power
savings but should deal with switch powering on and off. In this case, the only required functionality is detecting the activation
of an incoming link for waking up the switch to the regular operating mode.
Traﬃc is measured by estimating link utilization. Link utilization is measured at a switch basis by a counter that accounts
the number of transferred ﬂits. This counter is periodically checked and reset. To properly detect bursts of traﬃc, this checking
period should be as low as possible (see Section 4.2 for further details).
The power saving mechanism is implemented by each switch using two thresholds that control the mechanism behavior:
Uoff is the turn off threshold, and Uon is the turn on threshold. A traﬃc level requiring low link utilization in a switch (below Uoff )
causes links to be switched off, whereas high link utilization (above Uon ) indicates additional links should be connected.
On the other hand, when there is congestion in the network, links must be switched on as soon as possible. To avoid waiting
until the next check interval, our mechanism also performs a litmus test to detect congestion. It merely uses the length of the
local packet injection queue at the nodes as a symptom of congestion. If there are some pending packets, the network may be
congested and all the links of the switch are immediately turned on. Notice that other strategies to detect congestion are possible.
Network switches behave in a different manner depending on their position in the network.
3.2.1. Switches belonging to the Minimal Tree
For the root switch, (0, 0, . . . , 0), there is no need for any power saving mechanism because its output links (that are only
descending links) are always active. For the remaining switches in the minimal tree, the utilization of each “up” link is measured
and periodically, the average utilization of all the “up” links in a switch, uup , is calculated. Links that can be turned off are those
that do not belong to the MT.
•

•

When uup < Uoff , one of the “up” links is turned off. The ﬁrst link to be turned off is the k − 1(up) one, then the k − 2(up), and
so on. The link 0(up) cannot be turned off, as it belongs to the MT and provides the minimum connectivity with the upper
level.
When uup > Uon , an inactive “up” link is turned on. The selected link is the one with the lowest index.

3.2.2. Switches not belonging to the Minimal Tree
For the ascending (or up) links (notice that level 0 switches do not have these links), without any exception, exactly the same
utilization-based policy described for MT switches is applied. Additionally, turning on/off actions are anticipated depending on
adjacent switches:
•

•

When a switch detects that an input ascending link (connected to a “down” port in this level) has been turned off (i.e. it
initiates its transition from on to off), it also turns off an “up” link. Speciﬁcally, when it detects that the link attached to port
i(down) (0 ࣘ i < k) is being turned off, it turns off the i(up) link. As it receives less traﬃc from the lower level, it also needs less
bandwidth to the upper level. This policy achieves a balance between the incoming traﬃc from lower levels and the outgoing
traﬃc to upper levels. If all the incoming links from the lower level are turned off, all the outgoing links to the upper level can
be disconnected, as the switch does not receive ascending traﬃc.
When the switch detects that an incoming link from the lower level is being turned on (i.e., it begins its transition from off to
on), the switch immediately initiates the turning on of the corresponding “up” link. Notice that both processes can take place
simultaneously. Moreover, the connection of the “up” link could trigger more reconnections in the upper levels.

Descending (or down) links need a different approach as they do not have the redundancy of the up links. For this reason,
down links of a switch are controlled altogether. In a fat-tree, a message, in the descending routing phase, has a deterministic
path to reach its destination (namely, for a message addressed to a processing node (p0 , p1 , . . . , pn − 1 ) it must use the pl (down)
port in the level l).
Therefore, while a switch has active incoming links from the upper or lower levels, all the down links must be in the on state,
as they provide the necessary connectivity for descending messages. Only when all the input links of a switch are completely
turned off, descending links are powered down.
When a switch detects that an incoming link is being turned on (i.e. it begins its transition from off to on), the switch
immediately initiates the turning on of all its descending links and one ascending link to guarantee paths to all possible
destinations.
3.3. Mechanism control parameters. Aggressiveness and responsiveness
The behavior of the system greatly relies on the pair of thresholds (Uon and Uoff ) used by the power saving mechanism. The
effect of these two parameters can be analyzed in terms of two complementary aspects:

64

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

U on
− Uoff
uMAX

Uon< uMAX

Responsiveness

13
Uoff> 0
1/2 uMAX
3
1

0

10 11

12

6

7

8

4

5

2

9

Uon> 2Uoff
1/2 uMAX 3/4uMAX

Uavg

Aggressiveness
Fig. 3. Map of possible thresholds.

•

•

U

+U

Mechanism aggressiveness: this is controlled by the average value of the thresholds, Uavg = on 2 off . High average thresholds
provide an aggressive policy, since the mechanism keeps links disconnected even with high loads. On the other hand, if Uavg
is low, a conservative policy is applied since power savings are only tried for low loads.
Mechanism responsiveness: the hysteresis band, deﬁned by the difference Uon − Uoff , controls the mechanism responsiveness
against traﬃc variations. Higher hysteresis bands require higher traﬃc variations for the mechanism to be activated, while
lower traﬃc variations do not modify the system status, and vice versa.
On the other hand, a number of limitations apply to the set of possible threshold values.

•
•
•

•

Obviously, Uon > 0 and Uoff > 0.
Uon must be higher than Uoff , Uon > Uoff .
Uon must be lower than the maximum utilization reached by links with the network highest load (uMAX ), Uon < uMAX .
Otherwise, the network is saturated before trying to connect links (if there are disconnected links).
The difference between the thresholds must be high enough to avoid cyclic state transitions (connection/disconnection).

Concerning this latter condition, every time an outgoing link is turned off, for the same network traﬃc level, the load of the
remaining active links increases. If this load exceeds the Uon threshold, a turn-on link sequence may be triggered. This constraint
is conditioned by the fraction of up link bandwidth available for each switch state. We performed an in-depth analysis of this
effect for direct networks [17]. The conclusions we obtained can be easily translated to fat-tree networks. Every time an ascending
link is turned off, the load of the remaining active links increases. The highest increase in load is produced when moving from
two ascending “on” links to one, because the measured load is multiplied by two. For this reason, to avoid cyclic state transitions,
Uon must be higher than or equal to 2Uoff . In the worst case, the increase on average up link utilization when turning off up links
is not high enough to make the links go back to the previous state.
The map of possible thresholds according to the above restrictions is shown in Fig. 3. This diagram is represented as a
function of thresholds difference and average. Any point inside the shaded region provides a valid conﬁguration, with a different
aggressiveness and responsiveness setting.
3.4. Static thresholds
As described, the mechanism uses a pair of ﬁxed or static thresholds that deﬁne the conﬁguration. The use of constant
thresholds greatly simpliﬁes the implementation of the mechanism, since link utilization has to be compared with a unique pair
of values [17].
However, as stated above, when the mechanism is working, the available bandwidth changes with the number of operational
links. As a consequence the utilization of the links that remain operational is modiﬁed by the mechanism. For example, disconnecting 1 link out of 4 available links, multiplies the utilization of the 3 connected links by 4/3. This new utilization is compared
again with the static thresholds, in a new iteration. The behavior of the mechanism is the same as considering a reduction in
the thresholds value of a 3/4 factor and then measuring the utilization in relation to the total bandwidth (not to the available
bandwidth). These new thresholds are referred to as effective thresholds.
The effective thresholds, as a function of the up links number, in a 4-ary n-tree are shown in Table 2. The factor column shows
the available outgoing bandwidth fraction used to calculate the effective thresholds. Considering the results on the table, 34 Uon
can be considered as the lowest utilization for all the links to be in the on state, whereas 24 Uoff is the highest utilization that
guarantees the maximum power saving (3 switched off links out of 4 possible links) in a particular switch.

Number of active ascending links

Number of active ascending links

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

4
3
2
1

Uoff

4
3
2
1

Uon Traffic

Uoff

(a) Static thresholds.

65

Uon Traffic

(b) Dynamic thresholds.

Fig. 4. Switch state as a function of the outgoing traﬃc.

Table 2
Static and effective thresholds according to the available links for a
4-ary switch.
Up links

4
3
2
1

Static thresholds
On

Off

Uon
Uon
Uon
Uon

Uoff
Uoff
Uoff
0

Factor

1
3/4
2/4
1/4

Effective thresholds
On

Off

Uon
3/4Uon
2/4Uon
1/4Uon

Uoff
3/4Uoff
2/4Uoff
0

Fig. 4(a) shows the switch state (given by the number of active ascending links) as a function of the load traversing the switch
in ascending direction for a 4-ary n-tree. The state with all the links disconnected is not shown, because the last ascending link is
disconnected (except if belongs to the MT) when all the input links in ascending direction are already disconnected (zero load).
To understand the ﬁgure, follow the arrows indicated in the curves: from right to left for the descending load and from left
to right for the ascending load. For example, in Fig. 4(a), for a load producing links utilization higher than Uon , the 4 ascending
links are connected. If the traﬃc goes below Uoff the number of connected ascending links is being successively 4, 3, 2 and 1
following the stepped curve at the top. Then, if the traﬃc increases again to the original value, a reconnection of the links follows
the lower curve. The horizontal distance (in utilization) between the two curves shows the mechanism instantaneous sensitivity
for a given state. The available area between the shown curves and a horizontal line located at 4 connected links indicates the
network operating area where power is reduced.
As it can be seen, the reduction in available bandwidth when reducing the active outgoing links generates an effect that
can be viewed as a reduction in the hysteresis band (increase on responsiveness) of the mechanism. This is a positive effect,
since the network is more sensitive to congestion when the fraction of active links is reduced. In that situation, having higher
responsiveness increases the mechanism agility to react against small changes in traﬃc, providing additional active links if
needed. Otherwise, low responsiveness could lead to network congestion during limited periods of time, with a signiﬁcant
increase in latency.
An important limitation of the static thresholds approach is given by the condition that requires Uon to be higher than 2Uoff .
This makes the threshold average to be low for situations with several active links, and hence making the mechanism less
aggressive, which means reducing the margin for power saving. Moreover, this condition precludes the use of a mechanism that
is both very aggressive and very responsive (see Fig. 3).

3.5. Dynamic thresholds
We have devised an alternative version of the power saving mechanism which is based on dynamic thresholds. The objective
is increasing the working area where the power consumption is reduced. Graphically, it can be visualized as achieving a narrow
and constant hysteresis band for the effective thresholds, trying to get the curve followed when links are disconnected closer to
the curve followed when links are reconnected. This would increase the margin for reducing power consumption.
The strategy consists of trying to divide the network total utilization range in as many intervals as denoted by the network
arity. The intuition behind this approach is that every switch distributes the ascending traﬃc among k links when the network is
fully active (remember that the routing algorithm is adaptive in the ascending phase). If the traﬃc decreases 1/k of the nominal
traﬃc requiring the full switch bandwidth, it seems reasonable to reduce the available bandwidth by exactly the same amount,
thus turning off one link [18].

66

M. Alonso et al. / Parallel Computing 48 (2015) 59–80
Table 3
Dynamic and effective thresholds according to the available links for a
4-ary switch.
Up links

4
3
2
1

Dynamic thresholds
On

Off

Uon
Uon
Uon
Uon

3/4Uon
2/4Uon
1/4Uon
0

Factor

1
3/4
2/4
1/4

Effective thresholds
On

Off

Uon
3/4Uon
2/4Uon
1/4Uon

3/4Uon
6/16Uon
2/16Uon
0

The dynamic thresholds implementation is based on a ﬁxed “on” threshold, Uon , and a dynamic version of the “off” threshold,
Uoff , that depends on the number of active outgoing links according to the following expression:

Uoffi =

Uon (i − 1)
k

i being the number of active outgoing links in the ascending direction. Every switch in the network updates its off threshold
according to the above expression. If we consider a 4-ary n-tree, the set of dynamic thresholds and effective thresholds is shown
in Table 3, while the switch state transition diagram is shown in Fig. 4(b).
The dynamic implementation of thresholds should provide signiﬁcant improvements in terms of power saving, since the
fraction of switch utilization where some links are turned off increases with respect to the static version. This will be validated
in Section 4. The overlap among the transitions between 4 and 3 on links is not a problem (multiple alternative transitions due to
traﬃc oscillations) since the mechanism operation is based on the average switch utilization during ﬁxed length periods which
has the effect of low-pass ﬁltering quick traﬃc changes.
3.6. Implementation issues
As described above, the implementation of the proposed power saving mechanism is quite simple. We summarize in this
section what is required to implement the proposed power saving mechanism in a real environment.
Network switches must have the proper control circuitry to turn links on and off and they must be able to estimate link
utilization with the monitoring mechanism (basically a counter plus two comparators). In addition, the network-level power
saving mechanism, as described in this section, must be implemented at each switch. Routing algorithm is the same, regardless
of the power saving level.
In addition, the mechanism should be properly tuned, according to the actual topology size, expected network load and the
desired aggressiveness/responsiveness features of the mechanism. The subnet manager, at network initialization time or even
later, could do this tuning, at any time, by issuing some kind of reconﬁguration command to switches. “Neutral” thresholds
should be ﬁrst used (i.e., centered in the map, see Fig. 3), and, then, some tests can be done to empirically ﬁne tune them.
4. Performance evaluation
The objective of this section is twofold. On the one hand, we want to analyze in depth the behavior of the mechanism. On the
other hand, we want to analyze the impact of the mechanism on overall power and energy of the system. For these purposes,
three kind of experiments have been performed:
1. First, in Section 4.4, we analyze the behavior of the mechanism under constant traﬃc. Different injection rates (from low
load to saturation) of messages are tested. Different static and dynamic threshold values of the power saving mechanism are
evaluated and compared.
2. Next, in Section 4.5, the dynamic behavior of the mechanism is analyzed by using a network traﬃc that varies along time.
Both uniform and self-similar traﬃc with several on/off sources (with different injection rates) were considered. Different
static and dynamic threshold values of the power saving mechanism are evaluated and compared.
3. Finally, in Section 4.6, the impact of the mechanism on overall power and energy consumption is evaluated by using a
workload that resembles the one generated by HPC applications. We present results for the power saving mechanism with
static and dynamic thresholds.
4.1. Network model
Our evaluation tool is an in-house event-driven network simulator. The network consists of two types of nodes, processor
and switch nodes. Switches contain a routing control unit, an input and output-queued crossbar and as many physical links as
indicated by the arity of the network. Physical channels are split into three virtual channels. We assume a pipelined 4-stage
routing control unit and that switch and link delay are one clock cycle each. To enforce power saving, as buffers are considered
as one of the most power hungry components of the interconnection network [19], wormhole with small buffers (two ﬂits

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

67

per queue) is assumed as switching technique. A deadlock free minimal adaptive routing algorithm [15] is used. To route a
packet from a source to a destination node, the packet is ﬁrst routed to one of the closest common ancestors, and, from there,
to the destination. Thus, routing consists of two stages. The ﬁrst one is adaptive, routing the packet upstream to one of the
nearest common ancestor. In particular, we try to distribute network traﬃc by selecting the link with the highest number of
free virtual channels. Notice that this strategy improves network performance but it does not beneﬁt power saving, since traﬃc
is more evenly distributed across the network links reducing the opportunities to powering them down. The second phase is
deterministic (there is only one path) and sends the packet in the descending direction to its destination. Each switch also
includes the implementation of the power saving mechanism proposed in this paper.
Several network sizes were analyzed, from 256 to 4096 nodes, with different switch radix (k = 4, k = 8 and k = 16) and
number of stages (n = 2, n = 3 and n = 4).
4.2. Parameters of the power saving mechanism
The proposed mechanism works on a per link basis by measuring its utilization. As stated above, link utilization can be easily
estimated by using a counter that is increased every time a ﬂit is transferred across a link. As the required hardware is quite is
simple, we assume that the power consumed by the monitoring mechanism is negligible.
A link cannot be instantaneously turned on, but it requires a time Ton . Turning off a link also needs some time Toff to decrease
the circuit voltage level to zero. When a link is turned off, we assume that it becomes immediately unavailable but it continues
consuming power until Toff cycles have elapsed. Similarly, when a link is turned on, the new link is available to messages after Ton
cycles, but power consumption increases at once. The fact that Ton and Toff are not zero implies that the power saving mechanism
cannot respond to changes in traﬃc instantaneously. The lower the values of these parameters, the higher the ability of the
mechanism to detect rapid traﬃc changes. Based on the values reported by Kim et al., we have used Ton = Toff = 1000 clock
cycles [6,20]. On the other hand, the state of the network is periodically checked to decide if it is necessary to turn on or off any
link. The lower the check interval, the higher the responsiveness of the power-saving mechanism to changes in traﬃc. We use a
period greater than Ton and Toff in order to allow network stabilization after the changes. Speciﬁcally, link utilization is checked
twice the Ton and Toff , every 2000 clock cycles. It must be noticed that the hysteresis in the thresholds of the proposed power
saving mechanism also helps to avoid stability problems.
4.3. Traﬃc model
The traﬃc pattern is deﬁned according to the following parameters: the spatial distribution of the destinations, the average
injection rate of messages, the temporal distribution and the length of messages.
Experiments have been conducted with the aim of considering both loads resembling real applications and also loads that
are unfavorable to the mechanism proposed in this work. The worst case is a load without temporal or spatial variations such as
uniform random traﬃc (uniform spatial and temporal distribution) [6]. In this case, traﬃc ﬂows are uniformly distributed in the
network over time. Thus, it is virtually impossible to detect variations in network utilization to properly activate the mechanism
of power consumption reduction in order to maximize savings with minimal negative impact on performance. The opposite
would be a irregular traﬃc pattern that does not generate traﬃc in some network areas, allowing to permanently disconnect
them.
We use uniform random traﬃc in Sections 4.4 and 4.5. In the latter section, we complete the evaluation by using a self-similar
distribution of message generation, trying to consider the self-similar nature of many applications.
In Section 4.6, an evaluation using a closed load model is used. The simulation is set to exchange a ﬁxed number of messages.
However, there are dependencies between them, so that when a node receives a message, it responds with another one. It is also
possible to control the workload by setting the number of active nodes in the network.
Message size was set to 16-ﬂit except in those experiments where analyzing the effect of message length, where 256-ﬂit
messages were used.
4.4. Static behavior of the mechanism
In this section, we analyze the performance of the network with the power saving mechanism in operation under constant
traﬃc conditions. A 256-node (4-ary 4-tree) network is evaluated. The presented results have been obtained for conﬁgurations
where all links are initially connected and 500,000 messages are delivered at a constant average injection rate. Injection rate is
modiﬁed for each experiment to explore situations from low load to saturation.
We have explored different conﬁgurations compatible with the map of possible thresholds deﬁned in Fig. 3. Selected points
are highlighted in the ﬁgure and are deﬁned to illustrate various responsiveness and aggressiveness settings.
As shown in the ﬁgure, the distribution of points, and therefore the speciﬁc values of the thresholds depend on the parameter
uMAX or maximum network utilization. According to the results presented in Fig. 6(a), the value of the maximum throughput
for the evaluated topology is approximately 0.63 ﬂits/cycle/node. Assuming a link bandwidth of 1 ﬂits/cycle/node, this value is the
uMAX value to use. From this result, and according to the map of possible thresholds of Fig. 3, the test thresholds are calculated.
Fig. 5 shows the corresponding values for the connection threshold (Uon ) at the top, and the disconnection threshold (Uoff ) at the

68

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

120

Nominal
0.02625−0.44625
0.10500−0.52500
0.18375−0.60375

110
100

1
Relative Power Consumption

Average Message Latency from Generation (cycles)

Fig. 5. Test thresholds with uMAX = 0.63. 4-ary 4-tree.

90
80
70
60
50

0.9
0.8
0.7
0.6
0.5
0.4
0.02625−0.44625
0.10500−0.52500
0.18375−0.60375

40
0.3
30
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0

0.1

0.2

Traffic (flits/cycle/node)

Nominal
0.07875−0.39375
0.15750−0.47250
0.23625−0.55125
0.31500−0.63000

110
100

80
70
60
50

0.7
0.6
0.5
0.4

0.07875−0.39375
0.15750−0.47250
0.23625−0.55125
0.31500−0.63000

30
0.3

0.4

0.5

0.6

0.7

0

0.1

0.2

Traffic (flits/cycle/node)

90
80
70
60
50

0.7
0.6
0.5
0.4
0.05250−0.26250
0.13125−0.34125
0.21000−0.42000

0.3
0.2

0.7

0.8

30
0.1

0.6

0.9

40
0

0.5

1
Relative Power Consumption

Average Message Latency from Generation (cycles)

100

0.4

(d) Power, hysteresis of 0.315.

Nominal
0.05250−0.26250
0.13125−0.34125
0.21000−0.42000

110

0.3

Traffic (flits/cycle/node)

(c) Latency, hysteresis of 0.315.
120

0.7

0.8

0.3
0.2

0.6

0.9

40
0.1

0.5

1

90

0

0.4

(b) Power, hysteresis of 0.42.

Relative Power Consumption

Average Message Latency from Generation (cycles)

(a) Latency, hysteresis of 0.42.
120

0.3

Traffic (flits/cycle/node)

0.3

0.4

0.5

0.6

0.7

0

0.1

0.2

Traffic (flits/cycle/node)

(e) Latency, hysteresis of 0.21.

0.3

0.4

0.5

Traffic (flits/cycle/node)

(f) Power, hysteresis of 0.21.

Fig. 6. Results with static thresholds. 4-ary 4-tree.

0.6

0.7

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

1
Relative Latency x Relative Power

Relative Latency x Relative Power

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.05

0.15

0.2

0.25

0.3

0.35

0.4

0.8
0.7
0.6
0.5
0.4
0.02625−0.44625
0.10500−0.52500
0.18375−0.60375

0.3

0.05250−0.57750
0.1

0.9

0.45

0.5

0.05

0.1

0.15

Traffic (flits/cicle/node)

0.25

0.3

0.35

0.4

0.45

0.5

(b) L rel × P rel , hysteresis of 0.42.
1
Relative Latency x Relative Power

1
Relative Latency x Relative Power

0.2

Traffic (flits/cicle/node)

(a) L rel × P rel , hysteresis of 0.525.

0.9
0.8
0.7
0.6
0.5
0.4

0.07875−0.39375
0.15750−0.47250
0.23625−0.55125
0.31500−0.63000

0.3
0.05

69

0.1

0.15

0.2

0.25

0.3

0.35

0.4

Traffic (flits/cicle/node)

(c) L rel × P rel , hysteresis of 0.315.

0.9
0.8
0.7
0.6
0.5
0.4
0.05250−0.26250
0.13125−0.34125
0.21000−0.42000

0.3
0.45

0.5

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Traffic (flits/cicle/node)

(d) L rel × P rel , hysteresis of 0.21.

Fig. 7. Lrel × Prel with static thresholds. 4-ary 4-tree.

bottom. These values are organized according to the hysteresis or responsiveness, (Uoff − Uon ), and aggressiveness, Uavg . Only
the 13 pairs of thresholds that meet the constraints of Fig. 3 are shown.
Fig. 6 shows the average latency from generation time of messages and the relative power consumed by links vs. traﬃc, for
different hysteresis. The curves are identiﬁed by labels with the format “Uoff − Uon ”. The curve labeled “Nominal”, represents
the latency of messages when the power saving mechanism does not operate. Each graph shows curves for conﬁgurations with
the same responsiveness. That is, results shown in the same graph correspond to a row of the setting points shown in Fig. 3,
which, in turn, correspond to a row of Fig. 5. Additionally, the labels are sorted from top to bottom in order of increasing aggressiveness. As we can see, in all cases, power consumption signiﬁcantly decreases for low load, with a very moderate increase in
latency.
Notice that the threshold value has a signiﬁcant impact in power consumption. More aggressive thresholds achieve better
power savings. The price to pay is an increase in latency for low loads due to the reduced number of available links in the network
as they are disconnected. The most signiﬁcant impact in latency (and power saving) occurs for the most aggressive conﬁguration
(Uoff = 0.315, Uon = 0.630, Fig. 6(c) and (d)), which corresponds with setting 9 in the map of possible thresholds (Fig. 3). In this
case, power is saved for traﬃc below 0.34 ﬂits/cycle/node. When traﬃc increases, progressive reconnection of links makes latency
(and power) tend to its nominal value.
In order to verify whether the power saving compensate for the latency increase, we deﬁne a ﬁgure of merit as the product
Lrel × Prel . Lrel is the relative latency with respect to the latency obtained when no power saving mechanism is in use, and Prel is
the relative power consumption.
Lrel × Prel product results (Fig. 7) show that in all tested conﬁgurations the mechanism obtains good results. Notice that, as
expected, the most aggressive conﬁgurations provide more opportunities for saving power, as shown by the area in the graph
where Lrel × Prel is lower than one.
Dynamic thresholds. In Fig. 8, we show the results obtained with the same conﬁguration but using the dynamic thresholds
(described in Section 3.5). The experiments have been performed using as initial connection thresholds (Uon ) the ones listed in
Fig. 5. The curves are identiﬁed by labels corresponding to the threshold value. In this case, it does not make sense to refer to Uoff ,
since the thresholds for each switch are adjusted according to their status. We present the results to allow an easy comparison
with static thresholds for the same Uon values.
The analysis of the Lrel × Prel product indicates that there is a slight penalty when using the most aggressive conﬁgurations with dynamic thresholds. In particular when the threshold Uon is greater than 0.47, the Lrel × Prel product is slightly
greater than one (i.e. the increase in the latency of messages exceeds the reduction of power). For lower values of this

70

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

Relative Latency x Relative Power

1.1

1

0.9

0.8

0.13125
0.26250
0.34125
0.42000
0.47250

0.7

0.6
0.05

0.1

0.15

0.2
0.25
0.3
0.35
Traffic (flits/cicle/node)

0.4

0.45

0.5

Fig. 8. Lrel × Prel for dynamic thresholds. 4-ary 4-tree.

threshold, favorable results are obtained in all cases and there is room to deﬁne power saving strategies with a variable degree of
aggressiveness.
4.4.1. Effect of message length
With the aim of studying the inﬂuence of the length of the messages on the behavior of the mechanism, we also evaluate
256-ﬂit messages.
In all cases, we get similar results to those obtained for short messages but with better saving rates for low load with a very
moderate increase in the latency of messages. In particular, for 16-ﬂit messages, power is saved when the load is lower than 33%
of the saturation load, whereas this is 55% for 256-ﬂit messages. Long messages reduce the overhead due to the headers and for
the same traﬃc rate, the number of routings is reduced, so there are less opportunities to trigger up link connections, thereby
having a greater potential for power savings.
The results for the Lrel × Prel metric (Fig. 9) conﬁrm that the increased penalty observed in latency is widely compensated
by the reduction in power consumption, as happened with 16-ﬂit messages. Only one conﬁguration, the most aggressive one,
leads to Lrel × Prel values greater than one (the maximum value is 1.04). This conﬁguration uses a Uon value of 0.630, which is
the highest possible utilization of the network and Uoff = 0.315, so Uon = 2 × Uoff and there may be connection–disconnection
cycles. That is, this set of thresholds is at the limit of what we have deﬁned as acceptable. Any other tested conﬁguration provides
favorable results for this metric.
Dynamic thresholds. As with short messages, a greater potential is found for power saving than with the static threshold-based
mechanism. Some of the conﬁgurations can achieve power savings for traﬃc levels up to 0.50 ﬂits/cycle/node.
Fig. 10 shows in a single graph results for the Lrel × Prel metric on some selected conﬁgurations, illustrating the behavior
of the mechanism for increasing values of Uon . The conﬁguration that provides favorable results for the highest traﬃc margin
is Uon = 0.42, but with results very close to Uon = 0.34125 (in these cases there is a slight penalty for traﬃc rates between
0.3 ﬂits/cycle/node and 0.35 ﬂits/cycle/node). More aggressive policies do not provide improvements of Lrel × Prel , instead generate
penalties for traﬃc greater than 0.30 ﬂits/cycle/node. According to the results, Uon threshold should be lower than 0.35, similar to
the threshold recommended for 16-ﬂit messages.
4.5. Dynamic behavior of the mechanism
To analyze the dynamic behavior of the proposed mechanism, we ran simulations using time variable traﬃc rate (or intensity).
The simulation starts with a traﬃc level equivalent to 2% of the maximum load supported by the network. Then, the load grows
slowly and steadily up to a value of 100% of the saturation load (uMAX ). The load remains constant for 60,000 cycles. Then,
input traﬃc decreases again at a constant rate until achieving its initial value. This experiment illustrates the behavior of the
system under loads with increasing and decreasing slopes. Initially, all extra links are disconnected, i.e. only the minimal tree is
connected. Again, results are shown for a 4-ary 4-tree.
Fig. 11 shows the results, for 16-ﬂit messages and a conﬁguration of thresholds approximately at a midpoint in the threshold
map (setting 7 in Fig. 3). In particular, Uoff = 0.1575 and Uon = 0.4725. In all subﬁgures, the X axis indicates the elapsed time in
cycles. Fig. 11(a) plots the delivered traﬃc, with the shape of a dual ramp with a ﬂat top, as described above. Fig. 11(b) represents
the average latency of the messages, and Fig. 11(c) shows the results of relative power consumption. Finally, Fig. 11(d) combine,

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

1
Relative Latency x Relative Power

Relative Latency x Relative Power

1
0.9
0.8
0.7
0.6
0.5
0.4
0.02625−0.13125
0.10500−0.21000

0.3
0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.9
0.8
0.7
0.6
0.5
0.4
0.02625−0.44625
0.10500−0.52500
0.18375−0.60375

0.3
0.45

0.5

0.05

0.1

0.15

Traffic (flits/cicle/node)

0.25

0.3

0.35

0.4

0.45

0.5

0.45

0.5

(b) L rel × P rel , hysteresis 0.42.
1
Relative Latency x Relative Power

1
Relative Latency x Relative Power

0.2

Traffic (flits/cicle/node)

(a) L rel × P rel , hysteresis 0.105.

0.9
0.8
0.7
0.6
0.5
0.4

0.07875−0.39375
0.15750−0.47250
0.23625−0.55125
0.31500−0.63000

0.3
0.05

71

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.9
0.8
0.7
0.6
0.5
0.4
0.05250−0.26250
0.13125−0.34125
0.21000−0.42000

0.3
0.45

0.5

0.05

0.1

0.15

Traffic (flits/cicle/node)

(c) L rel × P rel , hysteresis 0.315.

0.2

0.25

0.3

0.35

0.4

Traffic (flits/cicle/node)

(d) L rel × P rel , hysteresis 0.21.

Fig. 9. Static thresholds and 256-ﬂit messages. 4-ary 4-tree.

Relative Latency x Relative Power

1.1

1

0.9

0.8

0.13125
0.26250
0.34125
0.42000
0.47250

0.7

0.6
0.05

0.1

0.15

0.2
0.25
0.3
0.35
Traffic (flits/cicle/node)

0.4

0.45

0.5

Fig. 10. Dynamic thresholds and 256-ﬂit messages. 4-ary 4-tree.

in the same graph, latency and relative power consumption. The behavior of the power saving mechanism is clearly observed
in the latter. In the ﬁrst phase of the simulation with very low load, power consumption stands at 0.33. For this conﬁguration,
with very low traﬃc rate, the minimal tree plus a few additional links are connected, and latency remains constant between 42
and 44 cycles. In the next phase, as injected traﬃc increases, the latency of messages grows up. These latency increase are more
noticeable for low loads because the network is closer to the minimal tree which is much more prone to congestion. In response,
the power saving mechanism activates the link connection process. As new links are connected, latency decreases, even when
the load continues growing, due to the availability of extra bandwidth. Once all links have been connected (this happens around
125, 000 cycles), the additional increase in traﬃc causes an increase in latency up to a value of approximately 140 cycles, where
the injected traﬃc reaches the maximum value (in this case 0.63 ﬂits/cycle/node).

M. Alonso et al. / Parallel Computing 48 (2015) 59–80
Thresholds 0.15750−0.47250
0.7

0.5
0.4
0.3
0.2
0.1
0
0

100000

200000

300000

400000

500000

Thresholds 0.15750−0.47250
140
120
100
80
60
40
0

100000

Time (cycles)

Relative Power Consumption

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
300000

Time (cycles)

(c) Power consumption.

400000

500000

Average Message Latency from Generation (cycles)

Thresholds 0.15750−0.47250

200000

400000

500000

(b) Latency.

1.1

100000

300000

Time (cycles)

(a) Delivered traffic.

0

200000

Thresholds 0.15750−0.47250
1.1
140

1

120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

Traffic (flits/cycle/node)

0.6

Average Message Latency from Generation (cycles)

72

0.4

40
0

100000

200000

300000

400000

0.3
500000

Time (cycles)

(d) Latency and power.

Fig. 11. Dynamic evaluation for a fat-tree with static thresholds Uoff = 0.1575 and Uon = 0.4725. 4-ary 4-tree.

The reduction in injected traﬃc is not perceived instantly in the delivered traﬃc because the network is close to saturation.
Therefore, it takes some time until delivered traﬃc reduces. This is indicated by the vertical line in Fig. 11(a) at cycle 340, 000
approx., just before the descending ramp. The ﬁrst consequence of reducing traﬃc is latency reduction. When the reduction of
traﬃc leads to a link utilization low enough (approximately at 400, 000 cycle time), the power saving mechanism starts switching
off links, causing a slight latency increase before stabilizing around 43 cycles.
Fig. 12 shows the obtained results with static thresholds for the most signiﬁcant conﬁgurations in the map of possible
thresholds. Notice that the behavior of the mechanism is similar to the example discussed in detail in Fig. 11.
A greater impact on latency curves is observed as the power saving mechanism is more aggressive. The most extreme case
is shown in Fig. 12(d). When the network is close to the minimal tree and aggressive thresholds are employed, the mechanism
waits until very high link utilization values are reached before connecting additional links and, since the connection is not
instantaneous, a transient period occur in which latency has some peak value which is absorbed shortly later.
For the experiments shown, we also observe that power consumption at the end of the experiment is slightly higher than the
initial value for the same traﬃc. This is due to the hysteresis band of thresholds and departure status of the network. Initially,
the network has all its possible links disconnected and the injected traﬃc is not enough to reconnect a signiﬁcant number of
links. Power consumption is therefore the minimum one. However, at the end, the network has all its links connected and the
reduction of traﬃc (and link utilization) is what causes the disconnection. The ﬁnal traﬃc injection rate used in the experiments
(2%) is not low enough to trigger all link disconnections for some threshold values. If we rerun the experiment with lower ﬁnal
injection rates, all possible links become switched off and the ﬁnal network power consumption is the same as the initial one.
It is also interesting to notice the horizontal lines that the power consumption curves present in the areas corresponding to
traﬃc transitions. These zones of constant power consumption in the form of a plateau, are best seen in increasing traﬃc. In
general four equispaced levels can be distinguished that correspond to four network states. i) the switches connected to processors
have only one connected uplink (minimal tree), ii) switches with two connected uplinks, iii and iv) switches with three or four
connected uplinks, respectively. As connections and disconnections are transmitted to neighboring switches, the status of the
switches directly connected to the processor nodes determines and deﬁnes the connection status (and consumption) of the
global network. This makes the network to show, on average, this four stable states with equally distributed power consumption
levels between the minimum of 33.2% and the maximum of 100%.
Dynamic thresholds. We repeated the same experiment using the dynamic thresholds version of the mechanism. Fig. 13 shows
selected results. In this case, each conﬁguration is characterized by only one threshold (Uon ), indicated in the title of the graphs.
The most signiﬁcant difference with respect to the static threshold version is the power consumption curve for decreasing loads.
This curve shows a stepped down behavior similar to what happens in the uphill. The power-down ramp depends on the off

1.1
140
120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

1

0.4

40
100000

200000

300000

400000

Thresholds 0.13125−0.34125
1.1
140

1

120

0.9
0.8

100

0.7
80

0.6

60

0.5
0.4

40
0

100000

Time (cycles)

140
120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

1

0.4

40
300000

400000

0.3
500000

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

1.1

200000

400000

0.3
500000

(b) Latency and power, setting 4.

Thresholds 0.10500−0.52500

100000

300000

Time (cycles)

(a) Latency and power, setting 3.

0

200000

Thresholds 0.18375−0.60375
1.1
140

1

120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

0

0.3
500000

73

Relative Power Consumption

Thresholds 0.05250−0.26250

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

0.4

40
0

100000

Time (cycles)

200000

300000

400000

0.3
500000

Time (cycles)

(c) Latency and power, setting 11.

(d) Latency and power, setting 12.

1.1
140
120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

1

0.4

40
100000

200000

300000

400000

1.1
140

1

120

0.9
0.8

100

0.7
80

0.6

60

0.5
0.4

40
0

100000

Time (cycles)

140
120

0.9
0.8

100

0.7
80

0.6

60

0.5
0.4

40
300000

400000

Time (cycles)

(c) Latency and power, setting 11.

0.3
500000

Relative Power Consumption

1

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

1.1

200000

400000

0.3
500000

(b) Latency and power, setting 4.

Uon = 0.52500

100000

300000

Time (cycles)

(a) Latency and power, setting 3.

0

200000

Uon = 0.60375
1.1
140

1

120

0.9
0.8

100

0.7
80

0.6

60

0.5

Relative Power Consumption

0

0.3
500000

Uon = 0.34125

Relative Power Consumption

Uon = 0.26250

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

Fig. 12. Dynamic evaluation for different settings on the map of possible thresholds. Static thresholds. 4-ary 4-tree.

0.4

40
0

100000

200000

300000

400000

0.3
500000

Time (cycles)

(d) Latency and power, setting 12.

Fig. 13. Dynamic evaluation for different setting points on the map of possible thresholds. Dynamic thresholds. 4-ary 4-tree.

1.1
140

0.9

120

0.8
100

0.7
0.6

80

0.5
60

0.4
Latency
Power
0

0.1

0.2

0.3

0.4

0.5

0.3
0.2
0.6

1.1
140

1
0.9

120

0.8
100

0.7
0.6

80

0.5
60

0.4
Latency
Power

40
0

0.1

Traffic (flits/cycle/node)

140

0.9

120

0.8
100

0.7
0.6

80

0.5
60

0.4
Latency
Power
0.2

0.3

0.4

Traffic (flits/cycle/node)

(c) Dynamic. Upward traffic.

0.5

0.3
0.2
0.6

Relative Power Consumption

1

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

1.1

0.1

0.4

0.5

0.2
0.6

(b) Static. Downward traffic.

Thresholds 0.31500−0.63000 − Upward Traffic

0

0.3

Traffic (flits/cycle/node)

(a) Static. Upward traffic.

40

0.2

0.3

Uon = 0.63000 − Downward Traffic
1.1
140

1
0.9

120

0.8
100

0.7
0.6

80

0.5
60

0.4
Latency
Power

40
0

0.1

0.2

0.3

0.4

0.5

Relative Power Consumption

40

Relative Power Consumption

1

Thresholds 0.31500−0.63000 − Downward Traffic

Relative Power Consumption

Thresholds 0.31500−0.63000 − Upward Traffic

Average Message Latency from Generation (cycles)

M. Alonso et al. / Parallel Computing 48 (2015) 59–80
Average Message Latency from Generation (cycles)

74

0.3
0.2
0.6

Traffic (flits/cycle/node)

(d) Dyn. Downward traffic.

Fig. 14. Latency and power versus load with upward and downward traﬃc for setting 9. 4-ary 4-tree.

threshold and this is modiﬁed by the dynamic version of the mechanism (Section 3.5). As a result, there is a slight impact on the
latency of messages as there are some peaks caused by the disconnection of links as traﬃc decreases.
The evolution of the latency of messages along with the power consumed when traﬃc increases or decreases allows to
complete the behavioral analysis of the mechanism. Fig. 14 represents the average latency and power versus delivered traﬃc,
both for upward and downward loads (from 2% to 100% and vice versa) for the two variants of the mechanism. Curves for
decreasing traﬃc must be read from right to left. For brevity, we selected only one of the tested conﬁgurations, in particular
setting 9, which is the most aggressive setting (similar results have been obtained for the remaining conﬁgurations). The
maximum latency in the graph is ﬁxed to provide a good resolution over the full range of traﬃc.
Fig. 14 shows that the curves for increasing traﬃc are very similar for both types of thresholds. The increase of traﬃc causes
higher utilization of links, which is shown by an increased latency, which, in turn, triggers connection of links when the Uon
threshold is exceeded. For low loads, where most of the links are disconnected, the network is very sensitive to sudden traﬃc
increases (the fat-tree drops to a minimal tree) and that is the cause of the ﬁrst latency peak. This peak is absorbed as soon as the
required links are actually connected. As already pointed out, there are some traﬃc intervals for which the network remains in a
stable power consumption state. For additional increases in traﬃc, the behavior repeats with softer latency peaks as the network
approaches a fat-tree and is better able to absorb traﬃc increases.
The power saving improvement by the dynamic thresholds is shown in the downward traﬃc curves. The interval of traﬃc
levels for which power saving is obtained increases with dynamic thresholds (Fig. 14(b) versus Fig. 14(d)). Power is saved from
a value of 0.42 ﬂits/cycle/node with dynamic thresholds versus 0.30 ﬂits/cycle/node with static thresholds. This indicates that the
dynamic implementation improves the performance of the mechanism, while the impact in latency is limited only to a slight
increase between 0.42 and 0.30 ﬂits/cycle/node.
4.5.1. Evaluation with self-similar traﬃc
We complete the evaluation by analyzing the behavior of the mechanism with self-similar traﬃc, which is more representative
of workloads that can be found in cluster machines. The self-similar load is generated by the aggregation of ON/OFF sources.
ON/OFF sources concatenate traﬃc injection periods with downtime periods, both with Pareto distribution. In our simulator,
each node acts as an ON/OFF source. We have checked the self-similarity property of generated traﬃc through the Selﬁs tool [21],
obtaining in all cases values for the Hurst parameter greater than 0.7.
Experiments were conducted using similar traﬃc ramps as those used with uniform traﬃc. The simulations begin with 2%
of the saturation load (uMAX ). After a small period of time, the load increases during 120,000 cycles until 85% of the saturation
load. We use a load lower than 100% because the higher variability nature of self-similar traﬃc causes short load peaks that

0.6

75

65

0.4

60
55

0.3

50
0.2

45
40

Traffic (flits/cycle/node)

0.5

70

0.1

35
30
100000

200000

300000

400000

Thresholds 0.10500−0.21000
80

1.1

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40
0.4

35
30
0

100000

Time (cycles)

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40
35

0.4

30

0.3
500000

300000

400000

Time (cycles)

(c) Latency and power, setting 9.

Relative Power Consumption

1.1

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

Thresholds 0.31500−0.63000

200000

400000

0.3
500000

(b) Latency and power, setting 2.

80

100000

300000

Time (cycles)

(a) Nominal traffic and latency.

0

200000

Thresholds 0.18375−0.60375
80

1.1

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40

Relative Power Consumption

0

0
500000

75

Relative Power Consumption

Nominal Performance
80

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

0.4

35
30
0

100000

200000

300000

400000

0.3
500000

Time (cycles)

(d) Latency and power, setting 12.

Fig. 15. Dynamic evaluation with self-similar traﬃc, static thresholds. 4-ary 4-tree.

can momentarily saturate the network. Traﬃc remains high for 60, 000 cycles. Then, injected traﬃc smoothly decreases until it
reaches the initial value. Initially, all possible network links are disconnected (i.e. relative power consumption is the minimal
tree one, 33.2% for the evaluated topology).
The results are shown in Figs. 15 and 16 for static and dynamic thresholds, respectively. Results are presented for the
most aggressive conﬁgurations of the map of potential thresholds. In both cases, subﬁgure (a) shows the nominal network latency (without the power saving mechanism) together with the accepted traﬃc (which is the same for all tested
conﬁgurations).
Comparing the results with those obtained for uniform traﬃc, the most signiﬁcant difference is the greater variability of
latency around its mean value. This is due to the traﬃc self-similarity, characterized by bursts of short duration. This is also
responsible for the shape of the power curves, which do not present the same aspect as with uniform traﬃc. With uniform traﬃc
four power consumption states can be clearly differentiated; with self-similar traﬃc there are not stable states but a gradual
transition between the minimum and maximum levels of power.
The differences for static and dynamic thresholds conﬁrm what we observe with uniform traﬃc. The behavior of the network
with upward load, that causes link connections, is very similar for both modalities. In contrast, with downward traﬃc, the
mechanism with dynamic thresholds tend to disconnect the links earlier (for higher loads) with a signiﬁcant impact on the
power curves. For example, in Fig. 16(c) (which corresponds with the most aggressive conﬁguration with dynamic thresholds)
the network starts saving power around cycle 300, 000, which is signiﬁcantly earlier than what happens with static thresholds
(at 330,000 cycle time, Fig. 15(c)).
4.5.2. Hysteresis diagrams
In order to fully characterize the dynamic behavior of the power saving mechanism, we constructed hysteresis diagrams of
consumed power versus delivered traﬃc. In these diagrams, we show the evolution of power when traﬃc increases, or decreases.
This illustrates the links connection process as traﬃc grows and the disconnection when it drops. Figs. 17 and 18 show results
for several conﬁgurations of the map of possible thresholds, with static and dynamic thresholds respectively.
We can observe that the behavior of the network ﬁts the theoretical analysis performed in Section 3. The shape of the
hysteresis diagrams are close to what is shown in Fig. 4(a), for static thresholds, and in Fig. 4(b), for dynamic thresholds.
A comparative analysis shows that the difference in performance between the two versions of the thresholds is the power
reduction for downward traﬃc. However, the upward traﬃc curve does not signiﬁcantly change when moving from static to
dynamic thresholds, as predicted by the theoretical analysis. It is experimentally found that the desired effect of keeping constant
the actual hysteresis band using dynamic thresholds is met. As a result, the traﬃc range which allows saving power is increased.

0.6

75

65

0.4

60
55

0.3

50
0.2

45
40

Traffic (flits/cycle/node)

0.5

70

0.1

35
30
100000

200000

300000

400000

1.1

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40
0.4

35
30
0

100000

Time (cycles)

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40
35

0.4

30

0.3
500000

300000

400000

Relative Power Consumption

1.1

Average Message Latency from Generation (cycles)

Average Message Latency from Generation (cycles)

Uon = 0.63000

200000

0.3
500000

400000

(b) Latency and power, setting 2.

80

100000

300000

Time (cycles)

(a) Nominal traffic and latency.

0

200000

Uon = 0.60375
80

1.1

75

1

70
0.9
65
60

0.8

55

0.7

50

0.6

45
0.5
40

Relative Power Consumption

0

0
500000

Uon = 0.21000
80

Relative Power Consumption

Nominal Performance
80

Average Message Latency from Generation (cycles)

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

Average Message Latency from Generation (cycles)

76

0.4

35
30
0

100000

Time (cycles)

200000

300000

0.3
500000

400000

Time (cycles)

(c) Latency and power, setting 9.

(d) Latency and power, setting 12.

Fig. 16. Dynamic evaluation with self-similar traﬃc, dynamic thresholds. 4-ary 4-tree.
Thresholds 0.05250−0.26250

Thresholds 0.07875−0.39375

1.1

1.1
1
Relative Power Consumption

Relative Power Consumption

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0

0.1

0.2

0.3

0.4

0.5

0.8
0.7
0.6
0.5
0.4
0.3

Upward
Downward

0.2

0.9

Upward
Downward

0.2
0.6

0

0.1

Traffic (flits/cycle/node)

0.3

0.4

0.5

0.6

Traffic (flits/cycle/node)

(a) Power consumption, setting 3.

(b) Power consumption, setting 6.

Thresholds 0.02625−0.44625

Thresholds 0.10500−0.52500

1.1

1.1
1
Relative Power Consumption

1
Relative Power Consumption

0.2

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0

0.1

0.2

0.3

0.4

0.5

Traffic (flits/cycle/node)

(c) Power consumption, setting 10.

0.8
0.7
0.6
0.5
0.4
0.3

Upward
Downward

0.2

0.9

Upward
Downward

0.2
0.6

0

0.1

0.2

0.3

0.4

0.5

Traffic (flits/cycle/node)

(d) Power consumption, setting 11.

Fig. 17. Hysteresis diagrams for static thresholds. 4-ary 4-tree.

0.6

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

77

Uon = 0.26250

Uon = 0.39375

1.1

1.1
1
Relative Power Consumption

Relative Power Consumption

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0

0.1

0.2

0.3

0.4

0.5

0.8
0.7
0.6
0.5
0.4
0.3

Upward
Downward

0.2

0.9

Upward
Downward

0.2
0.6

0

0.1

Traffic (flits/cycle/node)

0.2

(a) Power consumption, setting 3.

0.4

0.5

0.6

(b) Power consumption, setting 6.

Uon = 0.44625

Uon = 0.52500

1.1

1.1
1
Relative Power Consumption

1
Relative Power Consumption

0.3

Traffic (flits/cycle/node)

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0

0.1

0.2

0.3

0.4

0.5

0.8
0.7
0.6
0.5
0.4
0.3

Upward
Downward

0.2

0.9

Upward
Downward

0.2
0.6

0

0.1

Traffic (flits/cycle/node)

0.2

0.3

0.4

0.5

0.6

Traffic (flits/cycle/node)

(c) Power consumption, setting 10.

(d) Power consumption, setting 11.

Fig. 18. Hysteresis diagrams for dynamic thresholds. 4-ary 4-tree.
Table 4
Energy consumption for static thresholds. 4-ary 4-tree.
Active

Nominal T

Poweraware – static thresholds

nodes

(cycles)

Cycles

10%
20%
30%
40%
50%
60%
70%
80%
90%
100%

829275
426181
293784
224031
183735
155652
135697
121729
110223
101183

917576
484425
317378
238939
188027
155930
135697
121729
110223
101183

T
10.65%
13.67%
8.03%
6.65%
2.34%
0.18%
0%
0%
0%
0%

Relative power
37.78%
44.51%
61.35%
68.14%
85.29%
99.36%
100%
100%
100%
100%

E
−46.08%
−38.31%
−26.37%
−21.35%
−10.07%
−0.35%
0%
0%
0%
0%

4.6. Impact on energy
In this section, the evaluation is completed with energy results to provide additional evidence on the impact of the proposed
strategy. Several network sizes (256, 512 and 4096 nodes) with different switch radix and number of stages have been tested.
Energy consumption of the network is evaluated using a closed load model. The simulation is set to exchange a ﬁxed number
of messages (500,000 messages for 256 and 512-node topologies and 4 million messages for 4096-node topologies). There are
dependencies between the messages, so that when a node receives a message, it responds with another one. Thus, latency of
messages impacts the time new messages are generated, unlike the previous open load model. It is also possible to control the
workload by setting the number of active nodes in the network. The number of nodes generating messages is varied between
10% and 100%.
Detailed results for a 4-ary 4-tree are shown in Tables 4 and 5. Table 4 shows results for static thresholds, Uoff = 0.1575
and Uon = 0.4725 and Table 5 for dynamic thresholds, Uon = 0.4725. The selected conﬁguration corresponds to setting 7 in the
threshold map (Fig. 3), which provides intermediate values of responsiveness and aggressiveness.
The tables show in the second column (Nominal T) the time required to deliver all messages in a network without any power
saving mechanism. This time is increased when the power saving mechanism is activated (third and fourth columns of Tables 4

78

M. Alonso et al. / Parallel Computing 48 (2015) 59–80
Table 5
Energy consumption for dynamic thresholds. 4-ary 4-tree.
Active

Nominal T

Power aware – dynamic thresholds

nodes

(cycles)

Cycles

10%
20%
30%
40%
50%
60%
70%
80%
90%
100%

829275
426181
293784
224031
183735
155652
135697
121729
110223
101183

927982
499542
314330
247215
202670
173772
142056
128125
117142
108239

T
11.90%
17.21%
6.99%
10.34%
10.30%
11.64%
4.69%
5.25%
6.28%
6.97%

Relative power

E
−41.08%
−38.89%
−26.74%
−17.06%
−18.03%
−12.25%
−0.06%
−0.07%
−0.04%
−0.03%

42.54%
41.91%
61.74%
69.86%
68.83%
74.03%
87.85%
86.18%
88.31%
88.43%

Table 6
Energy consumption for static thresholds and different topologies.
Active nodes

Static thresholds
8-ary 3-tree
T

10%
20%
30%
40%
50%
60%
70%
80%
90%
100%

7.51%
1.34%
0.33%
0.11%
0%
0%
0%
0%
0%
0%

E
−35.38%
−28.45%
−14.58%
−1.68%
0%
0%
0%
0%
0%
0%

8-ary 4-tree
T
9.79%
4.54%
1.19%
0.38%
0.06%
0.01%
0%
0%
0%
0%

E
−44.98%
−37.40%
−25.82%
−13.10%
−2.80%
0%
0%
0%
0%
0%

16-ary 2-tree
T
2.17%
0%
0%
0%
0%
0%
0%
0%
0%
0%

E
−26.08%
−9.12%
0%
0%
0%
0%
0%
0%
0%
0%

16-ary 3-tree
T
1.12%
0%
0%
0%
0%
0%
0%
0%
0%
0%

E
−38.96%
−22.30%
−5.61%
0%
0%
0%
0%
0%
0%
0%

Table 7
Energy consumption for dynamic thresholds and different topologies.
Active nodes

Dynamic thresholds
8-ary 3-tree
T

10%
20%
30%
40%
50%
60%
70%
80%
90%
100%

4.80%
3.71%
3.24%
3.25%
4.22%
3.30%
4.29%
3.03%
2.67%
2.20%

E
−38.64%
−30.74%
−23.72%
−14.69%
−11.91%
−6.65%
−4.60%
−2.00%
−1.09%
−0.47%

8-ary 4-tree
T
3.67%
3.34%
3.64%
3.00%
2.82%
3.63%
4.31%
3.42%
4.30%
3.50%

E
−43.96%
−34.41%
−27.52%
−22.56%
−16.00%
−13.95%
−10.79%
−7.81%
−6.05%
−3.19%

16-ary 2-tree
T
0%
0.06%
0.19%
0.20%
0.23%
0.25%
0.13%
0%
0%
0%

E
−25.48%
−19.70%
−13.85%
−8.52%
−4.59%
−1.92%
−0.01%
0%
0%
0%

16-ary 3-tree
T
0.24%
0.19%
0.28%
0.45%
0.71%
0.94%
1.10%
1.05%
0.65%
0.05%

E
−36.53%
−28.82%
−20.36%
−13.54%
−10.41%
−7.26%
−4.72%
−2.41%
−0.44%
−0.02%

and 5, cycles, T), because some links are disconnected according to the selected threshold settings. The power consumed by the
network is, in this cases, a fraction of the nominal value (ﬁfth column, Relative power). In the analysis of energy consumption the
energy consumed by switches is also considered, assuming that they dissipate 17.6% of the total power of the network links [4].
Switch power is included because during the additional running time due to the power saving mechanism, all components of the
network are active (and not just the links) and contribute to energy consumption. The rightmost column of the tables ( Energy)
shows that energy consumption is always reduced with the power saving mechanism, despite the increase in the time required
to deliver all messages. Similar results were obtained for other conﬁgurations of the thresholds.
Results for the 4-ary 4-tree show that signiﬁcant energy savings (up to 46%) can be obtained, while keeping the performance
degradation below 17% in the worst case. As predicted by the static evaluation (Section 4.4), the mechanism provides higher
energy savings when using the dynamic thresholds. In addition, by tuning the mechanism aggressiveness, the impact on energy
and performance can be bounded to different requirements.
Additional experiments for various topologies have been conducted to illustrate the power saving mechanism behavior with
higher radix switches and higher number of levels. The results are summarized in Tables 6 and 7, for threshold sets corresponding
to setting 7 of the threshold map (Fig. 3). Only execution time and energy increase metrics are shown.

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

79

Results conﬁrm that signiﬁcant savings can be obtained when the power saving mechanism is used, in particular with dynamic
thresholds. In that case, it is interesting to notice that the performance degradation remains within 5% (even lower, 1.10%, for the
16-ary conﬁguration). The results show that conﬁgurations with higher radix, for the same number of nodes, give lower energy
savings (i.e. compare 8-ary 4-tree and 16-ary 3-tree results). A network with higher radix switches requires less links and hence
the applicability of our mechanism is reduced. In these conﬁgurations the traﬃc distributes among less links and their average
utilization increases. It is also important to notice that energy savings are obtained when less than 60% of the nodes in the
network are active. This is a typical situation in a Google cluster, where servers spend most of its time in the 10–50% utilization
range [3].
5. Conclusions
In this paper a power saving mechanism for fat-tree interconnection networks has been presented. The proposed mechanism
is based on connecting or disconnecting links in the network, based on the traﬃc, while maintaining full connectivity between
nodes. The mechanism has been characterized as a function of two control parameters, which allow to set the network energy
management policy. The exhaustive experimental evaluation presented demonstrates the ability of the mechanism to provide
signiﬁcant power savings with a very light impact in latency. It has been veriﬁed that the product of relative power by relative
latency provides very favorable results. Only in certain, extremely aggressive, conﬁgurations with long messages and dynamic
thresholds slightly unfavorable results have been observed. Even so, the mechanism supports a wide range of conﬁgurations that
combine aggressiveness and responsiveness with a positive power-latency balance.
Conﬁgurations with high node count and high radix switches (8-ary 4-tree and 16-ary 3-tree) can also beneﬁt from the
power saving mechanism. Experiments conducted on a 16-ary 3-tree showed that energy savings up to 36% can be obtained with
performance degradation below 1.10%.
Our ﬁndings, summarized in Table 1 (power consumption lower bound) and –7 (energy), indicate that the power reduction
mechanism ability to provide power savings depends on the network conﬁguration (i.e. switch radix, and hence number of
stages). For instance, an 8-ary 4-tree (4096 nodes) always provide better results if compared to a 16-ary 3-tree (4096 nodes)
running the same load. This is illustrated in Tables 6 and 7 and it is also supported by the fact that the power lower bound
is smaller in the 8-ary 4-tree (Table 1). The same conclusion is obtained when comparing other conﬁgurations with the same
number of nodes. Thus, the same behavior can be extended to even larger conﬁgurations.
To sum up, we have proposed a distributed power saving mechanism that does not require changes in the underlying routing
algorithm. The evaluation results show that the mechanism provides a signiﬁcant reduction of power consumption in fat-tree
networks with little impact in performance and an always favorable energy balance.
Acknowledgments
This work was supported by the Spanish Ministerio de Economa y Competitividad (MINECO) and by FEDER funds under Grant
TIN2012-38341-C04-01
References
[1] E.J. Kim, K.H. Yum, G.M. Link, N. Vijaykrishnan, M. Kandemir, M.J. Irwin, M. Yousif, C.R. Das, Energy optimization techniques in cluster interconnects,
in: Proceedings of the 2003 International Symposium on Low Power Electronics and Design, ISLPED’03, ACM, New York, NY, USA, 2003, pp. 459–464.
http://dx.doi.org/10.1145/871506.871620.
[2] A. Greenberg, J. Hamilton, D.A. Maltz, P. Patel, The cost of a cloud: research problems in data center networks, SIGCOMM Comput. Commun. Rev. 39 (1)
(2008) 68–73. http://dx.doi.org/10.1145/1496091.1496103.
[3] D. Abts, M.R. Marty, P.M. Wells, P. Klausler, H. Liu, Energy proportional datacenter networks, in: Proceedings of the 37th Annual International Symposium
on Computer Architecture, ISCA ’10, ACM, New York, NY, USA, 2010, pp. 338–347. http://dx.doi.org/10.1145/1815961.1816004.
[4] L. Shang, L.-S. Peh, N.K. Jha, Dynamic voltage scaling with links for power optimization of interconnection networks, in: Proceedings of the 9th International
Symposium on High-Performance Computer Architecture, HPCA ’03, IEEE Computer Society, Washington, DC, USA, 2003, p. 91.
[5] J.M. Stine, N.P. Carter, Comparing adaptive routing and dynamic voltage scaling for link power reduction, Computer Architecture Letters.
[6] V. Soteriou, L.-S. Peh, Dynamic power management for power optimization of interconnection networks using on/off links, in: Hot Interconnects 11, Stanford
University, Palo Alto, CA, 2003.
[7] V. Soteriou, L.-S. Peh, Design-space exploration of power-aware on/off interconnection networks, in: Proceedings of the 22nd International Conference on
Computer Design (ICCD’04), San Jose, 2004, pp. 510–517.
[8] M. Koibuchi, T. Otsuka, H. Matsutani, H. Amano, An on/off link activation method for low-power ethernet in pc clusters, in: Proceedings of the
2009 IEEE International Symposium on Parallel & Distributed Processing, IPDPS ’09, IEEE Computer Society, Washington, DC, USA, 2009, pp. 1–11.
http://dx.doi.org/10.1145/1815961.1816004.
[9] F. Petrini, M. Vanneschi, Performance analysis of wormhole routed k-ary n-trees, Int. J. Found. Comput. Sci. 9 (2) (1998) 157–177.
[10] J. Vetter, F. Mueller, Communication characteristics of large-scale scientiﬁc applications for contemporary cluster architectures, in: Proc. International
Parallel and Distributed Processing Symposium (IPDPS), 2002.
[11] J. Shalf, S. Kamil, L. Oliker, D. Skinner, Analyzing ultrascale application communication requirements for a reconﬁgurable hybrid interconnect, in: Super
Computing, 2005.
[12] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis, P. Sharma, S. Banerjee, N. McKeown, Elastictree: Saving energy in data center networks, in:
Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation, NSDI’10, USENIX Association, Berkeley, CA, USA, 2010, p. 17.
[13] Q. Yi, S. Singh, Minimizing energy consumption of fattree data center networks, SIGMETRICS Perform. Eval. Rev. 42 (3) (2014) 67–72.
http://dx.doi.org/10.1145/2695533.2695558.
[14] IEEE P802.3az, IEEE P802.3az Energy Eﬃcient, Ethernet Task Force home page, http://www.ieee802.org/3/az/ (2010).
[15] W.J. Dally, B. Towles, Principles and Practices of Interconnection Networks, Morgan Kaufmann, 2004.
[16] J. Duato, S. Yalamanchili, L. Ni, Interconnection Networks: an Engineering Approach, Morgan Kaufmann, 2002.

80

M. Alonso et al. / Parallel Computing 48 (2015) 59–80

[17] M. Alonso, J.-M. Martínez, V. Santonja, P. López, J. Duato, Power saving in regular interconnection networks built with high-degree switches, in: Proceedings
of the 19th IEEE International Parallel and Distributed Processing Symposium (IPDPS’05), IPDPS ’05, IEEE Computer Society, Los Alamitos, CA, USA, 2005.
[18] M. Alonso, S. Coll, V. Santonja, J.-M. Martínez, P. López, J. Duato, Power aware fat-tree networks using on/off links, in: Proceedings of the 3rd International
Conference on High Performance Computing and Communications (HPCC 2007), HPCC ’07, Lecture Notes in Computer Science, 2007.
[19] G. De Michelli, L. Benini, Network on Chips, Morgan Kauffmann, 2006.
[20] J. Kim, M.A. Horowitz, Adaptive supply serial links with sub-1v operation and per-pin clock recovery, IEEE J. Solid State Circuits 37 (11) (2002) 1403–1413.
[21] T. Karagiannis, M. Faloutsos, SELFIS: a tool for self-similarity and long-range dependence analysis, in: 1st Workshop on Fractals and Self-Similarity in Data
Mining: Issues and Approaches, Edmonton, Canada, 2002.

