Image and Vision Computing 30 (2012) 467–468

Contents lists available at SciVerse ScienceDirect

Image and Vision Computing
journal homepage: www.elsevier.com/locate/imavis

Discussion

Mathematical statistics and computer vision☆
Rama Chellappa ⁎
Department of Electrical and Computer Engineering and the Center for Automation Research, UMIACS, University of Maryland, College Park, MD, United States

a r t i c l e

i n f o

Keywords:
Computer vision
Mathematical statistics
Manifolds
Markov random ﬁelds
Particle ﬁlters
Simulated annealing

a b s t r a c t
In this discussion paper, I present my views on the role on mathematical statistics for solving computer vision
problems.
© 2012 Published by Elsevier B.V. Open access under CC BY-NC-ND license.

When I was looking for a Ph.D. dissertation topic, I accidentally came
across a paper by W. Larimore on Statistical Inference on Random Fields
in the Proceedings of the IEEE [6]. This paper discussed parameter estimation and hypothesis testing methods for two-dimensional non-causal
models. This paper led me to classical papers by Peter Whittle [9], Yu
Rozanov [7], John Woods [10] and Julian Besag [1], as well as the paper
by Besag and Moran [2] on parameter estimation in Gaussian Markov
random ﬁeld models. As a student in Electrical and Computer Engineering, I had been exposed to basic concepts in parameter estimation, random processes and decision theory and I was naturally attracted to the
possibilities of using mathematical statistical framework for computer vision problems. I wrote my dissertation on stochastic models for image
processing and understanding. Since then, I have worked on mathematical statistics-based approaches for many computer vision problems. Since
most computer vision problems are concerned with inferring some properties (radiometric, geometric,…) from images and videos, tools from
mathematical statistics are very useful solving computer vision problems.
When one considers inferring 3D geometry from images and videos,
computer vision problems can be immensely challenging for mathematical statisticians. Another reason, why statistical methods may be effective for computer vision problems is that one is then able to account for
degradations in the data using appropriate distributions; any prior information can also be incorporated in a Bayesian framework. By throwing in
non-parametric inference tools, manifolds etc., one can have even more
fun.
Statistical methods were not always welcome in computer vision. In
the early years, mostly linear models and Gaussian distributions were
used while developing statistical inference methods for computer vision;
the simplicity of these models did not ﬁnd favor with leading computer
vision researchers. My mentor Azriel Rosenfeld felt that unless methods
☆ This paper has been recommended for acceptance by special issue Opinions Editor
Sinisa Todorovic.
⁎ Tel.: + 1 301 405 4525; fax: + 1 301 314 9115.
E-mail address: rama@cfar.umd.edu.
0262-8856 © 2012 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.imavis.2012.03.008

that did not rely on linear models and Gaussian distributions were available, statistical methods will not scale up to the challenges of computer
vision problems. Ulf Grenander was working on abstract mathematical
and statistical models and methods for many computer vision problems
and presented his ﬁndings in books that were not easily understood by
mainstream computer vision researchers. As a result, Prof. Grenander's
work was seen as esoteric. David Cooper was also vigorously pursuing
Bayesian methods for boundary and object recognition [4]. A seismic
shift occurred in 1984 thanks to the seminal paper on simulated
annealing, stochastic relaxation and MAP restoration of images by
Geman and Geman [5] that appeared in PAMI. This paper demonstrated
that foundational methods from mathematical statistics can and will
make a signiﬁcant impact on computer vision. I believe from this moment on statistical models and methods became acceptable to the computer vision community. The paper by Geman and Geman opened a
ﬂoodgate of papers on image segmentation, restoration, classiﬁcation,
optical ﬂow estimation, etc. A deterministic alternative known as the iterated condition mode was presented in the paper by Besag [3] on statistical analysis of dirty pictures that appeared in the Journal of Royal
Statistical Society. The simulated annealing algorithm inspired algorithms like mean ﬁeld annealing, graduated non-convexity and maximum posterior marginal. The Bayesian formulation is an entrenched
methodology vigorously being pursued by numerous computer vision
researchers (for example, Geman, Yuille, Zhu and many others). MRFs
are here to stay in computer vision. Many optimization methods that
are currently popular have their roots in optimization of posterior probability density functions derived using MRF representations. A good example of the impact of MRF-driven methods is the well cited paper on
the comparison of energy minimization methods for MRFs that
appeared in PAMI in 2008 [8].
It is often said that timing is everything even in scholarly research
pursuits. Principal Component Analysis (PCA) is a well known dimensionality reduction in statistics. When PCA was applied to face representation and recognition in the late eighties and early nineties, it
generated a tsunami of papers leading to various subspace-based

468

R. Chellappa / Image and Vision Computing 30 (2012) 467–468

approaches for face and object recognition. Methods based on Fishers
Linear Discriminant Analysis (LDA), kernel-PCA, kernel-LDA and numerous variations thereof (for example, partial least squares) have
been developed over the last two decades. The application of Support
Vector Machines (SVM) to problems ranging from OCR to face recognition is another positive example of the impact of methods rooted in
mathematics statistics in solving computer vision problems. Although
these methods appear to be effective, they do not work well with variations due to pose, illumination variations, occlusions etc. Recently,
methods based on domain adaptation are being developed for designing classiﬁers that can adapt to the so called domain shifts due to
pose, illumination, blur, etc. This appears to be a promising approach.
Another well known example that clearly illustrates the impact of
statistical inference on computer vision is the emergence of particle ﬁlters. Although the idea of particle ﬁltering was known to the radar
tracking community and as jump diffusion process in stochastic ﬁltering
world, the adaptation of particle ﬁlters to a tracking problem (by Isard
and Blake) that computer vision researchers can relate to contributed
to its immense success. In fact, the particle ﬁlter has become one of
the major tools in the computer vision tool kit, that even students
who have not been exposed to random process or estimation theory
or Kalman ﬁlter are able to use it with ease! The lesson to be learned
here is that if a computer vision algorithm can be in OpenCV or
MATLAB, it has a long life! On the ﬂip side, it is not a good idea that
folks who have not been exposed to even the basics of random processes and estimation theory should be applying particle ﬁlters. This can be
discussed in a separate article on how to educate a computer vision
researcher.
The last example I would like to mention is the statistical analysis of
boundaries, shapes using landmarks, manifolds etc. Following the seminal works of Kendall, Mardia, Grenander, Cooper and others, recent efforts by Anuj Srivastava, Laurent Younes, Mike Miller and others have
brought new models and methods based on differential geometry and
statistical inference to bear fruits in an important area of computer
vision. Statistical inference on manifolds for object, event, and gesture
recognition is gaining acceptance by computer vision community.
Other concepts from mathematical statistics that have impacted the
computer vision area are robust methods for computer vision, performance evaluation, Monte Carlo techniques, Lasso and ensemble learning.
Space limitations do not permit an elaborate discussion of these topics.
Developing effective statistical inference methods that can handle
structured (or geometric) data is something that should be of interest
to computer vision researchers and mathematical statisticians. One of
my teachers at Purdue, Prof. K.S. Fu used to say that statistical methods
that cannot handle structure would not be very effective for pattern recognition and image understanding and that is why he strongly believed
in designing grammars for pattern analysis and recognition problems. In
the late seventies and early eighties, Prof. Fu and his students introduced
stochastic grammars and associated inference methods for many image
analysis and pattern recognition problems. The design and inference of
stochastic grammars, ontologies, Markov logic networks, etc. for large
scale computer vision problems have received much attention during

the past decade and will continue to be a productive area of research.
Past and ongoing research in the area of probabilistic reasoning and
graphical models pioneered by Pearl and Jordan respectively provide
ample inspiration for integrating structure and statistical inference.
Given the diversity of computer vision research area (computer
scientists, electrical engineers, statisticians, neuroscientists, and psychophysicists are involved), it is tempting for one or more groups of
researchers to claim that “their view of the elephant” is the best
one. This is not productive. Over the years, I have learnt and my students have taught me how to apply methods from Statistics (GMRFs,
Cramer–Rao bounds, Fisher–Rao metric, inference on manifolds),
Electrical Engineering (Kalman ﬁlters, dynamic models, information
theory, Cramer–Rao bounds), and Computer Science (graph matching,
stochastic Petri nets, non-monotonic reasoning and sub-modular functions) for a wide variety of computer vision problems. I have enjoyed all
of these and feel computer vision provides sufﬁcient space for anyone
who is earnest enough to get involved. Let us all have a pleasant ride
for the years to come.
In sum, mathematical statistical methods have played and will
continue to play a big role to play in the development of robust algorithms for many computer vision problems. In order to work in this
area or appreciate ongoing works in this area, the students are well
advised to take additional courses in statistical inference, differential
geometry, random processes, estimation theory, linear system theory,
and optimization techniques.
In writing this article, I may have inadvertently omitted many related works of signiﬁcant impact. Space limitations do not permit
elaborate discussions of all the efforts that are related to the theme
of this article. My sincere apologies to those who have promoted
the application of mathematical statistics principles to computer vision problems but are not mentioned here.

References
[1] J.E. Besag, Spatial interaction and the statistical analysis of lattice systems, J. Roy.
Stat. Soc. B 36 (1974) 192–236.
[2] J.E. Besag, P.A.P. Moran, On the estimation and testing of spatial interaction in
Gaussian lattice process, Biometrika 62 (1975) 555–562.
[3] J.E. Besag, On the statistical analysis of dirty pictures, J. Roy. Stat. Soc. B 48 (1986)
259–302.
[4] D.B. Cooper, H. Elliott, F. Cohen, L. Reiss, P. Symosek, Stochastic boundary estimation
and object recognition, Comput. Vis. Graph. Image Process. 12 (1980) 326–356.
[5] S. Geman, D. Geman, Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images, IEEE Trans. Pattern Anal. Mach. Intell. 6 (1984) 721–741.
[6] W.E. Larimore, Statistical inference on stationary random ﬁelds, Proc. IEEE 65
(1977) 961–970.
[7] Yu. Rozanov, On Gaussian ﬁelds with given distributions, Theory Probab. Appl. 11
(1967) 381–391.
[8] R. Szeliski, R. Zabih, D. Daniel Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala,
M. Tappen, C. Rother, A comparative study of energy minimization methods for
Markov random ﬁelds with smoothness-based priors, IEEE Trans. Pattern Anal.
Mach. Intell. 30 (2008) 1068–1080.
[9] P. Whittle, On stationary processes in the plane, Biometrika 41 (1954) 434–449.
[10] J.W. Woods, Two-dimensional discrete Markovian ﬁelds, IEEE Trans. Inf. Theory
18 (1972) 232–240.

