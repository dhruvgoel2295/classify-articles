Neural Networks 71 (2015) 55–61

Contents lists available at ScienceDirect

Neural Networks
journal homepage: www.elsevier.com/locate/neunet

Novel conditions on exponential stability of a class of delayed neural
networks with state-dependent switching
Guodong Zhang a,∗ , Yi Shen b
a

College of Mathematics and statistics, South-Central University For Nationalities, Wuhan 430074, China

b

School of Automation, Huazhong University of Science and Technology, Wuhan, 430074, China

article

info

Article history:
Received 29 August 2014
Received in revised form 1 April 2015
Accepted 30 July 2015
Available online 6 August 2015
Keywords:
Exponential stability
Switching neural networks
Filippov solutions
Time-varying delays

abstract
This paper is concerned with the global exponential stability on a class of delayed neural networks with
state-dependent switching. Under the novel conditions, some sufficient criteria ensuring exponential
stability of the proposed system are obtained. In particular, the obtained conditions complement and
improve earlier publications on conventional neural networks with continuous or discontinuous righthand side. Numerical simulations are also presented to illustrate the effectiveness of the obtained results.
© 2015 Elsevier Ltd. All rights reserved.

1. Introduction
Memristive devices as neurons synapses have been the focus
of much recent attention in neurodynamic systems (Chua, 2011;
Huang, Feng, & Mohamad, 2012; Itoh & Chua, 2009; Kim, Sah, Yang,
Roska, & Chua, 2012; Sharifiy & Banadaki, 2010). Such memristive
neural systemscan provide enormous inspiration for theoretical
physicist, systems biologist and electronics engineer to design
brain-like processing systems, which promote the development
of hardware that mimics biological architectures in the nervous
systems (Itoh & Chua, 2009; Liu, 2009).
Recently, Hu and Wang (2010) study asymptotic stability of
memristive neural networks by constructing proper Lyapunov
functionals and using differential inclusion theory, and then, stability and synchronization control of memristive neural networks
were further investigated in Bao and Zeng (2013), Chen, Zeng, and
Jiang (2014), Guo, Wang, and Yan (2013), Guo, Wang, and Yan
(2014), Wu and Zeng (2012a), Wu, Wen, and Zeng (2012), Wu
and Zeng (2014), Wu and Zeng (2012b), Wen and Zeng (2012),
Yang, Cao, and Yu (2014), Zhang, Shen, and Sun (2012), Zhang and
Shen (2013), Zhang, Shen, Yin, and Sun (2013) and Zhang and Shen
(2014).
Generally speaking, because memristors possess the characteristic of pinched hysteresis loop, memristive neural networks

∗

Corresponding author. Tel.: +86 27 87543630; fax: +86 27 87543130.
E-mail address: zgdhbnu@163.com (G. Zhang).

http://dx.doi.org/10.1016/j.neunet.2015.07.016
0893-6080/© 2015 Elsevier Ltd. All rights reserved.

should be coefficients switching system. In fact, the previous work
in Bao and Zeng (2013), Chen et al. (2014), Guo et al. (2013, 2014),
Wu and Zeng (2012a), Wu et al. (2012), Wu and Zeng (2014,
2012b), Wen and Zeng (2012), Yang et al. (2014), Zhang et al.
(2012), Zhang and Shen (2013), Zhang et al. (2013) and Zhang
and Shen (2014) only considered neural networks with some special coefficients switching, they should be called state-dependent
switching neural networks, which are different from the neural
networks with discontinuous neuron activations, and also are different from the neural networks with discontinuous right side in
which switching depends on time t. Therefore, neural networks
with state-dependent switching complement earlier publications
on conventional neural networks with continuous or discontinuous right-hand side.
Whereas, in the past decades, state-dependent switching nonlinear system has not received considerable attention. With the
development and application of memristors, some fundamental
results and the preparative work for the studies of state-dependent
nonlinear system with its various generalizations may be an important area of study, to really allow the memristors to be readily
used in neural networks.
However, in the existing literatures, the obtained results about
state-dependent switching neural networks are somewhat complicated and some conditions are very difficult to verify. So, it is worth
thinking deeply about that within mathematical framework of the
Filippov solutions.
In this paper, we will build the novel conditions, then under
the proposed conditions, some sufficient criteria ensuring expo-

56

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61

nential stability of a class of delayed neural networks with statedependent switching as follows:
dxi (t )
dt

= −di (xi (t ))xi (t ) +

n


aij (xj (t ))fj (xj (t ))

dxi (t )

j =1

+

n


Through the theories of differential inclusion and set-valued
map (Aubin & Cellina, 1984; Clarke, Ledyaev, Stem, & Wolenski,
1998; Filippov, 1988), from (1), it follows that

dt

+

j =1

aij (xj (t )) =

n


t ≥ 0, i ∈ N ,

(1)

(5)

where

d∗i , xi (t ) ≤ 0,
d∗∗
xi (t ) > 0,
i ,

d∗i , xi (t ) < 0,
co{d∗i , d∗∗
xi (t ) = 0,
i },
d∗∗
xi (t ) > 0,
i ,


co[di (xi (t ))] =

a∗ij , xj (t ) ≤ 0,
a∗∗
xj (t ) > 0,
ij ,

bij (xj (t − τj (t ))) =

co[bij (xj (t − τj (t )))]gj (xj (t − τj (t ))) + Ii ,

for a.a. t ≥ 0, i ∈ N ,





co[aij (xj (t ))]fj (xj (t ))

j =1

where
di (xi (t )) =

n

j =1

bij (xi (t − τj (t )))gj (xj (t − τj (t )))

+ Ii ,

∈ −co[di (xi (t ))]xi (t ) +

(2)

b∗ij , xi (t − τj (t )) ≤ 0,
b∗∗
xi (t − τj (t )) > 0,
ij ,



(3)

∗ ∗∗ ∗ ∗∗
in which d∗i > 0, d∗∗
i > 0, aij , aij , bij , bij , i, j = 1, 2, . . . , n, are all
constant numbers. Obviously, the neural networks (1) are a statedependent switching system, which is the generalization of those
for conventional neural networks.
The organization of this paper is as follows. Preliminaries are
introduced in Section 2. In Section 3, some novel conditions for
exponential stability are derived. And then, numerical simulations
are given to demonstrate the effectiveness of the proposed
approach in Section 4. Finally, our conclusion is given.

(6)

 ∗
aij , xj (t ) < 0,
∗ ∗∗
co[aij (xj (t ))] = co{aij , aij }, xj (t ) = 0,
a∗∗ , x (t ) > 0,
j
ij
 ∗
bij , xj (t − τj (t )) < 0,
∗ ∗∗
co[bij (xj (t − τj (t )))] = co{bij , bij }, xj (t − τj (t )) = 0,
b∗∗ , x (t − τ (t )) > 0.
j
j
ij

(7)

(8)

It is obvious that the set-valued map
dxi (t )

−co[di (xi (t ))]xi (t ) +

dt

n


co[aij (xj (t ))]fj (xj (t ))

j =1

+

n


co[bij (xj (t − τj (t )))]gj (xj (t − τj (t ))) + Ii ,

j =1

Remark 1. This paper considers a class of neural networks are
different from the neural networks with discontinuous neuron
activations, and also are different from the neural networks with
discontinuous right side in which switching depends on time
t. Therefore, the obtained results here complement and extend
earlier publications on conventional neural network dynamical
systems with continuous or discontinuous right-hand side.

Definition 1. A constant vector x∗ = (x∗1 , x∗2 , . . . , x∗n )T is called an
equilibrium point of system (1), if
n


co[aij (x∗i )]fj (x∗j )

j =1

In this letter, solutions of all systems considered in the following
are intended
Filippov’s sense (Filippov, 1988). We define ∥φ∥ =
in
n
sup−τ ≤t ≤0 [ i=1 | φi (t ) |p ]1/p , where p is a constant and p ≥ 1,
for ∀ φ = (φ1 (t ), φ2 (t ), . . . , φn (t ))T ∈ C([−τ , 0], Rn ), co{ξ , ξ i }
i

denotes the convex hull of {ξ , ξ i }. For a continuous function k(t ) :
i

R → R, D+ k(t ) is called the upper right Dini derivative and defined
as D+ k(t ) = limh→0+ 1h (k(t + h) − k(t )). The initial conditions of
systems (1) are assumed to be xi (s) = ϕi (s), s ∈ [−τ , 0], ϕi (s) ∈
C([−τ , 0], R), i ∈ N.
Now, we do following assumption for system (1):
(H1 ) For j ∈ N , ∀s1 , s2 ∈ R, the neuron activation functions fj , gj
are bounded, fj (0) = gj (0) = 0 and satisfy
fj (s1 ) − fj (s2 )
s1 − s2

(9)

has nonempty compact convex values. Furthermore, it is uppersemicontinuous.

0 ∈ −co[di (x∗i )]x∗i +

2. Preliminaries

σj− ≤

for a.a. t ≥ 0, i ∈ N

≤ σj+ ,

ρj− ≤

gj (s1 ) − gj (s2 )

+

−

s1 − s2

n


co[bij (x∗i )]gj (x∗j ) + Ii ,

i ∈ N.

j =1

Remark 3. Under assumption (H1 ), if Ii = 0, i ∈ N, we can get the
origin (0, 0, . . . , 0)T is an equilibrium point of system (1).
Definition 2. The equilibrium point x∗ of system (1) is globally
exponentially stable if there exist constants β ≥ 1 and ε > 0 such
that


n


1/p
|xi (t ) − xi |

∗ p

≤ β e−εt ∥ϕ − x∗ ∥,

for ∀ t ≥ 0,

i=1

where the constant ε is the degree of exponential convergence.

≤ ρj+ ,

where s1 ̸= s2 , and σj , σj , ρj , ρj are constants.
−

+

+

Remark 2. Under the assumption (H1 ), we have the following
properties:

Lemma 1. If assumption (H1 ) holds, then there is at least a
local solution x(t ) of system (1) with initial condition ϕ(s) =
(ϕ1 (s), ϕ2 (s), . . . , ϕn (s))T ∈ C((−τ , 0], Rn ), and the local solution
x(t ) can be extended to [0, +∞) in the sense of Filippov.

(4)

Remark 4. The proof of Lemma 1 is similar to Lemma 3.1 in Duan,
Huang, and Guo (2014), we omit it here.

where Fj (u) = fj (s1 ) − fj (s2 ), Gj (u) = gj (s1 ) − gj (s2 ), u = s1 −
s2 , σj = max{|σj− |, |σj+ |}, ρj = max{|ρj− |, |ρj+ |}, j ∈ N.

Lemma 2 (Zhang et al. (2013)). If assumption (H1 ) holds, then system (1) exists at least one equilibrium point x∗ = (x∗1 , x∗2 , . . . , x∗n )T .

|Fj (u)| ≤ σj |u|,

|Gj (u)| ≤ ρj |u|,

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61

Now, let zi (t ) = xi (t ) − x∗i , z (t ) = (z1 (t ), z2 (t ), . . . , zn (t ))T , i ∈
N, we have
dzi (t )
dt

∈ −{co[di (xi (t ))]xi (t ) − co[di (x∗i )]x∗i }

In a similar way, we get

|co[bij (xj (t − τj (t )))]fj (xj (t − τj (t ))) − co[bij (x∗j )]fj (x∗j )|
≤ Bij ρj |zj (t − τj (t ))|.

n

+

(18)

3. Main results


{co[aij (xj (t ))]fj (xj (t )) − co[aij (x∗j )]fj (x∗j )}

Theorem 1. Under assumption (H1 ) and τ˙j (t ) ≤ σ0 < 1, then, the
equilibrium point x∗ of system (1) is globally exponentially stable if
the following condition is also satisfied:

j =1
n

+
{co[bij (xj (t − τj (t )))]gj (xj (t − τj (t )))
j =1

− co[bij (x∗j )]gj (x∗j )}.

57

(10)

− pDi + (p − 1)

pαij
pβij
pγij
pξij
n

[Aijp−1 σj p−1 + Bijp−1 ρjp−1 ]

j =1

Lemma 3. Under assumption (H1 ), the following conditions are true:

+

(i) {co[di (xi (t ))]xi (t ) − co[di (x∗i )]x∗i }sgn(zi (t )) ≥ Di |zi (t )|;
(ii) |co[aij (xj (t ))]fj (xj (t )) − co[aij (x∗j )]fj (x∗j )| ≤ Aij σj |zj (t )|;

n

δj

δi

j =1

(iii) |co[bij (xj (t − τj (t )))]gj (xj (t − τj (t ))) − co[bij (x∗j )]gj (x∗j )| ≤
Bij ρj |zj (t − τj (t ))|,
∗
∗∗
∗
where Di = min{d∗i , d∗∗
i }, Aij = max{|aij |, |aij |}, Bij = max{|bij |,
−
+
−
+
∗∗
|bij |}, σj = max{|σj |, |σj |}, ρj = max{|ρj |, |ρj |}, zj (t ) =
xj (t ) − x∗j , zj (t − τj (t )) = xj (t − τj (t )) − x∗j , i, j ∈ N.
Proof. If for xi (t ) = 0, x∗i = 0, we can easily have part (i)–(iii)
hold. And from (6), we can get
(1) For xi (t ) < 0, x∗i < 0, then

{co[di (xi (t ))]xi (t ) − co[di (x∗i )]x∗i }sgn(zi (t ))
= [d∗i (xi (t ) − x∗i )]sgn(zi (t ))
(11)

In a similar way, we get
(2) For xi (t ) > 0, x∗i > 0 or x∗i < 0 < xi (t ) or xi (t ) < 0 < x∗i ,
then

{co[di (xi (t ))]xi (t ) − co[di (x∗i )]x∗i }sgn(zi (t )) ≥ Di |zi (t )|.

(12)

So, from (11) and (12), we have

{co[di (xi (t ))]xi (t ) − co[di (x∗i )]x∗i }sgn(zi (t )) ≥ Di |zi (t )|.

(13)

From (4) and (7), we can get
(1’) For xj (t ) < 0, x∗j < 0, then

(19)

pβij
pγij
pξij
pαij
n
n


δj
[Aijp−1 σj p−1 + Bijp−1 ρjp−1 ] +
δ
j =1 i
j=1


eετ
p(1−γji ) p(1−ξji )
p(1−αji ) p(1−βji )
B
ρi
< 0.
(20)
× Aji
σi
+
1 − σ0 ji
n δj p(1−γji ) p(1−ξji )
Now, let µi =
ρi
, then we consider the
j=1 δi Bji

V (t , z ) =

n


δi eεt |zi (t )|p +

i=1

×

n


µi

i=1



eετ
1 − σ0

t

t −τi (t )

Vi (s, zi (s))ds,

(21)

where Vi (s, zi (s)) = δi eεs |zi (s)|p , s ≥ 0, i ∈ N.
We calculate the upper right derivation of V (t , z ) along the
solution of system (10). Under assumption (H1 ) and from Lemma 3,
we obtain
n 


εVi (t , zi (t )) + pδi eεt |zi (t )|p−1 sgn(zi (t ))

i=1

×D zi (t ) +

= |aij fj (xj (t )) − fj (xj )|
≤ Aij σj |xj (t ) − x∗j | = Aij σj |zj (t )|.

(14)

(2’) For xj (t ) > 0, x∗j > 0, then

µi eετ
[Vi (t , zi (t ))
1 − σ0

− (1 − τ˙i (t ))Vi (t − τi (t ), zi (t − τi (t )))]



n 
n


≤
(ε − pDi )Vi (t , zi (t )) +
pσj Aij |zi (t )|p−1 |zj (t )|

|co[aij (xj (t ))]fj (xj (t )) − co[aij (x∗j )]fj (x∗j )|
∗
= |a∗∗
ij fj (xj (t )) − fj (xj )|

i=1

j =1
n

≤ Aij σj |xj (t ) − x∗j | = Aij σj |zj (t )|.

(15)

+

(3) For x∗j < 0 < xj (t ) or xj (t ) < 0 < x∗j , then





pρj Bij |zi (t )|p−1 |zj (t − τj (t ))| +

j =1

µi eετ
1 − σ0

×[Vi (t , zi (t )) − (1 − τ˙i (t ))Vi (t − τi (t ), zi (t − τi (t )))]

|co[aij (xj (t ))]fj (xj (t )) − co[aij (xj )]fj (xj )|
∗

≤ |co[aij (xj (t ))]fj (xj (t ))| + |co[aij (xj )]fj (xj )|



∗

n 
n


≤
(ε − pDi )Vi (t , zi (t )) +
pσj Aij |zi (t )|p−1 |zj (t )|

≤ Aij σj |xj (t )| + Aij σj |xj |
∗

= Aij σj |xj (t ) − x∗j | = Aij σj |zj (t )|.

 < 0,

ε − pDi + (p − 1)

+

∗

p(1−ξji )

ρi

Proof. For i ∈ N, we can choose a small ε > 0 such that

D+ V (t , z ) =

∗

∗

1 − σ0



where δi > 0, 0 ≤ αij ≤ 1, 0 ≤ βij ≤ 1, 0 ≤ γij ≤ 1,
0 ≤ ξij ≤ 1, p > 1, i, j ∈ N.

|co[aij (xj (t ))]fj (xj (t )) − co[aij (x∗j )]fj (x∗j )|

i=1

(16)

+

So, from (14)–(16), we have

n


j =1

pρj Bij |zi (t )|

p−1



|zj (t − τj (t ))| + µi eετ

j =1

|co[aij (xj (t ))]fj (xj (t )) − co[aij (x∗j )]fj (x∗j )|
≤ Aij σj |zj (t )|.

Apji(1−αji ) σip(1−βji ) +

p(1−γji )

Bji

following Lyapunov functional

≥ Di |yi (t ) − xi (t )| = Di |zi (t )|.

∗



(17)



Vi (t , zi (t ))
×
− Vi (t − τi (t ), zi (t − τi (t ))) .
1 − σ0

(22)

58

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61

Furthermore, it follows from the fact
p
1

p
2

p ς1 ς2 · · · ςp ≤ ς + ς + · · · + τ ,

ςi ≥ 0, i = 1, 2, . . . , p,

p
p

we can get
n


pAij σj |zi (t )|p−1 |zj (t )|

From Theorem 1, and basing on 2-norm, we have Corollary 1.

j=1

=

n


αij
p−1

p(Aij

βij

(1−αij )

σj p−1 |zi (t )|)p−1 (Aij

(1−βij )

σj

Corollary 1. Under assumption (H1 ) and τ˙j (t ) ≤ σ0 < 1, then, the
equilibrium point x∗ of system (1) is globally exponentially stable if
the following condition is also satisfied:

|zj (t )|)

j =1

≤ (p − 1)

n


pαij
p−1

Aij

pβij

σj p−1 |zi (t )|p +

j=1

n


p(1−αij )

Aij

p(1−βij )

σj

|zj (t )|p ,

j =1

n



pBij ρj |zi (t )|p−1 |zj (t − τj (t ))|

≤ (p − 1)



pξij

pγij
p−1

Bij

ρjp−1 |zi (t )|p

j =1

+

n


p(1−γij )

Bij

p(1−ξij )

ρj



n
n


δj
Bji ρi
− 2Di +
[Aij σj + Bij ρj ] +
Aji σi +
< 0.
δ
1 − σ0
j =1
j =1 i

|zj (t − τj (t ))|p .

(23)

Corollary 2. Under assumption (H1 ), then, the equilibrium point x∗
of system (1) is globally exponentially stable if the following condition
is also satisfied:

j =1

−pDi + (p − 1)

Now, from (20) and (23), we obtain

n


D+ V (t , z )

i=1

×Vi (t , zi (t )) +

n

δi
j =1

×Vj (t , zj (t )) +

δj

+

j =1

n

δi
j=1

δj

p(1−αij )

Aij

p(1−γij )

Bij

i=1

×ρ

p(1−ξij ) ετ

ρj

p(1−αji ) p(1−βji )
Aji
i

e

δi

σ

+

eετ
1 − σ0

δi

−2Di +

n


dxi (t )
dt

Aji σi < 0.

(28)

2


aij (xj (t ))fj (xj (t ))

2


bij (xj (t − τj (t )))gj (xj (t − τj (t )))

j=1

ετ

Vi (0, zi ) + max {µi e }
1≤i≤n

i =1

≤ β∥ϕ − x∗ ∥pp e−εt ,

δi

j=1

+

V (0, z )e−εt

δ0

j=1

= −di (xi (t ))xi (t ) +

It follows from (21) and (24) that

≤

n

δj

Example. Consider the following state-dependent switching neural networks

Vi (t , zi (t ))



(27)

Now, we perform some numerical simulations to illustrate our
analysis.



n
e−εt 

< 0,

4. Numerical example

(24)

δ0

p(1−βji )

σi

Aij σj +

j =1

p(1−γji )
Bji

≤ 0.
1

p(1−αji )

Aji

Corollary 3. Under assumption (H1 ), then, the equilibrium point x∗
of system (1) is globally exponentially stable if the following condition
is also satisfied:

j =1

p(1−ξji )
i

∥z (t )∥pp ≤

pβij

σj p−1

where δi > 0, 0 ≤ αij ≤ 1, 0 ≤ βij ≤ 1, p > 1, i, j ∈ N.

p(1−βij )

σj

×Vj (t − τj (t ), zj (t − τj (t )))

 Vi (t , zi (t ))
+ µi eετ
− Vi (t − τi (t ), zi (t − τi (t )))
1 − σ0
pβij
pγij
pξij 
n 
n  pαij


p−1
p−1
p−1
p−1
+ Bij ρj
=−
ε − pDi + (p − 1)
Aij σj

n

δj

 δj
n

j =1

j =1

pαij
p−1

Aij

j =1

pβij
pγij
pξij 
n 
n  pαij


p−1
p−1
≤
ε − pDi + (p − 1)
Aij σj
+ Bijp−1 ρjp−1

+

(26)

When system (1) without time delays and bij (t − τj (t )) = 0,
i, j ∈ N, then, corresponding to Theorem 1 and Corollary 1, we
have Corollaries 2 and 3, respectively.

j=1
n

Remark 5. In Eq. (19), because all values of the parameters
αij , βij , γij , ξij can select randomly in the interval [0, 1], therefore,
we can properly choose these parameters to get Eq. (19) hold basing on practical problem, so Eq. (19) is more general and flexible.
In fact, Eq. (26) shows us an example for the application of Eq. (19).



0

Vi (s, zi (s))ds

i, j = 1 , 2

where

−τ

t ≥ 0,

+ Ii ,



(25)

d1 (x1 (t )) =

where

δ
β = [1 + max {µi eετ }] ≥ 1,
1≤i≤n
δ0
δ = max {δi }.

δ0 = min {δi },
1≤i≤n

1≤i≤n

It follows from (25) that the system (1) is globally exponentially
stable. This completes the proof of Theorem 1.

d2 (x2 (t )) =

3.5, x1 (t ) ≤ 0,
3, x1 (t ) > 0,



3, x2 (t ) ≤ 0,
3.5, x2 (t ) > 0,




−0.1, x1 (t ) ≤ 0,
−0.2, x1 (t ) > 0,

1, x2 (t ) ≤ 0,
a12 (x2 (t )) =
0.5, x2 (t ) > 0,
a11 (x1 (t )) =

(29)

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61

Fig. 1. Choose randomly 100 initial conditions, time behaviors of state x1 (t ) in
system (29) with input I = (1.5, −1.5).

a21 (x1 (t )) =
a22 (x2 (t )) =
b11 (x1 (t − τ1 (t ))) =
b12 (x2 (t − τ2 (t ))) =
b21 (x1 (t − τ1 (t ))) =
b22 (x2 (t − τ2 (t ))) =



0.5, x1 (t ) ≤ 0,
1, x1 (t ) > 0,



−0.2,
−0.1,

x2 (t ) ≤ 0,
x2 ( t ) > 0 ,



−0.5,
−0.2,

x1 (t − τ1 (t )) ≤ 0,
x1 (t − τ1 (t )) > 0,



1.0,
0.5,



0.5, x1 (t − τ1 (t )) ≤ 0,
1, x1 (t − τ1 (t )) > 0,



−0.2,
−0.5,

59

Fig. 2. Choose randomly 100 initial conditions, time behaviors of state x2 (t ) in
system (29) with input I = (1.5, −1.5).

x2 (t − τ2 (t )) ≤ 0,
x2 (t − τ2 (t )) > 0,

x2 (t − τ2 (t )) ≤ 0,
x2 (t − τ2 (t )) > 0,

t

e
, take the activation function as f1 (x1 ) = g1 (x1 ) =
and τj (t ) = 1+
et
sin(x1 ), f2 (x2 ) = g2 (x2 ) = tanh(x2 ). By simple computing, we get
A11 = 0.2, A12 = 1, A21 = 1, A22 = 0.2, B11 = 0.5, B12 = B21 =
1, B22 = 0.5, σi = ρi = 1, τ˙j (t ) ≤ σ0 = 41 < 1, i, j = 1, 2. Now,
let p = 2, δ1 = δ2 = 1, then, from (26), we have

−2D1 + 2A11 + A12 + A21 +
+

4B21
3

7B11
3

Fig. 3. Phase portrait of system (29) with input I = (1.5, −1.5).

+ B12

= −0.1 < 0,

−2D2 + 2A22 + A12 + A21 +
+ B21 = −0.1 < 0.

7B22
3

+

4B12
3

All the conditions of Corollary 1 are satisfied. It follows from Corollary 1, that system (29) is globally exponentially stable. Choose randomly 100 initial conditions and external input I = (1.5, −1.5)T ,
time behaviors of state variables x1 (t ), x2 (t ) of system (29) are
shown in Figs. 1 and 2, respectively. Fig. 3 is the phase portrait
of system (29) with input I = (1.5, −1.5). When external input
I = (0, 0)T , we know the origin of system (29) is globally exponentially stable. Choose randomly 100 initial conditions and external
input I = (0, 0)T , time behaviors of state variables x1 (t ), x2 (t ) of
system (29) are shown in Figs. 4 and 5, respectively. Some examples can also be given for other theorems, here they are omitted.

Remark 6. In order to study the dynamical behaviors of statedependent switching neural networks, the previous works (Wen
& Zeng, 2012; Wu et al., 2012; Zhang & Shen, 2013; Zhang et al.,
2013) do the following assumptions:
co[di , di ]yi (t ) − co[di , di ]xi (t ) ⊆ co[di , di ](yi (t ) − xi (t )),
co[aij , aij ]fj (yj (t )) − co[aij , aij ]fj (xj (t )) ⊆ co[aij , aij ]

×(fj (yj (t )) − fj (xj (t ))), co[bij , bij ]gj (yj (t )) − co[bij , bij ]
×gj (xj (t )) ⊆ co[bij , bij ](gj (yj (t )) − gj (xj (t ))),

(30)

however, the above conditions are difficult to verify. So, basing on
the novel conditions in Lemma 3 of this paper, we do not need the
above conditions (30). Therefore, our paper improves the previous
results.
Remark 7. In this paper, di (xi (t )), i ∈ N is not a constant and
the previous works (Chen et al., 2014; Wu & Zeng, 2012a, 2014)
need conditions (30) in their analysis, so the recent results about

60

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61

5. Conclusion
In this paper, we adopt nonsmooth analysis and control theory to handle state-dependent switching neural networks with
discontinuous right-hand side. In particular, several novel conditions in Lemma 3 were given, and basing on these conditions, some
sufficient criteria ensuring global exponential stability of statedependent switching neural networks are obtained. The results
here complemented earlier publications on conventional neural
network dynamical systems with continuous or discontinuous
right-hand side and the novel conditions proposed here improved
the present works (Filippov, 1988; Guo et al., 2013, 2014; Hu &
Wang, 2010; Huang et al., 2012; Itoh & Chua, 2009; Kim et al., 2012;
Zhang & Shen, 2013; Zhang et al., 2013). Numerical simulations are
given to illustrate effectiveness of the proposed theories. In addition, the novel conditions in Lemma 3 can be applied for dealing
with other dynamical behaviors, such as synchronization of the
state-dependent switching neural networks.
Acknowledgments
Fig. 4. Choose randomly 100 initial conditions, time behaviors of state x1 (t ) in
system (29) with input I = (0, 0).

This work is supported by Natural Science Foundation of
Hubei Province of China under Grant No. 2015CFB512, the Key
Program of National Natural Science Foundation of China under
Grant No. 61134012, and the Fundamental Research Funds for the
Central Universities of South-Central University for Nationalities
(CZQ15019).
References

Fig. 5. Choose randomly 100 initial conditions, time behaviors of state x2 (t ) in
system (29) with input I = (0, 0).

state-dependent switching neural networks obtained in Chen et al.
(2014), Wu and Zeng (2012a), Wu and Zeng (2014), Zhang and Shen
(2013) cannot be used here.
Remark 8. In the system (1), since di (xi (t )), aij (xj (t )) and bij (xj (t −
τj (t ))), i, j ∈ N are discontinuous, the results for the neural
networks with continuous right-hand side in Huang et al. (2012),
Liu (2009), Song (2008), Sheng and Yang (2008), Tan and Tan
(2007), Yang and Cao (2009), Zhao (2004) cannot be used here.
Remark 9. In fact, the novel conditions in Lemma 3 of this paper
provided us a new way to study the dynamical behaviors of
state-dependent switching neural networks. And other dynamical
behaviors, such as synchronization, of system (1) can be also
similarly investigated. These issues will be the topic of future
research.

Aubin, J. P., & Cellina, A. (1984). Differential inclusions. Germany: Berlin: SpringerVerlag.
Bao, G., & Zeng, Z. G. (2013). Multistability of periodic delayed recurrent neural
network with memristors. Neural Computing and Applications, 23, 1963–1967.
Chen, J. J., Zeng, Z. G., & Jiang, P. (2014). On the periodic dynamics of memristorbased neural networks with time-varying delays. Information Sciences, 279,
358–373.
Chua, L. O. (2011). Resistance switching memories are memristor. Applied Physics
A, 102, 765–783.
Clarke, F. H., Ledyaev, Y. S., Stem, R. J., & Wolenski, R. R. (1998). Nonsmooth analysis
and control theory. New York: Springer.
Duan, L., Huang, L. H., & Guo, Z. Y. (2014). Stability and almost periodicity for delayed
high-order Hopfield neural networks with discontinuous activations. Nonlinear
Dynamics, 77, 1469–1484.
Filippov, A. F. (1988). Differential equations with discontinuous right-hand sides.
Dordrecht: Kluwer.
Guo, Z. Y., Wang, J., & Yan, Z. (2013). Global exponential dissipativity and
stabilization of memristor-based recurrent neural networks with time-varying
delays. Neural Networks, 48, 158–172.
Guo, Z. Y., Wang, J., & Yan, Z. (2014). Attractivity analysis of memristor-based
cellular neural networks with time-varying delays. IEEE Transactions on Neural
Networks and Learning Systems, 25, 704–717.
Hu, J., & Wang, J. 2010. Global uniform asymptotic stability of memristor-based
recurrent neural networks with time delays. Proceedings of international joint
conference on neural networks, pp. 2127–2134 IJCNN 2010. Barcelona, Spain.
Huang, Z. K., Feng, C. H., & Mohamad, S. (2012). Multistability analysis for a general
class of delayed Cohen-Grossberg neural networks. Information Sciences, 187,
233–244.
Itoh, M., & Chua, L. O. (2009). Memristor cellular automata and memristor discretetime cellular neural networks. International Journal of Bifurcation and Chaos, 19,
3605–3656.
Kim, H., Sah, M. P., Yang, C. J., Roska, T., & Chua, L. O. (2012). Neural synaptic
weighting with a pulse-based memristor circuit. IEEE Transactions on Circuits
and Systems I. Regular Papers, 59, 148–158.
Liu, M. Q. (2009). Optimal exponential synchronization of general chaotic delayed
neural networks: An LMI approach. Neural Networks, 22, 949–957.
Sharifiy, M. J., & Banadaki, Y. M. (2010). General spice models for memristor and
application to circuit simulation of memristor-based synapses and memory
cells. Journal of Circuits, Systems and Computers, 19, 407–424.
Sheng, L., & Yang, H. (2008). Exponential synchronization of a class of neural networks with mixed time-varying delays and impulsive effects. Neurocomputing,
71, 3666–3674.
Song, Q. K. (2008). Exponential stability of recurrent neural networks with
both time-varying delays and general activation functions via LMI approach.
Neurocomputing, 71, 2823–2830.
Tan, M. J., & Tan, Y. (2007). Global exponential stability of periodic solution of
neural network with variable coefficients and time-varying delays. Applied
Mathematical Modelling, 33, 373–385.

G. Zhang, Y. Shen / Neural Networks 71 (2015) 55–61
Wen, S. P., & Zeng, Z. G. (2012). Dynamics analysis of a class of memristor-based
recurrent networks with time-varying delays in the presence of strong external
stimuli. Neural Processing Letters, 35, 47–59.
Wu, A. L., Wen, S. P., & Zeng, Z. G. (2012). Synchronization control of a class
of memristor-based recurrent neural networks. Information Sciences, 183,
106–116.
Wu, A. L., & Zeng, Z. G. (2012a). Dynamic behaviors of memristor-based recurrent
neural networks with time-varying delays. Neural Networks, 36, 1–10.
Wu, A. L., & Zeng, Z. G. (2012b). Exponential stabilization of memristive neural
networks with time delays. IEEE Transactions on Neural Networks and Learning
Systems, 23, 1919–1929.
Wu, A. L., & Zeng, Z. G. (2014). New global exponential stability results for a
memristive neural system with time-varying delays. Neurocomputing, 144,
553–559.
Yang, X., & Cao, J. (2009). Stochastic synchronization of coupled neural networks
with intermittent control. Physics Letters A, 373, 3259–3272.

61

Yang, X., Cao, J., & Yu, W. (2014). Exponential synchronization of memristive CohenGrossberg neural networks with mixed delays. Cognitive Neurodynamics, 8,
239–249.
Zhang, G. D., & Shen, Y. (2013). New algebraic criteria for synchronization
stability of chaotic memristive neural networks with time-varying delays. IEEE
Transactions on Neural Networks and Learning Systems, 24, 1701–1707.
Zhang, G. D., & Shen, Y. (2014). Exponential synchronization of delayed memristorbased chaotic neural networks via periodically intermittent control. Neural
Networks, 55, 1–10.
Zhang, G. D., Shen, Y., & Sun, J. W. (2012). Global exponential stability of a
class of memristor-based recurrent neural networks with time-varying delays.
Neurocomputing, 97, 149–154.
Zhang, G. D., Shen, Y., Yin, Q., & Sun, J. (2013). Global exponential periodicity and
stability of a class of memristor-based recurrent neural networks with multiple
delays. Information Sciences, 232, 386–396.
Zhao, H. (2004). Global asymptotic stability of Hopfield neural network involving
distributed delays. Neural Networks, 17, 47–53.

