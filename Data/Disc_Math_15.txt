Discrete Mathematics 311 (2011) 1998–2019

Contents lists available at ScienceDirect

Discrete Mathematics
journal homepage: www.elsevier.com/locate/disc

Diameter, connectivity, and phase transition of the uniform random
intersection graph
Katarzyna Rybarczyk
Faculty of Mathematics and Computer Science, Adam Mickiewicz University, 60–769 Poznań, Poland

article

abstract

info

Article history:
Received 15 July 2009
Received in revised form 18 May 2011
Accepted 23 May 2011
Available online 17 June 2011
Keywords:
Uniform random intersection graph
Diameter
Connectivity
Phase transition

We study properties of the uniform random intersection graph model G(n, m, d). We find
asymptotic estimates on the diameter of the largest connected component of the graph
near the phase transition and connectivity thresholds. Moreover we manage to prove an
asymptotically tight bound for the connectivity and phase transition thresholds for all
possible ranges of d, which has not been obtained before. The main motivation of our
research is the usage of the random intersection graph model in the studies of wireless
sensor networks.
© 2011 Elsevier B.V. All rights reserved.

1. Introduction
Given integers n and m, let V = {v1 , . . . , vn } and W = {w1 , . . . , wm } denote a set of vertices and a set of features,
respectively. Let D(vi ) ⊆ W be a set of features prescribed to a vertex vi ∈ V . We assume that D(v1 ), . . . , D(vn ) are drawn
independently according to the same probability distribution. A random intersection graph is a graph with vertex set V in
which two vertices vi and vj are joined by an edge if D(vi ) ∩ D(vj ) ̸= ∅ (see [12]).
If we assume that, given d = d(n), D(vi ) are drawn uniformly at random from all d-element subsets of W , i.e. for any set
A ⊆ W (|A| = d)
1
Pr{D(vi ) = A} =  m  ,
d

then we call such a random intersection graph a uniform random intersection graph and denote it by G(n, m, d).
Random intersection graphs were introduced in the article of Karoński et al. [16] and in Singer–Cohen’s Ph.D. Thesis [23]
and then followed by many other papers (for instance [1,10,11,17,21,22,24]). These works focus on random intersection
graphs in which D(vi ) is assigned to vi according to the binomial distribution, i.e. each element of W is added to D(vi )
with probability p independently of all other elements of W and vertices of V . First results concerning a uniform random
intersection graph appeared in the paper of Godehardt and Jaworski [12]. In fact, results concerning G(n, m, d) presented
in [12] are corollaries of theorems proved for a more general model introduced in [12].
In [20] di Pietro et al. pointed out that a uniform random intersection graph is the most appropriate model for theoretical
research on wireless sensor networks. The most important questions from the point of view of applications in wireless
sensor networks are: how to pick parameters n, m and d to have a uniform random intersection graph with high probability
(i.e. with probability tending to one as n tends to infinity): connected, having a largest component containing a constant
fraction of vertices and having the small diameter. In this article we concentrate on all three problems.

E-mail address: kryba@amu.edu.pl.
0012-365X/$ – see front matter © 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.disc.2011.05.029

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

1999

A partial answer to the first question was given by di Pietro et al. in [20], where it was proved that for c > 8 and m ≥ n if
d2

∼

c ln n

,

m
n
(where an ∼ bn if an /bn → 1) then with high probability G(n, m, d) is connected. Some partial results concerning the
connectivity of a uniform random intersection graph, for constant d, may also be found in the work of Godehardt et al. [13].
The connectivity of a uniform random intersection graph was also the subject of the recent result of Blackburn and Gerke [2].
In [2] the authors proved that, in the case when m = nα and α is a given constant, G(n, m, d) is with high probability
connected for c > 1 and disconnected for c < 1. In their work they conjectured that as d2 /m = (ln n + ω)/n a graph
G(n, m, d) is with high probability disconnected for ω → −∞ and connected for ω → ∞ (as n → ∞).
The second question is in fact the question concerning the so-called phase transition in a random intersection graph
(a sudden appearance of the giant connected component containing a constant fraction of vertices of a random graph). The
phase transition in the context of the analysis of wireless sensor networks was introduced by Hwang and Kim in [14]. The
first attempt to present a rigorous mathematical proof of the phase transition in a random intersection graph in the context
of wireless sensor networks was that of Bloznelis et al. [5]. It was shown that for d asymptotically larger than ln n if
d2

∼

c

,

m
n
then the phase transition threshold is for c = 1 (i.e. with high probability there is no giant component as c < 1 and there
is exactly one giant component as c > 1). In addition the result gave an asymptotic approximation of the size of the largest
component for c > 1. This result was supplemented by the one proved in [13], that for any constant d the phase transition
threshold in G(n, m, d) is for c = 1 and
d(d − 1)

∼

c

.

m
n
One of the aims of our paper is to fill the gaps left by the previous ones (i.e. to establish sharp threshold functions for
the connectivity and phase transition of G(n, m, d) for all d ≥ 2). Moreover we want to solve the third problem mentioned.
Namely, we want to estimate the diameter (maximum number of hops that a message has to make to traverse the wireless
sensor network) of the largest component near the connectivity and the phase transition thresholds.
In our paper we use standard, asymptotic Landau’s notation: an = O(bn ), an = Ω (bn ), an = o(bn ), an ∼ bn , and an ≍ bn
|a |
for ∃C >0 |an | ≤ C |bn |, ∃C >0 |an | ≥ C |bn |, |bn | → 0, ban → 1 and ∃C ,c cbn ≤ |an | ≤ Cbn , respectively. With an exception of
n
n
Section 4 all limits are taken as n → ∞. The phrase ‘‘with high probability’’ means with probability tending to one as n
tends to infinity. In addition, for any two vertices u, v ∈ V and an integer k, d(u, v) = k means that a shortest path from
u ∈ V to v ∈ V has length k. By the diameter of a graph G we denote
diam G := max{d(u, v) : u, v ∈ G}.
The paper is organised as follows. In Section 2 we state and discuss the main results of the paper. Namely, the theorems
concerning the connectivity and phase transition thresholds are presented. Their relations to other published results is also
discussed in detail. Moreover the diameter of the largest component on the thresholds is given. In Section 3 we give some
simple facts and known lemmas, which are used frequently later on. In the next section we prove some general lemmas
concerning branching processes. In Section 5 we show how to approximate the BFS procedure in G(n, m, d) with branching
process with the binomial distribution of the number of children. In the last section we give proofs of the main theorems.
2. Main results
The following theorems give asymptotically tight results concerning the connectivity and phase transition thresholds in
G(n, m, d) for all ranges of d. Theorem 1 is an answer to the conjecture posed by Blackburn and Gerke in [2].
Theorem 1. Let {bn } be a sequence, b > 0 be a constant, d = d(n) ≥ 2 and
d2 n
m

= ln n + bn .

(i) If bn → −∞ then with high probability G(n, m, d) is disconnected.
−b
(ii) If bn → b then the probability that G(n, m, d) is connected tends to e−e .
(iii) If bn → +∞ then with high probability G(n, m, d) is connected.
Until the paper submission the sharp result as stated above in (i) and (iii) has been proved by Blackburn and Gerke for
d = 2 in [2]. In the same paper, also a weaker threshold was shown, in more general setting, for m = nα and constant
α (i.e. for d2 ≍ n1−α ln n). Moreover also a weaker threshold for d = O(1) has been known (see Godehardt et al. [13]).
In addition, after the paper submission, an independent work [19] of Makowski and Yagan appeared, in which the above
theorem in case (i) and (iii) is proved for m = Ω (n) (i.e. for d2 = Ω (ln n)). To the best of our knowledge the result presented
here is still the best known since it covers all cases of m and d.

2000

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

Theorem 2. Let c > 0 be a constant, d = d(n) ≥ 2 and
d(d − 1)n

∼ c.

m

(i) If c < 1 then with high probability the largest component of G(n, m, d) is of size O(ln n).
(ii) If c > 1 then with high probability there is exactly one large component of G(n, m, d) containing a constant fraction of
vertices and the second largest component is of size O(ln n).
Until the paper submission the above result has been known for d = O(1) (i.e. for m ≍ n) [13] and ln n = o(d) (i.e. for
n ln n = o(m)) [5]. After the paper submission, independently of our work, two papers of Bloznelis [3,4] appeared, which
give an alternative proof of the above theorem for general random intersection graphs.
Another problem which is interesting from the point of view of applications in wireless sensor network is to determine
the diameter in a uniform random intersection graph. In fact we are interested in the diameter of the component containing
a constant fraction of vertices near the phase transition and connectivity thresholds. In a classical random graph G(n, p) the
problem of determining the diameter was widely studied by Bollobás [6,7], Chung and Lu [9], Łuczak [18] and answers
to the most intriguing questions were found. To the best of our knowledge, no results concerning the diameter of a
random intersection graphs are known. The following results may possibly be generalised to some other models of random
intersection graphs. Namely, in the proof a good approximation by the binomial branching process is described and given
such approximation it is possible to use techniques introduced in [8] to generalise the theorems to a wider class of random
intersection graphs (for example those, where |D(vi )| has distribution concentrated on a finite number of values).
Theorem 3. Let c > 0 be a constant, G be the largest component of G(n, m, d) and d = d(n) ≥ 2.
(i) If
d2 n

∼ c,

m ln n

then with high probability
ln n

diam G ∼

ln ln n

.

(ii) If c > 1 and
d(d − 1)n

∼ c,

m

then with high probability
diam G ≍ ln n.
3. Auxiliary inequalities and notation
We frequently use the notation
D(S ) =



D(v)

(1)

v∈S

for any subset of the set of the vertices S ⊆ V .
We use the following estimates

 x −y 
z
x
z

  =

z −1 
∏

1−

i=0

≥ 1−

z −1
−



y
x−i
y

x−i
i=0


z −1 
−
y
zy
y
= 1−
−
−
x
−
i
x
x
i=0
= 1−

≥ 1−

zy
x
zy

−

z −1
−

yi

i=0

x(x − i)
2

−

z y

.

≥1−

zy
x

−

y

z 

x( x − z )

2

x
2x(x − z )
Also in proofs we use Chernoff’s bounds (see, for example, Theorem 2.1 in [15]).

(2)

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2001

Lemma 1. If X has binomial distribution and t > 0, then
t2


Pr{X ≤ EX − t } ≤ exp −



2EX

(3)

3t 2


Pr{X ≥ EX + t } ≤ exp −

,


2 (3EX + t )

.

(4)

We also use the following simple fact.
Lemma 2. Let Ai and Bi be events such that Ai ∩ Bi = ∅ for all i ≥ i0 . Let moreover





Pr Ai0 ∪ Bi0 ≥ 1 − p
and



i−1


Pr Ai ∪ Bi 
B ≥ 1 − q,
j=i j


0

for p, q ∈ [0; 1] and all i > i0 . Then for all k ≥ i0


Pr

k


Ai ∪

k



≥ (1 − p)(1 − q)k−i0 .

Bi

i=i0

i=i0

Proof. Notice that
k


Ai ∪

k


Bi ⊇

and events i=i0 Bi , (Ai ∩
enough to prove that

k


Pr

k



+

Bi

i=i0

Ai ∩

i−1

j =i 0

i−1



Bj

∪ Ai0

j =i 0

i=i0 +1

Bj ) and Ai0 are pairwise disjoint (since Ai and Bi are disjoint for all i ≥ i0 ). Therefore it is



k
−



k


Bi ∪

i=i0

i=i0

i=i0

k


Pr Ai ∩

i−1



+ Pr{Ai0 } ≥ (1 − p)(1 − q)k−i0

Bj

(5)

j=i0

i=i0 +1

for all k ≥ i0 . We prove (5) by induction with respect to k. Let k = i0 , then (5) follows directly by the assumptions. Now
assume that (5) is fulfilled for k ≥ i0 . Then


Pr

k+1



Bi

+

i=i0



k +1
−

Pr Ai ∩

= Pr

k+1



Bi


Bj

+ Pr{Ai0 }

j=i0

i=i0 +1



i−1



+ Pr Ak+1 ∩

i=i0

k



Bi

+

i=i0

k
−


Pr Ai ∩

i−1



Bj

+ Pr{Ai0 }

j=i0

i=i0 +1


 



k
k
k
i−1


−


= Pr Ak+1 ∪ Bk+1  Bi Pr
Bi +
Pr Ai ∩
Bj + Pr{Ai0 }
i=i
i=i0
j=i0
i=i0 +1
0


  




k
k
k
i −1


−


≥ Pr Ak+1 ∪ Bk+1  Bi · Pr
Bi +
Pr Ai ∩
Bj + Pr{Ai0 }
i=i
i=i
j =i
i=i +1


0

≥ (1 − p)(1 − q)

k+1−i0

0

0

0

.

4. Branching process lemmas
The results in this section, in its main part, are similar to the results presented in [9]. However we need them in a different
form (instead of the BFS procedure we are going to use a branching process), so some of the techniques used to obtain these
results, as well as constants in the statements have to be modified.
In this section all limits are taken as N → ∞. Also, notation O(·) and o(·) is used with respect to N.
We consider a general branching process B with the number of children of the ith individual distributed according to the
random variable Xi . In fact we look at the branching process B (P ′ , P , N ) with the number of children of the first individual

2002

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

distributed according to the binomial distribution Bin(N , P ′ ) and the number of children of other individuals distributed
according to the binomial distribution Bin(N , P ) (P ′ ≥ P) in which the number of children of an individual does not depend
on the number of children of an other individual. For the sake of further analysis we describe such process as creation of
a random subtree of an infinite directed, labelled tree T rooted in a vertex v (first individual). For the branching process
B (P ′ , P , N ), T is an infinite tree with all vertices with out-degree N and all vertices with in-degree 1, except v which has
in-degree 0. First we take the vertex v and independently, one by one, according to the labelling of T , we add each of its
N out-neighbours (children) with probability P ′ to a subtree. Now they are children of v . Then we take the first child of v
(according to the labelling of T ) and add to a subtree each of its out-neighbours randomly with probability P. Then a second
child of v etc.
By a step of the branching process we mean adding all children of one individual. An individual whose children have been
already added we call saturated. By a substep of a step of the process we mean adding or not adding to a subtree one of the
out-neighbours (in T ) of an individual. The process dies out when there are no more unsaturated vertices in the subtree.
By the ith generation of the process we denote all the individuals of the created subtree, which are at distance i from
v . By gi we denote the number of individuals in the ith generation of B (P ′ , P , N ) (i.e. g0 = 1, g1 has binomial distribution
Bin(N , P ′ ), etc.).
Now we state four lemmas concerning sizes of generations. Some of the constants in the proofs and statements of the
lemmas may be picked differently, but we state them as examples and to show the order of magnitude. In all proofs we set
Pr∗ {·} = Pr{·|gi−1 = g }.

(6)

Lemma 3. Let NP = CN ln N, CN → C , C > 0, i0 ≥ 1, and bi0 > 4/C . Let gi be the size of the ith generation of the branching
process B (P , P ′ , N ). If
gi0 ≥ bi0 ,
then for sufficiently large N with probability 1 − o( N1 )
gi ≥ b(CN ln N )i−i0


where b = bi0 −

4bi

0

for all i ≥ i0 ,

.

C

Proof. For i ≥ i0 + 1 define bi by the recursion

√

3(i − i0 )bi−1
bi = bi−1 − √ √
.
i−1−i0
CN (CN ln N )
We have bi ≤ bi0 for all i ≥ i0 ,

∑∞

k=0

(k + 1)xk =

(7)
1

(1−x)2

C ln N
for all 0 < x < 1, and √C (√(NC ln N )−1)2 → √1 , thus for all i ≥ i0 + 1
C
N
N

and sufficiently large N

√

3(i − i0 )bi−1
bi = bi−1 − √ √
i−1−i0
CN (CN ln N )

= bi0 −

i−1
−

√

√

CN

k=i0


≥ bi0 −
≥ bi0 −

√
k−i
(CN ln N ) 0

i−1−i
3bi0 −0

CN



3(k + 1 − i0 )bk

3bi0

k=0

√

k+1
√
k
(CN ln N )
CN ln N

CN ( (CN ln N ) − 1)2


≥ bi0 −

4bi0
C

= b.
Assume now that for a given i ≥ i0 + 1, gi−1 = g ≥ bi−1 (CN ln N )i−i0 −1 . Then
E = E(gi |gi−1 = g ) ≥ bi−1 (CN ln N )i−i0 .
From the recursion (7) we have

(bi−1 − bi )2
bi−1

(CN ln N )i−i0 = 3(i − i0 ) ln N .

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2003

Therefore by Chernoff’s bound (3)
Pr∗ gi ≤ bi (CN ln N )

i−i0







≤ Pr∗ gi ≤



bi
bi−1

E



(bi−1 − bi )
= Pr∗ gi ≤ E −
E
bi−1



(bi−1 − bi )2

≤ exp −

2b2i−1

(bi−1 − bi )2


≤ exp −

2bi−1


E

(CN ln N )

i−i0





3
≤ exp − (i − i0 ) ln N
2



1

=

i−i0

3

N2

,

(8)

where Pr∗ is defined as in (6).
Define Ai as the event that gi ≥ b(CN ln N )i−i0 and A′i as the event that gi ≥ bi (CN ln N )i−i0 . Let ωj , j ≥ 0, be events of the
form: {gj = g }. By definition of the Markov chain we have:
Pr{ωi |ωi−1 , ωi−2 , . . . , ωi0 } = Pr{ωi |ωi−1 }.
After summing over all ωi ∈ A′i we get
Pr{A′i |ωi−1 , ωi−2 , . . . , ωi0 } = Pr{A′i |ωi−1 }.
Thus
Pr{A′i ∩ {ωi−1 , ωi−2 , . . . , ωi0 }}
Pr{A′i ∩ {ωi−1 }}

=

Pr{ωi−1 , ωi−2 , . . . , ωi0 }

After summing over all {ωi−2 , . . . , ωi0 } ∈



′



Ai ∩ {ωi−1 } ∩

i−2

j=i0





i −2

′

Pr{ωi−1 }

{ωi−1 } ∩

Aj

j=i0


′

Aj

j=i0

=

Pr{A′i ∩ {ωi−1 }}

A′j we get

i−2



.

.

Pr{ωi−1 }

Therefore by the above equality and (8) for all ωi−1 ∈ A′i−1

 


i−i0
i−2


1
′
′
Ai {ωi−1 } ∩
Aj = Pr{A′i |ωi−1 } ≥ 1 −
.
3

N2
j =i
0

Thus we get that for any K > i0



K


′

Ai



i=i′0


−

=

Pr AK ∩ {ωK −1 } ∩

ωK −1 ∈A′K −1





K −2

′



′

Ai

i=i0


 

K
−2
K
−2




′
′
Pr AK {ωK −1 } ∩
Ai Pr {ωK −1 } ∩
Ai

i=i
i=i


−

=

′

ωK −1 ∈A′K −1


≥


1−

0

1
N

K −i0 



K −1

Pr

3
2





A′i

i=i0

≥ ···
≥

K
∏
i=i0 +1




1−

1
3

N2

i−i0 

Pr{A′i0 }

0

2004

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019


≥


i−i0 
K
−
1

1−

3

N2

i=i0 +1


≥

3

1−



N2

1
3

Pr{A′i0 }

Pr{A′i0 }.

3

N2 N2 −1

Letting K → ∞ (independently of N) we get

 
 

 
3
∞



N2
1
1
 ′
′ ′
=1−o
.
Pr
Ai  Ai0 ≥ Pr
Ai  Ai0 ≥ 1 − 3 3


N
N2 N2 −1
i=i0 +1
i=i0 +1


∞


Lemma 4. Let i0 ≥ 1 and gi be the size of the ith generation of the branching process B (P , P ′ , N ). Moreover let
(i) NP = CN → C > 1,

√

(ii) bi be defined by the recursion: bi0 = √ 4C

, b i = b i −1 − 
2

( C −1 )

3bi−1
i−i
CN 0

for all i ≥ i0 + 1.

Then for sufficiently large N
bi ≥

√

C

for all i ≥ i0

2( C − 1)2

and
i−i0

Pr{gi ≤ bi CN

i−i0 −1

ln N | gi−1 ≥ bi−1 CN

1

ln N } ≤

for all i > i0 .

3

N2

√

1
Proof. We have bi ≤ bi0 for all i ≥ i0 , 1−√
→ 1−1√C , and 4 − 2 3 >
CN

1
,
2

√

3bi−1
bi = bi−1 − √ i−i
0
CN
i−i0 −1

≥ bi0 −



3bi0

−

1

√
CN

j=0

≥ bi0 −



3bi0 √

j +1

1
CN − 1

√ √
2 3 C

4C

= √
− √
√
( C − 1)2
( C − 1)( CN − 1)
≥

C

.
√
2( C − 1)2
i−i0 −1

If we assume that for a given i ≥ i0 + 1, gi−1 = g ≥ bi−1 CN
i−i0

E = E(gi |gi−1 = g ) ≥ bi−1 CN

ln N, then

ln N .

By Chernoff’s bound (3),
i−i0

Pr∗ {gi ≤ bi CN


ln N } ≤ Pr∗ gi ≤


≤ exp −




bi
bi−1

E

(bi−1 − bi )2
2bi−1
3

i−i0

CN


ln N



≤ exp − ln N
2

=

1
3

N2

.
i−i0 −1

The result follows after summing over all possible g, g ≥ bi−1 CN

ln N.

thus for sufficiently large N, for all i ≥ i0 + 1

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2005

Lemma 5. Let NP = CN ln N, CN → C , C > 0, NP ′ = CN′ ln N, CN′ → C ′ , C ′ > 0, and gi be the size of the ith generation of the
branching process B (P , P ′ , N ). Then with probability 1 − o( N1 ) for sufficiently large N
gi ≤ b

CN′
CN

where b = 4 +

(CN ln N )i for all i ≥ 1,
2
.
C′

√

Proof. Let b0 = 1 and b1 =

12CN′ +1+2CN′ +1
.
2C ′
N

We know that g0 = 1 and Eg1 = CN′ ln N, thus by Chernoff’s bound (4)

Pr{g1 ≥ b1 CN′ ln N } = Pr{g1 ≥ CN′ ln N + (b1 − 1)CN′ ln N }



3 (b1 − 1)2 ′
≤ exp −
CN ln N
2 (2 + b1 )
1

=

.

3

N2

Now, for i ≥ 2 define sequence bi by the recursion
bi = bi−1 +

√


√

2 b i −1
CN′

i−1

√
CN ln N

i −1

.

Then obviously

(bi − bi−1 )2 =

4bi−1
CN′

i−1
(CN ln N )i−1

and

√
bi − bi−1
3bi−1

= 

i−1

2
′

3 bi−1 CN

√
CN ln N

i −1

< 

2
′

3 CN CN ln N

= o(1),

for sufficiently large N and o(1) is uniform over all i ≥ 2.
C′

Assume now that for a given i ≥ 2, gi−1 = g ≤ bi−1 CN (CN ln N )i−1 . Then
N

′

E = E(gi |gi−1 = g ) ≤ bi−1

CN
CN

(CN ln N )i .

This and Chernoff’s bound (4) imply that,


Pr∗ gi ≥ bi

CN′
CN

(CN ln N )i





C′
C′
= Pr∗ gi ≥ bi−1 N (CN ln N )i + (bi − bi−1 ) N (CN ln N )i
CN
CN


CN′
≤ Pr∗ gi ≥ E + (bi − bi−1 ) (CN ln N )i
CN


′ 2
2 (CN )
2i
(
C
ln
N
)
3(bi − bi−1 )
N
CN2



≤ exp − 
CN′
i
2 3E + (bi − bi−1 ) C (CN ln N )
N


(C ′ )2
3(bi − bi−1 )2 N2 (CN ln N )2i
CN



≤ exp − 
CN′
CN′
i
i
2 3bi−1 C (CN ln N ) + (bi − bi−1 ) C (CN ln N )
N
N
 


2 ′
bi − bi−1 (bi − bi−1 ) CN
≤ exp − 1 −
(CN ln N )i
3bi−1
2bi−1
CN
 


≤ exp − 1 − 

2

3 CN2 ln N


≤

1
3

N2
for sufficiently large N.

i−1

,

 2(i − 1) ln N 

2006

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

Finally we prove inductively that, for sufficiently large N, bi ≤ b for all i. Notice that b0 < b, b1 < 4 +
sufficiently large N we have 4 +

1
CN′

√

bj < b for all 1 ≤ j < i. Since b1 < 4 +

√


, and for

1
CN′

C ln N
2 b
√ N
2
CN′ CN ln N ( CN ln N −1)

, CN′ → C ′ , and √

= o(1) we obtain

√
i−1

2 bi−1

bi = bi−1 +

1
′

CN

< b (it follows from inequality ( 12CN′ + 1 − 1)2 > 0 and CN′ → C ′ ). Assume now that

√

′

CN

i−1

CN ln N

√
i −1
−
2 bj
j
 ′ √


= b1 +

CN

j=1

√

i−1
−

2 b

≤ b1 + 

CN′ CN ln N j=1

j

√
CN ln N

√

≤ 4+
< 4+

1
CN′

j

CN ln N

2 b

+

j −1

CN ln N

√

CN′ CN ln N ( CN ln N − 1)2

2
C′

for sufficiently large N.
Define for i ≥ 0, Ai as the event that gi ≤ b(CN ln N )i , A′i as the event that gi ≤ bi (CN ln N )i , and ωi to be an event of the
form {gi = g }. Then by the above inequalities
Pr{A′1 } ≥ 1 −

1
3

N2

and using the same reasoning as in the proof of Lemma 3 we get that for any ωi−1 ∈ A′i−1

 

i−1

i −2


1
′
′
Aj = Pr{A′i |ωi−1 } ≥ 1 −
Ai {ωi−1 } ∩
.
3

N2
j =i
0

Thus using the same reasoning as this in the proof of Lemma 3 we get that for any K > 0
Pr



K

Ai

≥ Pr



K

A′i

i=1

i =1

≥

 
K
∏



1

1−

i−1  
1−

3

N2

i =2

1

≥ 1−

3

−

3

N2

1

1

N2

−

3

3

3

N2 N2 −1

N2

 
= 1−o

3

i−1
∞ 
−
1
i =2

3



N2

N2

= 1−

1

1

N

.

Therefore the result follows by letting K → ∞ (independently of N).
Lemma 6. Let NP = CN (CN → C > 1), NP ′ = CN′ (CN′ → C ′ > 1), and gi be the size of the ith generation of the branching
process B (P , P ′ , N ). Then with probability 1 − o( N1 ) for sufficiently large N
gi ≤ i2 CN′ CNi−1 ln N

for all i ≥ 1.

Proof. Let g0 = 1 and bi = i2 for all i ≥ 1. Then Eg1 = CN′ and by Chernoff’s bound (4)





Pr g1 ≥ CN′ ln N = Pr g1 ≥ CN′ + CN′ (ln N − 1)


3 (ln N − 1)2 ′
≤ exp −
CN
2 (ln N + 2)

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

3 (ln N − 1)2
CN′ ln N
= exp −
2 ln N (ln N + 2)
 
1
=o
.



2007



N

Assume now that, for i ≥ 2, gi−1 = g ≤ bi−1 CN′ CNi−2 ln N, then E = E(gi |gi−1 = g ) ≤ bi−1 CN′ CNi−1 ln N. Thus by Chernoff’s
inequality (4) and the fact that for i ≥ 2
3 (bi − bi−1 )2
2 (bi + 2bi−1 )

=

3 (4i2 − 4i + 1)
2 (3i2 − 4i + 2)

>

3
2

,

we have
Pr∗ {gi ≥ bi CN′ CNi−1 ln N } = Pr∗ {gi ≥ bi−1 CN′ CNi−1 ln N + (bi − bi−1 )CN′ CNi−1 ln N }

≤ Pr∗ {gi ≥ E + (bi − bi−1 )CN′ CNi−1 ln N }


3 (bi − bi−1 )2 (CN′ )2 CN2i−2 ln2 N
≤ exp −
2 (3E + (bi − bi−1 )CN′ CNi−1 ln N )


3 (bi − bi−1 )2 ′ i−1
≤ exp −
CN CN ln N
2 (bi + 2bi−1 )


1

≤

CN′ CNi−1

3

N2

.

Define for i ≥ 1, Ai as the event that gi ≤ bi CN′ CNi−1 ln N. By previous results

 
Pr {A1 } = 1 − o

1

N

and using the same properties of Markov chains as in Lemma 3 for any ωi−1 ∈ Ai−1


 

CN′ CNi−1
i−2


1

Aj = Pr{Ai |ωi−1 } ≥ 1 −
Pr Ai {ωi−1 } ∩
.
3

N2
j =1
Thus using the same reasoning as this presented in Lemmas 3 and 5 we get that for any K > 0
Pr



K

Ai

 
≥ 1−o

i=1

1

N

−

CN′ CNi−1
K 
−
1
3

i =2

∞
1 −

 
≥ 1−o

1

N

 
= 1−o

1

N

N2

−

3

N 2 i=2



1

CN′ CNi−1 −1

3

N2

,

since CN′ , CN > 1 and for sufficiently large N a series
letting K → ∞ (independently of N).

∑∞

i=2

(

1
3

′

i−1

)CN CN

−1

is convergent. Therefore the lemma follows by

N2

Lemma 7. Let NP = CN → C > 1, bN be such that bN / ln N → ∞ and gk be the size of the ith generation of the branching
process B (P , P ′ , N ). Then with probability 1 − o( N1 ) for sufficiently large N, if for a given k ≥ 1
gk−1 ≤ bN ,
then
gk ≤ 2CN bN .

2008

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

Proof. If gk−1 = g ≤ bN , then E = E(gk |gk−1 = g ) ≤ CN bN . Thus from Chernoff’s inequality (4)
Pr∗ {gk ≥ 2CN bN } ≤ Pr∗ {gk ≥ E + CN bN }



3
≤ exp − CN bN
8
 
1
=o
.
N

The Lemma follows after summing over all possible g ≤ bN .
5. BFS in G (n, m, d )
For the sake of further analysis we describe a BFS procedure starting at some vertex v as creation of a subtree of an
infinite, directed, labelled tree T similar to the one described in the previous section. Set an ordering of the vertices from V .
The root of T is a copy of the vertex v . The set of out-neighbours of the root is a copy of V . Each of those out-neighbours has
the set of out-neighbours being a copy of V and so on. The labelling of T corresponds to the ordering of V .
Now consider a BFS procedure in G(n, m, d) starting at a given vertex v ∈ V . We describe this procedure as the creation
of a random subtree of T which is isomorphic to the BFS-tree rooted in v = v1 in G(n, m, d).
First we add to the subtree of T the root (a copy of the vertex v = v1 ). Then, in the first substep, we take the first (according
to the ordering of V ) out-neighbour of the root in T and add it to the subtree if and only if it is a copy of a neighbour of v
in G(n, m, d). Then we proceed according to the ordering of V with the following substeps in the same way for all the outneighbours of the root in T . After n substeps (the first step) we add v to the set of saturated vertices. Then in the next step we
take the vertex u added to the subtree as the second one (denote the copy of the vertex u in V by v2 , v2 ∈ V ). In the n substeps
of this step we add one by one its out-neighbours in T to the subtree if they are a copy of a neighbour of v2 in G(n, m, d) and
their copies have not been added to the subtree before. Finally, after the step, we add the considered vertex to the saturated
vertices. Then we take the third vertex added to the subtree and so on. We proceed until there are no unsaturated vertices
in the subtree.
Assume that we proceed only so far that the BFS-tree has still less than nmax = n/(ln n)4/3 vertices. After t (t ≥ 0) steps
and t ′ substeps of the step t + 1 we have t saturated vertices, s unsaturated vertices and an ordered set of the copies of the
vertices of the BFS-tree discovered so far VBFS = {v1 , . . . , vt , vt +1 , . . . , vt +s } (labels given by the order of adding). By our
assumption we have that t + s ≤ nmax , since otherwise we would have stopped the procedure. Now set

|B| = 0,

B = Bt = ∅,

for t = 0;

B = Bt = D({v1 , . . . , vt }),

|B| ≤ dt , for t > 0;
|A| ≤ ds, for t ≥ 0,

(9)

A = At = D({vt +1 , . . . , vt +s }) \ Bt ,

where D(S ) is defined as in (1). Moreover set for t ≥ 0
D∗ (vt +1 ) = D(vt +1 ) \ Bt .

(10)

Thena copy of the next
 vertex

from V \ VBFS (according to the ordering of the set V ) is added to the BFS-tree with probability
1−

m−|B|−|D∗ (vt +1 )|
d


1−

≤

/

m−|B|
d

m−|B|−|D∗ (vt +1 )|
d



m−|B|
d

 2
d


1+





≤

dnmax

. Since (2), |D∗ (v1 )| ≤ d, |D∗ (vt +1 )| ≤ d − 1 (for t > 0) and |B| ≤ dnmax we obtain
d|D∗ (vt +1 )|
m − |B|


1+

d



2(m − |B| − d)

d

d2 nmax



+
+
= P+′ for t = 0;
m
m
2(m − dnmax − d)
2(m − dnmax )(m − dnmax − d)
 − dnmax

d
d2 nmax

 d(d − 1) 1 + dnmax +

+
= P+ for t > 0.
m
m − dnmax
2(m − dnmax − d)
2(m − dnmax )(m − dnmax − d)

(11)

Note that at each step of the procedure |V \ VBFS | ≤ n.
Now we consider a subtree of the BFS-tree. Namely we are interested in the subtree of the BFS-tree created by the root
and the vertices, which add d − 1 new features to D(VBFS ) and form with the root a connected component. As before, assume
that we are after t steps (t ≥ 0) and t ′ substeps of the step t + 1 and we have t saturated vertices, s unsaturated vertices and
the vertex set of the BFS-tree VBFS = {v1 , . . . , vt , vt +1 , . . . , vt +s }. Moreover assume that vt +1 is contained in the considered
subtree of the BFS-tree. Now we check the next (according to ordering of V ) vertex v ′ , whether it is added to the BFS-tree
and also added to the subtree. More formally, the vertex v ′ is added to the BFS-tree, if it is a neighbour of vt +1 . In addition it
is added to the subtree if

|D∗ (v ′ )| = |D(v ′ ) \ (D({v1 , . . . , vs+t }))| = d − 1.

(12)

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2009

∗
Since |D∗ (v1 )| = d and, for t > 0,
t +1 hasbeen previously added to the subtree) the probability that
 |D∗ (vt +1)|= d − 1(v
|
D
(vt +1 )|
m−|B∪A|
B|
′
v is added to the subtree equals
/ m−|
. Since |A| + |B| ≤ dnmax and (2) we have
1
d−1
d



|D∗ (vt +1 )|



1



m−|B|
d

m−|B∪A|
d−1







m−|B∪A|
d−1
|D∗ (vt +1 )|d


=
(m − |B| − d + 1) m−|B|
d−1

|D (vt +1 )|d


(d − 1)|A|
(d − 1)2 |A|
≥
· 1−
−
m
m − |B|
2(m − |B|)(m − |B| − d + 1)

 2
d3 nmax
d2 nmax
d


−
1
−
= P−′ , for t = 0;

m
m − dnmax
2(m − dnmax )(m − dnmax − d)
≥



d2 nmax
d3 nmax
(d − 1)d


1−
−
= P− , for t > 0.
m
m − dnmax
2(m − dnmax )(m − dnmax − d)
∗



(13)

Note that at each step of the procedure |V \ VBFS | ≥ n − nmax .
1

Notice that we are interested in the case d2 n/m = O(ln n) ( i.e. d2 nmax = O(mnmax ln n/n) = O(m/(ln n) 3 ) = o(m)).
Thus it follows from (11), (13), that until we have not discovered more than nmax vertices, a general BFS procedure may
′
be approximated by two branching processes B+ (P+
, P+ , N+ ) and B− (P−′ , P− , N− ) with the number of children of an
′
individual distributed according to the binomial distributions Bin(N+ , P+
), Bin(N+ , P+ ) and Bin(N− , P−′ ), Bin(N− , P− ),
respectively, where


N+ = n,





2


P ′ = d 1 + O dnmax
,
+
m
m





dnmax
d(d − 1)


P+ =
1+O
,
m

m


N− = n − nmax ,


 2


2


P ′ = d 1 − O d nmax
,
−

(14)

m

m

(15)


 2




d(d − 1)
d nmax

P− =
1−O
.
m

m

Namely we can couple our BFS procedure by B− from below and by B+ from above.
Let us introduce some new notation. Let gi+ and gi− be the sizes of the ith generation of the branching processes B + and
−
B , respectively. Recall that d(u, v) = k means that a shortest path from u ∈ V to v ∈ V has length k. Let us denote for
v ∈ V and k ≥ 0

Γk (v) := {u ∈ V : d(v, u) = k},
k

Nk (v) :=
Γi (v).
i=0

We can couple substeps of the BFS procedure by the substeps of the branching processes in such a way that
gk− ≤ |Γk (v)| ≤ gk+ ;
k
−

gi− ≤ |Nk (v)| ≤

i =0

k
−

(16)

gi+ ;

i =0

with probability one as far as |Nk (v)| ≤ nmax .
We will be also interested in the number of vertices in each Γi (v) which are not necessarily contained in the above
mentioned subtree but anyway add d − 1 new features to D(VBFS ). The following lemma shows that under some conditions
with high probability the number of those vertices in Γi (v) does not vary much from |Γi (v)|.
Lemma 8. Let b = o( nlnmax
) and
n

d2 n
m

= O(ln n). If i0 is the smallest index such that

|Γi0 (v)| ≥ b
and i0 = O(ln n), then with probability 1 − o(1/n) for large n

|{v ′ ∈ Γi0 (v) : |D∗ (v ′ )| = d − 1}| ≥ b′ ,

2010

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

where

b′ =



b − 1

 √

4
n
for b = o √
;


2b

for

3

ln n

b

→ ∞.

ln n

Proof. First we estimate probability that v ′ has added d − 1 vertices to D(VBFS ) conditioned on the event that v ′ ∈ Γi0 (v)
(i0 = O(ln n)), v ′ has been added to the BFS-tree as one of the first b vertices from Γi0 (v), and Γi (v) < b for all 1 ≤ i ≤ i0 .
Assume that we are after t steps and t ′ substeps of the BFS procedure, vt +1 ∈ Γi0 −1 (v), and we have already added to the
BFS-tree j < b vertices from Γi0 (v). Let v ′ be the vertex from V \ VBFS considered in the next substep. Define A, B, D∗ (vt +1 ),
and D∗ (v ′ ) as in (9), (10), and (12). Then by the assumptions

|B| < di0 b;
|A| < 2db.
The probability that v ′ is added to the BFS-tree in this substep is at most





|D∗ (vt +1 )|
1



m−|B|
d

m−|B|
d−1


,



since
subsets of W with at least one feature in D∗ (vt +1 ) and no feature in B is bounded from above
 the number
  of d-element

|D∗ (vt +1 )|

by

(we may choose one feature from D∗ (vt +1 ) on |D∗ (vt +1 )| ways and then d − 1 features form W \ B,

m−|B|
d−1

1

however some sets we would count several times and some of them would have d − 1 elements). Moreover, the probability
that v ′ is added to the BFS-tree in this substep and |D∗ (v ′ )| = d − 1 equals





|D∗ (vt +1 )|
1



m−|B|−|A|
d−1

m−|B|
d




.

Therefore the probability that, conditioned on the event that v ′ has been added to the BFS-tree in this substep, |D∗ (v ′ )| =
d − 1 is at least



m−|B|−|A|
d−1



m−|B|
d−1





=

d−2 
∏

1−

l =0

> 1−
> 1−

|A|
m − |B| − l



d|A|
m − |B| − d
2d2 b
m − di0 b − d

= 1−ξ
and

ξ=

2d2 b
m − di0 b − d


=O

d2 b
m




=O

b ln n
n



.

Assume now that |Γi0 (v)| ≥ b, i0 = O(ln n), and |Γi (v)| < b, for all i < i0 . Let X be a random variable with the
binomial distribution Bin(b, 1 − ξ ). By the above considerations, for the first b vertices added to Γi0 (v), 1 − ξ bounds
from below the probability that v ′ , which is added to Γi0 (v), has D∗ (v ′ ) = d − 1. Therefore the random variable which
equals to |{v ′ ∈ Γi0 (v) : |D∗ (v ′ )| = d − 1}| stochastically dominates X and in order to prove the thesis it is enough
to prove that with probability 1 − o(1/n) we have X ≥ b′ . This would imply that with probability 1 − o(1/n) we have
|{v ′ ∈ Γi0 (v) : |D∗ (v ′ )| = d − 1}| ≥ b′ .
√
4

Let b = o( √

), then if we set b′ = b − 1


b
′
Pr{X ≥ b } =
ξ (1 − ξ )b−1 + (1 − ξ )b
b−1
≥ bξ (1 − (b − 1)ξ ) + 1 − bξ
= 1 − b(b − 1)ξ 2
n

ln n

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019


= 1−O

b4 ln2 n

2011



n2

 
1

= 1−o

n

.

In the case lnbn → ∞ equality Pr{X ≥ b′ } = 1 − o
variable X .

1
n

follows by a simple application of Chernoff’s bound (3) for the random

In the next lemmas we use following considerations concerning the BFS procedure described above.
Remark 1. Let P ∈ (0; 1], λ be a positive integer, T be an infinite tree described above with out-degrees N and N− be such
that N− ≤ N − λ. Consider a BFS procedure in a random graph G(n, m, d) starting at a vertex v . Take the ith substep of the
substeps in which we add or not a vertex, a copy of which has not been added to the subtree of T (BFS-tree) before (in the
following considerations we will count only those substeps). Assume that, as long as the BFS-tree has less then λ vertices,
for any such a substep, the probability that w is added to the subtree is at least P. Then, as long as the BFS-tree has less then
λ vertices, the number of vertices added to the subtree in the ith substep may be coupled from below by a Bernoulli random
∑kN−
variable Xi with the success probability P. Let now X =
i=1 Xi , i.e. X is a random variable with the binomial distribution
Bin(kN− , P ). Let moreover λ = b + k − 1. By the above described coupling argument, if
X ≥ b + k − 1 = λ,
then only two possible events could occur: the first that in the first kN− substeps at least λ vertices has been added to the
BFS-tree and the second that the BFS procedure has stopped before the kth step (i.e. in at most k − 1 steps). Note that, since
k(N − λ) ≤ kN− ≤ kN, the first event implies that at least b = b + k − 1 vertices has been added in the first k steps. At most
k − 1 of them may be unsaturated, therefore this implies that there exists k0 ≤ k such that after k0 th step there are at least
b unsaturated vertices. Thus
Pr{X ≥ b + k − 1 = λ} ≤ Pr{A},
where A is the event that either the BFS procedure has stopped in at most k − 1 steps or there is k0 ≤ k such that after k0 th
step there are at least b unsaturated vertices.
Now we take a closer look at the first steps of the BSF procedure. The following two lemmas show that the BFS procedure
in G(n, m, d) either stops or spreads fast.
Lemma 9. Let v ∈ V , C > 0, b > 0 be any arbitrarily chosen constant, and n, m, d be as in Theorem 3(i). If for P− and N−
defined as in (15) we have P− N− / ln N− → C , then with probability 1 − o(1/n) the BFS procedure in G(n, m, d) starting at the
vertex v either stops in at most ⌊ C1 ⌋ steps or there is 1 ≤ i0 ≤ ⌊ C1 ⌋ + 1, such that

|Γi0 (v)| ≥ b.
Proof. Let N− P− / ln N− = CN , λ = λ = 2b + ⌊ C1 ⌋, k = ⌊ C1 ⌋ + 1 and X have the binomial distribution Bin(kN− , P− ).

− ⌊ C1 ⌋ < 1, there exists a constant 0 < ε < 13 such that ⌊ C1 ⌋ + 1 ≥
C
sufficiently large n to have CN ≥ 1 − ε . Thus for sufficiently large n
 

1
CN
+ 1 CN ≥
(1 + 3ε) ≥ (1 − ε)(1 + 3ε) = 1 + 2ε − 3ε 2 > 1 + ε.
Moreover, since

1
C

C

C

Therefore, for sufficiently large n,
Pr {X ≤ λ} =


λ 
−
kN−
i =0

i

i
P−
(1 − P− )kN− −i


i
λ 
−
kN−
P−
= (1 − P− )
1+
i
1 − P−
i=1

i 
λ 
−
kN− P−
≤ exp(−kP− N− ) 1 +
1 − P−
i =1




kN−


  


1
= exp −
+ 1 CN ln N−
C

kN− P−
1−P−

λ+1

kN− P−
1−P−

−1

−1

1+3ε
.
C

In addition we can choose

2012

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

1

≤

1+ε

N−

O((ln N− )λ )

 
1

=o

n

.

Thus by Remark 1 with probability 1 − o( 1n ) either the BFS procedure stops in at most ⌊ C1 ⌋ steps or there is k0 ≤ ⌊ C1 ⌋ + 1
such that after k0 th step we have at least 2b unsaturated vertices. Assume that the vertex considered in the k0 th step is a
member of the set Γj (v). Of course j ≤ ⌊ C1 ⌋ and all unsaturated vertices are members of Γj (v) or Γj+1 (v). Thus

|Γj (v)| + |Γj+1 (v)| ≥ 2b.
Therefore there exists 1 ≤ i0 ≤ ⌊ C1 ⌋ + 1 (either i0 = j or i0 = j + 1) such that

|Γi0 (v)| ≥ b.
Lemma 10. Let v ∈ V and n, m, d be as in Theorem 3(ii). If for P− and N− defined as in (15) we have P− N− → C > 1, then
with probability 1 − o(1/n) the BFS procedure in G(n, m, d) starting at the vertex v either stops in at most √ 16C2
ln N − 1
( C −1) (C −1)

16C

steps or there is 1 ≤ i0 ≤ √

( C −1)2 (C −1)

ln N, such that

4C

ln N + 1.
|Γi0 (v)| ≥ √
( C − 1)2
Proof. Let P− N− = CN , k = √

16C

( C −1)2 (C −1)

ln N and b = √ 4C

( C −1)2

ln N =

(C −1)k
4

. Let moreover X be defined as in the proof of

Lemma 9. The proof is analogous to that of Lemma 9. The only thing that have to be shown is that with probability o( 1n )
X ≤ k − 1 + 2b + 1 = k − 1 +

(C − 1)k
2

+ 1.

Since then with probability 1 − o( 1n ) either the BFS procedure stops quickly or there exists suitable j such that

|Γj (v)| + |Γj+1 (v)| ≥ 2b + 1
and the lemma follows.
Now notice that

EX = kN− P− = CN k.
By Chernoff’s bound (3), for sufficiently large n


Pr X ≤ k +

(C − 1)k



2



(2CN − C − 1)2
≤ exp −
k
8CN


≤ exp −
 
=o

1
n

9 (2CN − C − 1)2 C
8

(C − 1)2 CN


ln N

.

The above inequalities follow from the facts that
k= √

16C

( C − 1) (C − 1)
2

ln N >

9C

(C − 1)2

ln N

and

(2CN − C − 1)2 C
→ 1.
(C − 1)2 CN
6. The proof of the main theorems
In the previous sections we have gathered auxiliary results. The proofs of the main theorems follow in a sequence of
lemmas and facts.

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2013

6.1. Proof of Theorem 1
Lemma 11. Let d2 n/(m ln n) = cn → c , c > 0. Then there exists n1 = o(nmax ) and k1 ∼ ln n/ ln ln n, such that in G(n, m, d)
Pr{∀v∈V |Nk1 (v)| ≤ n1 } = 1 − o(1)
Proof. Define P+ as in Section 5. By (14)
and N+ P+ = CN+ ln N+ ,

′
N+ P+
= CN′ + ln N+

where


N+ = n,

′

′

CN + → C = c

d−1

and CN+ → C =
c

d

c

for d = O(1),
for d → ∞.

Set




C′

 ln N+ − 32 ln ln N+ − ln 4 + C2 − ln CN+
N+

k1 = 
ln CN+ + ln ln N+





.

Surely k1 ∼ ln n/ ln ln n. By Lemma 5 and (16) for given v ∈ V with probability 1 − o(1/n) we have

|Nk1 (v)| ≤ 1 +


k1
−
CN′ +
CN+
i =1

4+

2



C

(CN+ ln N+ )i = o(nmax ).

(17)

Therefore substituting
n1 = 1 +


k1
−
CN′ +
i =1

4+

CN+

2



C

(CN+ ln N+ )i

finishes the proof.
Lemma 12. Let d2 n/(m ln n) = cn → c . Then with high probability G(n, m, d) consists only of components of size at most ⌊ C1 ⌋,
where



d−1

C =
c

d

for d = O(1),

c

for d → ∞

and at most one component G of size Ω (n/ ln n), such that
diam G ≤ k
for some k ∼ ln n/ ln ln n.
Proof. Let c > 0 be any constant and d2 n/(m ln n) = cn → c . Define P− as in Section 5. By (15)
′
N− P−
= CN′ − ln N− ,

N− P− = CN− ln N− ,

and

where


N− = n − nmax , CN− → C = c
′

′

and

d−1

CN− → C =
c

d

c

for d = O(1),
for d → ∞.

Let moreover

 
3

d

3
2
d−1
b= c

3


c2

and b′ be such that


b = b′ −

4b′
C

.

for d = O(1);
for d → ∞

(18)

2014

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

Let v ∈ V . By Lemma 9 with probability 1 − o(1/n) either v is contained in the component of size at most ⌊ C1 ⌋ or

∃1≤i

 

1
0≤ C

+1

|Γi0 (v)| ≥ b′ + 1.

(19)

Let V ∗ be the set of those vertices in V for which (19) is fulfilled. Let also for each v ∈ V ∗ i0 = i0 (v) and bi0 = b′ . By Lemma 8
we know that if |Γi0 (v)| ≥ b′ + 1 (i.e. if v ∈ V ∗ ), then with probability 1 − o(1/n) at least bi0 vertices from Γi0 (v) have
|D∗ (·)| = d − 1. Let


k2 =

1 ln N− − ln ln N− + ln b − 2 ln b



ln ln N− + ln CN−

2

 
+

1

C

+ 1.

Notice that k2 ∼ ln n/(2 ln ln n), therefore k2 ≤ k1 for large n. By the above considerations, Lemma 11, (16) and the fact
that k2 ≤ k1 , for large n, with probability 1 − o(1/n), the following steps of the BFS process may be approximated by the
branching process B− with gi−
≥ bi0 . By Lemma 3 and for given v ∈ V ∗ with probability 1 − o(1/n)
0

|Γk2 (v)| ≥ gk−2 ≥ b(CN− ln N− )k2 −i0 ≥ λ,
where


λ = λ(n) =

b

N−
ln N−

.

Therefore
Pr{∀v∈V ∗ |Γk2 (v)| ≥ λ} = 1 − o(1).
Moreover we know that this result is obtained from the branching process approximation of the BFS-subtree, in which each
vertex adds d − 1 new elements to D(VBFS ). Therefore also
Pr{∀v∈V ∗ |D(Γk2 (v)) \ D(Nk2 −1 (v))| ≥ (d − 1)λ} = 1 − o(1).
For any two vertices v, v ′ ∈ V ∗ define
B = B(v, v ′ ) = D(Nk2 −1 (v) ∪ Nk2 −1 (v ′ ));
A = A(v) = D(Γk2 (v)) \ D(Nk2 −1 (v));
A′ = A′ (v ′ ) = D(Γk2 (v ′ )) \ D(Nk2 −1 (v ′ )).
By the above considerations with high probability for all v, v ′ ∈ V ∗

|B| + |A| + |A′ | ≤ 2dnmax ,

|A| ≥ (d − 1)λ,

|A′ | ≥ (d − 1)λ.

By Lemma 11

|V \ (Nk2 (v) ∪ Nk2 (v ′ ))| ≥ n − 2nmax .
Thus using (2) and the independence of choices of the feature sets, either BFS-trees started at v and v ′ (v, v ′ ∈ V ∗ ) have
joined before exploring vertices at distance k2 + 1 or





Pr ∀w∈V \(Nk (v)∪Nk (v ′ )) D(w) ∩ A = ∅ or D(w) ∩ A′ = ∅
2
2




≤ 1 −



|A|

| A′ |

1



1



m−|B|−|A|−|A′ |
d−2

m−|B|
d



 n−2nmax




λ (d − 1) dn
≤ exp −
(m − |B| − d + 1)(m − |B| − d + 3)
2

3



m−|B|−|A|−|A′ |
d−2



m−|B|
d−2

 2



λ (d − 1)3 d4 n2
nmax
dnmax
1
−
O
≤ exp −
+
d3 n
m2
n
m
 
1
= exp(−3(1 + o(1)) ln n) = o 2 ,
n

since


λ = (1 + o(1))

3
c2



d
d−1

3

n
ln n

.





1−

2nmax
n





K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2015

Therefore, with high probability the vertices from V ∗ form one connected component. Let G the graph induced on V ∗ , since
with high probability for any two vertices v, v ′ ∈ V ∗ , v and v ′ are joined by a path of length at most 2k2 + 2,
diam G ≤ k,

(20)

where k = 2k2 + 2 ∼ ln n/ ln ln n.
Lemma 13. Let
d2 n

− ln n = bn

m

and Xn be the number of isolated vertices in G(n, m, d)
(i) If bn → −∞, then with high probability the number of isolated vertices in G(n, m, d) tends to infinity.
(ii) If bn → b, for some constant b, then Xn tends in distribution to the random variable with the Poisson distribution Po(e−b ).
(iii) If bn → ∞, then with high probability there are no isolated vertices in G(n, m, d).
Proof. The lemma follows immediately from the proof from [12], where it was shown using the method of moments that
the number of isolated vertices in G(n, m, d) tends in distribution to the Poisson distribution.
d2 n
m

Fact 1. Let d = 2,

− ln n = bn , bn = Ω (1), then with high probability there are no isolated edges in G(n, m, d).

Proof. For d = 2 the probability that there exists an isolated edge in G(n, m, d) is at most

  m−2  n−2


n

 1  2 
m
m

2

≤

2

n

2



1



m 1 −

2

2

  m−3  n−2 
2
2(m − 2)

+ m  m  
2

2(n−2)

2

+

m

2

2(m − 2)

m
2


1−

3

2(n−2) 

m

2



 2


n
4n
n
6n
=O
exp
−
+
O
exp
−
m2
m
m
m

 2


 2
n
3
3
n
exp (− ln n − bn ) + O
exp − ln n − bn
=O
m2
m
2
2
√


 n 
n
=O
+O
m2
m




2
ln n
(ln n)
+O √
=O


n

n

= o(1).
Proof of Theorem 1. By hypothesis of the theorem


d=

m(ln n + bn )
n

,

(i) Follows by Lemma 13(i).
(ii) and (iii) Assume that bn = o(ln n). Notice that
d2 n
m ln n

= cn → c ,

where c = 1. By Lemma 12 with high probability G(n, m, d) consists of one large component and components of size at
most ⌊ C1 ⌋. By definition

 
1

C


=

d

1

d−1c




=

2
1

for d = 2;
for d ̸= 2,

therefore by Fact 1, for all d ≥ 2 with high probability G(n, m, d) consists of one component and isolated vertices. The result
follows by Lemma 13.

In case (iii) and bn = Ω (ln n) we can use a simple coupling. We choose d′ = m(ln n + b′n )/n with b′n = o(ln n) and
such that d′ ≤ d. We can construct a coupling of G(n, m, d′ ) and G(n, m, d) such that G(n, m, d′ ) is a subgraph of G(n, m, d)
with probability 1 (for more details on the coupling see the proof of Lemma 3 in [5]). Therefore if G(n, m, d′ ) is with high
probability connected then G(n, m, d) is with high probability connected.

2016

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

6.2. Proof of Theorem 2
Lemma 14. Let d(d − 1)n/m = cn → c, c > 0, then with high
√ probability G(n, m, d) consists of components
√ of size at most
√ 16C
ln(n − nmax ) and at most one component of size Ω ( n ln n). Let G be the component of size Ω ( n ln n), then there
2
( C −1) (C −1)

exists k, k ≍ ln n, such that with high probability
diam G ≤ k.
Proof. Define P− and P+ as in Section 5. By (14) and (15)
′
N+ P+
= CN′ + ,

′
N− P−
= CN′ − ,

N+ P+ = CN+ ,

N− P− = CN− ,

for N+ = n, N− = n − nmax and some
CN+ , CN−
′

′

 d
c
→C = d−1
′

c

for d = O(1);
for d → ∞,

CN+ , CN− → C = c .
Let
b=

√

C

2( C − 1)2
16C

,

t = √
ln N−
( C − 1)2 (C − 1)
and


k4 =

1 ln N− − ln ln N− + 2 ln 2 − ln b
2



ln CN−

[
+

√

16C

( C − 1)2 (C − 1)


ln N−

+ 1.

Let v ∈ V . Consider now the BFS procedure in G(n, m, d) starting at a vertex v .
By Lemma 10 with probability 1 − o(1/n) either v is contained in the component of size at most t − 1 or
4C

∃1≤i0 ≤t |Γi0 (v)| ≥ √
ln N + 1.
( C − 1)2

(21)

Let V ∗ be the set of vertices from V for which (21) is fulfilled and i0 = i0 (v) be the smallest index such that |Γi0 (v)| ≥
ln N + 1.

√ 4C
( C −1)2

In the proceeding part of the proof we show that for any v ∈ V ∗ with probability 1 − o(1/n) there exists k(v) ≤ k4 such
that (22) is fulfilled. Let Γj (v)− to be the set of these vertices from Γj (v), which have added d − 1 elements to D(VBFS ) during
BFS procedure. Now we define events
A0 − ∃i0 ≤t ∀i<i0 |Γi (v)| < 3 N− ln N−

and |Γi0 (v)| ≥ 3 N− ln N− ;

B0 − ∃i0 ≤t ∀i≤i0 |Γi (v)| < 3 N− ln N−

4C
and |Γi0 (v)− | ≥ √
ln N
( C − 1)2







and given the value of i0 for i > 0
Ai − |Γi+i0 (v)| ≥ 3 N− ln N− ;



Bi − |Γi+i0 (v)| < 3 N− ln N−



and

|Γi+i0 (v)− | ≥ bi+i0 CNi − ln N− ,

where bj is defined as in Lemma 4.
By Lemma 8 for v ∈ V ∗ with probability 1 − o(1/n)
4C

|Γi0 (v)− | ≥ √
ln N .
( C − 1)2
Therefore

 
Pr{A0 ∪ B0 } ≥ 1 − p

for some p = o

1
n

.

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

Notice that for j = O(ln n) the event

j

i =0

2017

Bi implies that

 √

|Nj+i0 (v)| ≤ (i0 + j)3 N− ln N− = O ln n n ln n = o(nmax ).


and

|Γj+i0 (v)− | ≥ bj+i0 CNj − ln N− .
j

Therefore under the condition

i =0

Bi the following steps of the BFS procedure may be coupled from below by the steps of
−3

j

−

the branching process B− with gj ≥ bj+i0 CN− . Thus by Lemma 4 for q = N− 2



j


Pr Aj+1 ∪ Bj+1 
B
≥ Pr{gj−+1 ≥ bj+1+i0 CNj+−1 |gj− ≥ bj+i0 CNj − }
i=i i


0

≥ 1 − q.
k′ −i0

Given v ∈ V ∗ , let k′4 be the smallest number such that bk′ CN4−
k′4 ≤ k4 and

k′4 −i0
i

4

√

ln N− > 3 N− ln N− . Obviously by definition of k4 we have

Bi = ∅. Therefore by Lemma 2, since Ai and Bi are disjoint,

Pr{∃k≤k′ |Γi (v)| ≥ 3 N− ln N− } = Pr



 ′
k4


4

Ai ∪

 i =i


k′4


Bi



i=i0

0

′

≥ (1 − p)(1 − q)k4
≥ 1 − p − k′4 q
 
1
= 1−o
.
n

Therefore with probability 1 − o(1/n), for a given vertex v ∈ V ∗ , there exists k(v) ≤ k4 — the smallest index such that


|Γk(v) (v)| ≥ 3 N− ln N− .

(22)

Let v ∈ V be such that there exists k(v) ≤ k4 described by the above equation. By Lemma 7 and approximation of the
following steps by the branching process B+
∗


|Nk(v) (v)| ≤ (k4 + 2CN+ ) · 3 N− ln N− = o(nmax ).
√

Moreover, by Lemma 8 with probability 1 − o(1/n) at least 23 3 N− ln N− vertices from Γk(v) (v) add at least d − 1 vertices
to D(VBFS ), therefore with probability 1 − o(1/n)


|D(Γk(v) (v)) \ D(Nk(v)−1 (v))| ≥ 2(d − 1) N− ln N− .
For any two vertices v, v ′ ∈ V ∗ define
B = B(v, v ′ ) = D(Nk(v)−1 (v) ∪ Nk(v)−1 (v ′ ));
A = A(v) = D(Γk(v) (v)) \ D(Nk(v)−1 (v));
A′ = A′ (v ′ ) = D(Γk(v ′ ) (v ′ )) \ D(Nk(v ′ )−1 (v ′ )).
By the above considerations it follows that with high probability, for all pairs v, v ′ ∈ V ∗

|B| + |A| + |A′ | ≤ 2dnmax ,
|A| ≥ (d − 1)λ,
√
where λ = 2 N− ln N− . Moreover

|A′ | ≥ (d − 1)λ,

|V \ (Nk(v) (v) ∪ Nk(v′ ) (v ′ ))| ≥ n − 2nmax .
Thus using (2), either BFS-trees started at v and v ′ have joined before exploring vertices at distance k(v) and k(v ′ ),
respectively, or
Pr{∀w∈V \(Nk(v) (v)∪Nk(v′ ) (v ′ )) D(w) ∩ A = ∅ or D(w) ∩ A′ = ∅}


≤ 1 −



| A|
1



| A′ |



1



m−|B|−|A|−|A′ |
d−2

m−|B|
d



 n−2nmax


2018

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

 2



λ (d − 1) d2 (d − 1)2 n2
nmax
dnmax
≤ exp −
1
−
O
+
dn
m2
n
m


d−1 2
= exp −4
c (1 + o(1)) ln n
d
 
1
=o 2 ,
n

since

λ2 = (1 + o(1))4n ln n and c > 1.
Therefore, we have that with high probability any two vertices from V ∗ are joined by a path of length at most 2k4 + 2.
Thus, if G is a component induced on V ∗
diam G ≤ 2k4 + 2.

(23)

Moreover, as it has been pointed out before, with high √
probability each vertex is either in the component of size O(ln n) or
is contained in V ∗ and V ∗ is one component of size Ω ( n ln n). This finishes the proof, since 2k4 + 2 ≍ ln n.
Lemma 15. Let d(d − 1)n/m = cn → c Then there exists n1 = o(n) and k ≍ ln n, such that in G(n, m, d)
Pr{∀v∈V |Nk (v)| ≤ n1 } = 1 − o(1)
Proof. Define P+ , N+ and CN+ as in the proof of Lemma 14. Let


k3 =

9
2

ln N+ −

ln ln N+


.

ln CN+

By Lemma 6 and (16) for given v ∈ V with probability 1 − o(1/n)

|Nk1 (v)| ≤ 1 +

k3
−
CN′ +
i =1

CN+



k

k3
−

ix

i−1

xk3 +1 − 1

′

i2 CNi + ln N+ = O k23 CN3+ ln N+



= o(nmax ).

The estimates above follow from the fact that
k3
−


2 i

i x = x

2

i=1

k3
−


i(i − 1)x

i−2


+x

i=2

= x2



xk3 +1 − 1
x−1



i=2

′′


+x

x−1

.

Proof of Theorem 2. (i) This part of the theorem follows from the coupling with B+ defined as in Section 5 (obviously
coupling is also valid for c < 1) and the analogous proof as in Section 6 in [15].
(ii) √
By Lemma 14 with high probability the graph consist of components of size O(ln n) and at most one component of size
Ω ( n ln n). Then using coupling with B− and B+ and estimating both the expected value and the variance of the number of
vertices in the small components, as in [15], we get that the giant component consists of a constant fraction of vertices.
6.3. Proof of Theorem 3
Proof of Theorem 3. The upper bound on the diameter of the largest component follows by Lemmas 12 and 14, Theorems 1
and 2. In order to obtain the lower bound in case (i) we have to combine Theorem 2 and Lemma 11. By Theorem 2, under
the assumptions of (i), with high probability the largest component in G(n, m, d) is of order Ω (n). Moreover by Lemma 11
for some k ∼ ln n/ ln ln n with high probability for all v ∈ V we have Nk (v) = o(n). Thus with high probability the
giant component in G(n, m, d) is of order Ω (n) and Nk (v) = o(n) for any v from the giant component. Therefore with
high probability there exist v, v ′ in the giant component at distance at least k (since there exists v ′ in the giant component
such that v ′ ̸∈ Nk (v)). This implies the lower bound in case (i). The lower bound in case (ii) follows in the similar way by
Theorem 2 and Lemma 15.
Acknowledgements
I would like to thank my adviser professor Jerzy Jaworski for fruitful discussions, his invaluable advice and support.
I acknowledge the support by Ministry of Science and Higher Education, grant NN206 2701 33, 2007–2010.

K. Rybarczyk / Discrete Mathematics 311 (2011) 1998–2019

2019

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]

M. Behrisch, Component evolution in random intersection graphs, The Electronical Journal of Combinatorics 14 (1) (2007) R17.
S.R. Blackburn, S. Gerke, Connectivity of the uniform random intersection graph, Discrete Mathematics 309 (16) (2009) 5130–5140.
M. Bloznelis, Component evolution in general random intersection graphs, SIAM Journal on Discrete Mathematics 24 (2) (2010) 639–654.
M. Bloznelis, The largest component in an inhomogeneous random intersection graph with clustering, The Electronical Journal of Combinatorics 17
(1) (2010) R110.
M. Bloznelis, J. Jaworski, K. Rybarczyk, Component evolution in a secure wireless sensor network, Networks 53 (2009) 19–26.
B. Bollobás, The evolution of random graphs, Transactions of the American Mathematical Society 286 (1984) 257–274.
B. Bollobás, Random Graphs, Academic Press, 1985.
B. Bollobás, S. Janson, O. Riordan, The phase transition in inhomogeneous random graphs, Random Structures and Algorithms 31 (2007) 3–122.
F. Chung, L. Lu, The diameter of sparse random graphs, Advances in Applied Mathematics 26 (2001) 257–279.
C. Efthymiou, P.G. Spirakis, On the existence of Hamiltonian cycles in random intersection graphs, in: Luís Caires, Giuseppe F. Italiano, Luís Monteiro,
Catuscia Palamidessi, Moti Yung (Eds.), ICALP, in: LNCS, vol. 3580, springer, 2005, pp. 690–701.
J.A. Fill, E.R. Scheinerman, K.B. Singer-Cohen, Random intersection graphs when m = ω(n): an equivalence theorem relating the evolution of the
G(n, m, p) and G(n, p) models, Random Structures and Algorithms 16 (2000) 159–176.
E. Godehardt, J. Jaworski, Two models of random intersection graphs for classification, in: Studies in Classification, Data Analysis and Knowledge
Organization, Springer, 2003, pp. 67–81.
E. Godehardt, J. Jaworski, K. Rybarczyk, Random intersection graphs and classification, in: Studies in Classification, Data Analysis and Knowledge
Organization, Springer, 2007, pp. 67–74.
J. Hwang, Y. Kim, Revisiting random key pre-distribution schemes for wireless sensor networks, in: SASN ’04: Proceedings of the 2nd ACM Workshop
on Security of ad hoc and Sensor Networks, ACM, 2004, pp. 43–52.
S. Janson, T. Łuczak, A. Ruciński, Random Graphs, Wiley, 2001.
M. Karoński, E.R. Scheinerman, K.B. Singer-Cohen, On random intersection graphs: the subgraph problem, Combinatorics, Probability and Computing
8 (1999) 131–159.
A.N. Lagerås, M. Lindholm, A note on the component structure in random intersection graphs with tunable clustering, The Electronical Journal of
Combinatorics 15 (1) (2008) N10.
T. Łuczak, Random trees and random graphs, Random Structures and Algorithms 13 (1998) 485–500.
A.M. Makowski, O. Yagan, Zero-one laws for connectivity in random key graphs, arXiv:0908.3644v1.
R. Di Pietro, L.V. Mancini, A. Mei, A. Panconesi, J. Radhakrishnan, How to design connected sensor networks that are provably secure, in: Proceedings
of the 2nd IEEE International Conference on Security and Privacy for Emerging Areas in Communication Networks, SecureComm 2006, 2004.
K. Rybarczyk, Equivalence of a random intersection graph and G(n, p), Random Structures and Algorithms 38 (1–2) (2011) 205–234.
K. Rybarczyk, D. Stark, Poisson approximation of the number of cliques in random intersection graphs, Journal of Applied Probability 47 (2010)
826–840.
K.B. Singer, Random intersection graphs, Ph.D. Thesis, The Johns Hopkins University, Department of Mathematical Sciences, 1995.
D. Stark, The vertex degree distribution of random intersection graphs, Random Structures and Algorithms 24 (2004) 249–258.

