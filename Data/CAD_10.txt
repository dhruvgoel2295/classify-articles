Computer-Aided Design 59 (2015) 229–238

Contents lists available at ScienceDirect

Computer-Aided Design
journal homepage: www.elsevier.com/locate/cad

Editing 3D models on smart devices
Yuna Kang a,∗ , Hyungki Kim a , Hiromasa Suzuki b , Soonhung Han a
a

KAIST, Guseong-dong, Daejeon, 305-701, Republic of Korea

b

The University of Tokyo, Komaba 4-6-1, Meguro, Tokyo, 153-8904, Japan

article
Keywords:
Smart device
Feature based
Multi-touch
Gesture
3D modeling

info

abstract
This study proposes a 3D CAD system available on smart devices, which are now a part of everyday life
and which are widely applied in various domains, such as education and robot industry. If an engineer
has a new idea while traveling or on the move, or in the case of collaboration between more than two
engineers, this 3D CAD system allows modeling to be performed in a rapid and simple manner on a smart
device. This 3D CAD system uses the common multi-touch gestures associated with smart devices to keep
the modeling operations simple and easy for users. However, it is difficult to input the precise geometric
information to generate 3D CAD models by such gestures. It is also impractical to provide a full set of
modeling operations on a smart device due to hardware limitations. For this reason, the system excludes
several complicated modeling operations. This work provides a scheme to regenerate a parametric 3D
model on a PC-based CAD system via a macro-parametrics approach by transferring the 3D model created
on a smart device in an editable form to a PC-based CAD system. If fine editing is needed, the user can
perform additional work on a PC after reconstruction. Through the developed system, it is possible to
produce a 3D editable model swiftly and simply in the smart device environment, allowing for reduced
design time while also facilitating collaboration. This paper discusses the first-ever system design of a 3D
CAD system on a smart device, the selection of the modeling operations, the assignment of gestures to
these operations, and use of operation modes. This is followed by an introduction of the implementation
methods, and finally a demonstration of case studies using a prototype system with examples.
© 2013 Elsevier Ltd.
This is an open access article under the CC BY-NC-ND license
(http://creativecommons.org/licenses/by-nc-nd/3.0/).

1. Introduction
In recent years, smart devices in the form of smartphones and
smart pads have become widely available with the development of
networks, the miniaturization of the Central Processing Unit (CPU),
and the advancement of mobile technology [1]. Smart devices are
equipped with several built-in sensors, including cameras, and
allow intuitive inputs via a capacitive touch screen. In addition,
wireless networks are available through various paths, such as WiFi, 3G, Long-Term Evolution (LTE) and Bluetooth. The lightweight
and portable design makes smart devices ideal for use on the
move [2].
Technologies are being extensively developed with the advancement of smart devices. Due to the great convenience and diverse applications offered by these devices, many studies on the
application of smart devices to various fields such as industry, defense, and education, are underway. Active investments are also

∗

Corresponding author. Tel.: +82 42 350 3080; fax: +82 42 861 6080.
E-mail address: balbal86@kaist.ac.kr (Y. Kang).

expected as there are numerous opportunities to apply smart devices to industries.
In the field of CAD (Computer-Aided Design), there has been
some effort to apply recent technology i.e., smart and ubiquitous technology. It is expected that mobile communications, ubiquitous sensing and computing technology, smart reasoning and
agent-based computing, natural interaction techniques and other
such technology will play a part in forming the paradigm of
next-generation CAD/E systems and environments [3]. Thus, various studies of CAD have been done in recent years. Research in
the CAD field includes system architecture studies for humancentered CAD agent systems [4], new CAD interface studies using a
brain–computer interface [5], studies that combine CAD and augmented reality environments [6], as well as the digital signal processing studies for networking and sensing [7].
Smart devices also can be a key component of technology for
the next generation CAD/E systems. In particular, when engineers
use 3D modeling in product design, smart devices can be employed to make 3D models of new design drawings. If an engineer
has a new idea while traveling or when on the move, or in the

http://dx.doi.org/10.1016/j.cad.2013.08.001
0010-4485/© 2013 Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).

230

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

Fig. 1. Sketch-based modeling.

case of collaboration between more than two engineers, smart devices allow modeling to be done in a more rapid, simple, and easy
manner. As such, we propose a 3D CAD system for use on smart
devices.

as STEP or IGES do not save the modeling history. Thus, it is difficult
to edit 3D models in a commercial CAD system after exchanging.
Due to these complex problems, 3D CAD systems for smart devices
cannot be easily developed [2].

1.1. Problem definition

1.2. Target system

There are many commercial CAD systems for the PC environment, but even the CAD companies behind the creation of such systems have only managed to develop 3D model viewers or cookbook
applications in smart devices. Regarding the few CAD modeling applications for smart devices such as AutoCAD WS, only 2D modeling
is supported. Why does not an application that supports 3D modeling exist? From a developer’s perspective, there are several reasons
that make it difficult to create CAD modeling programs for smart
devices.
The first reason is that most PC-based commercial CAD systems
are too heavy. Generally, several convenient functions and engines
are built into commercial CAD systems, including graphic engines
for the gorgeous rendering of 3D models, leading to a large installation capacity and high computational complexity. Of course, there
are several approaches to solving this problem, such as cloud computing or remote control of the network server, but these methods
cannot run on a stand-alone device without a network and are thus
limited to specific environments in which users have access to the
internet.
Second, the input commands of smart devices are mostly limited to touch commands. Recent smart devices have minimal hardware buttons, and the user must touch the screen of the smart
device with a finger to choose menus or to operate the device. Commercial CAD systems have numerous functions, implying that using these many functions on a smart device would require a very
complex UI, a range of menus, or complex definitions of touch commands.
In addition, it is not easy to perform complex or precise tasks on
a smart device due to the small screen and rough position pointing
with touch commands. Real industrial models have complex
geometries, and accurate scales of each part in units of millimeters
or less are needed. Thus, the entire task, including detailed
modeling, cannot be done solely on a smart device. Additional,
detailed corrective work must be done on a PC environment after
finishing the simple and rough work on a smart device. During this
process, another problem can occur. The file format of 3D models
from the smart device must be readable in the CAD program on the
PC. However, the market for smart device applications has been
recently dominated by venture-capital companies and private
developers. If the developer of a CAD application is a commercial
CAD company, the proprietary file format of the commercial CAD
can be used for additional work on the PC. However, if this is not
the case, the CAD file must be translated (exchanged) to another
commercial CAD file or the developer must provide a PC CAD
application in order to edit the model. There are several standard
formats for exchanging 3D models, but most standard formats such

The target of this study is as follows:
1. develop a CAD modeling system for smart devices. Define a
subset of modeling functions for creating light smart-device
applications.
2. use multi-touch commands as input for CAD modeling. Perform
mapping between the functions of the defined subset and
multi-touch gestures. Make additional menus and buttons to
support gesture inputs.
3. save the modeling procedure in the form of a macro file. The
system saves the entire modeling procedure as a file in the ASCII
format, which allows users to modify models on a PC.
Through the developed system, it is possible to produce a 3D editable model swiftly and simply in the smart device environment,
thus reducing the design time while also facilitating collaboration.
In Section 2, several existing CAD modeling studies involving
the use of touch-enabled devices are introduced, and Section 3
describes our method in detail. Section 4 shows the results of the
actual implementation of the proposed method.
2. Related work
Smart devices have only recently been developed, but there are
many studies on pen gesture inputs (one-point touch) as touchenabled devices have been around since the 2000s. These concepts
fall under the concept of sketch-based modeling. There are several
methods of sketch-based modeling; the most efficient among them
can vary according to the shape of the target models, the method of
user interaction, and depending on several limitations [8]. Sketchbased modeling can be divided into two methods (see Fig. 1);
the first is gestural modeling, and the second is reconstructional
modeling [9].
Gestural modeling refers to the process of interpreting sequential strokes as specific modeling functions and creating a model by
a pre-defined method. Reconstructional modeling is the process of
considering an entire set of stroke inputs as a projected image of
a 3D model, and creating a model using geometric regeneration
technology [9]. From a design perspective, reconstructional modeling is intuitive and effective for sketching ideas. However, the
entire set of strokes is interpreted; therefore, it is difficult to recognize complex models correctly and modify a model because the
modeling process is not saved. On the other hand, in gestural modeling, the user can understand the modeling process and modify
models by repeating the input gestures. Therefore, in this research,
we focus on gestural modeling in order to create editable 3D models that can be modified at a later point in time.

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

There has been one modeling study involving sequential peninput strokes: Kim et al. 2006 [10]. This study also pointed out the
recognition problem associated with the reconstructive modeling
of complex models and presented a method to generate complex
CAD models using sequential single strokes. On the other hand, in
2012, Cheon et al. [8] introduced a method to exchange 3D models
from a gestural modeling system to a PC (Personal Computer)
Mechanical CAD (MCAD) system. These researchers made maps
between gestural modeling commands based on analyses of single
strokes and the modeling commands of MCAD; they determined
that 3D models generated from sketching can be regenerated
and modified in the MCAD system. These two studies used pen
stroke-based modeling without multi-touch input and mapped
gestural modeling commands from an analysis of single strokes
to CAD modeling commands. Kim’s study allows the generation of
relatively more diverse and complex models, but Cheon’s study has
the advantage of simple modification of models after modeling.
Currently, it is possible to have two or more simultaneous touch
inputs (known as multi-touch input) using a capacitive touchscreen. Past pen-input methods were only able to accept one-point
inputs. Thus, the multi-touch method is more diverse and scalable.
As a result, there have been many studies on multi-touch modeling
in recent years [11–13]. In 2012, Ranglani [13] used a multi-touch
system for the manipulation of 3D models. He suggested a method
for operating and arranging 3D models using certain commands
such as move, rotate, and lift. Until recently, most multi-touch
studies have been limited to manipulation and navigation models
such as that in Ranglani’s research.
In 2010, a company called SpaceClaim [12] released a demo
for 3D modeling using multi-touch commands. This system is
based on their own modeling program, and it allows users to
deal with complex 3D models by, for instance manipulating these
models. It also allows users to select and delete certain parts of the
models using multi-touch input commands. This system is fairly
intuitive and has shown good results, but these results are only
applicable to a multi-touch PC environment. Multi-touch gestures
are adopted for manipulation, but for most other operations, multitouch gestures are not used. Only one-finger touch commands
are executed via a PC mouse for selecting menus and modifying
models. This program is fairly intuitive as well, but due to the
complex User Interface (UI) of this program for PCs, it is not suitable
for smart devices.
As explained above, several simple 3D models may be generated
using the existing studies without multi-touch input, but the
ability to create various models is limited; the shapes of singlestroke commands are not intuitive, and the recognition part of the
stroke must become more complex in order to allow the use of
more commands. On the other hand, studies on multi-touch input
have typically been focused on the manipulation of models rather
than modeling.
In addition, other studies use hand gestures from cameras [14]
or motion sensors [15] for modeling and manipulation in the
virtual reality field. However, additional devices are needed for
these systems, and the accuracy and robustness of the sensor data
were lower than those factors of touch input data, making hand
gestures inadequate for smart devices.
Accordingly, in this study, we propose a simple and light CAD
application that is applicable to smart devices with multi-touch
gestures.
3. 3D modeling on smart devices
3.1. Modeling functions
There are many functions for 3D modeling, including modeling
functions and manipulating functions.

231

Table 1
Modeling & manipulating functions.
Sketch function

Feature function

Manipulate function

Line
Circle

Extrusion
Cut extrusion
Sweep
Datum plane

Move
Rotate
Zoom in/out
Front/top/right view

It is inefficient to implement every function onto smart devices
because each commercial CAD system has slightly different types
of functions and because the vast number of functions cannot all
be accounted for. As such, we defined an essential subset for smart
devices in a prior study [2]. The neutral function set defined in
the MPA research [16,17] (a detailed explanation is provided in
Section 3.3) is considered as the full set of modeling functions
from which we chose some functions that were deemed necessary.
A detailed explanation of the process for defining the subset is
provided in the aforementioned previous study [2]. The conditions
for defining the subset are as follows:
1. if a shape created using a function can be created using another
function, the two functions are integrated into a single function;
2. when integrating, the function with a higher frequency of use
is chosen;
3. functions that produce complex shapes and with a low frequency of use are excluded;
4. functions that are not generally used as a reference to make
another feature are excluded to reduce the level of complexity
(e.g., Fillet).
The defined subset of modeling functions and manipulation
functions is shown in Table 1. Only six modeling functions are
adopted. A detailed explanation of each modeling function is given
in Fig. 2. While there are many functions for manipulating, we
adopted a few functions that are commonly used in every commercial CAD system.
3.2. User interface
In the field of graphics or design, there is substantial research on
editing mesh models [18,19] or design [20,21] with sketch input.
These studies have provided various tools and interfaces for users.
Our study also supports a modeling tool and manipulating tool
using gestures, buttons and menus.
The target of our system is smart devices. Therefore, we used
multi-touch gestures by default. Before defining a multi-touch
gesture, some constraints should be defined for the smart device
environment:
1. the user uses the fingers of one hand only to make gestures. We
assume that the smart device is held in the other hand of the
user. Of course, the user can lay the device down on a table and
use both hands, but in consideration of common circumstances,
all gestures are assumed to be made with one hand;
2. the system does not require detailed/accurate input from the
user (e.g., selecting a point of an edge) because it is difficult to
enter an accurate input through a finger-touch action;
3. fewer gestures are generally associated with multi-touch gestures that are more familiar. If needed, a small number of menus
and buttons can be used to reduce the number of gestures;
4. duplicated gestures can exist and can be distinguished according to the operation mode.
3.2.1. Modeling tool
For this study, we adopted simple and familiar gestures for
users. Users are more familiar with frequently used gestures in the
existing studies or devices than they are with new gestures.

232

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

Fig. 2. Explanation of the modeling functions.

Table 2
Standard multi-touch gestures of the iPad.
Action

Gesture

Zoom in/out
Go home
Show multitasking bar
Switch applications
Select/edit mode
Move screen/scene

Pinch in/out actions with 2 fingers
Retract actions with more than 4 fingers
Upward sweep actions with more than 4 fingers
Sideward sweep actions with more than 4 fingers
Hold actions with 1 finger
Sweep actions with 1 finger

Ordinary people can use various multi-touch gestures while
using the iPad. It has provided some standard multi-touch gestures
as shown in Table 2.
As shown above, a different number of fingers even for the same
action can result in different commands. Here, each action is very
simple, such as a sweep and a pinch. Thus, we also focus on these
characteristics to define gestures. With commonly used gestures,
we can reduce the users’ learning time for gestures as well as
the gesture recognition errors of the system. Cheon’s study found

that the recognition difficulty increases with the complexity of the
stroke [8]. We tried to minimize the gesture recognition errors of
the system by simplifying the stroke path of the gestures.
What is problematic here is that there are various functions
to be mapped to the gestures. Because we only use the fingers
of one hand, we decided that introducing operation modes as
complex finger motions is not desirable. We noticed that the modeling process of MCAD systems flows in the general pattern of
Selection → Sketch → Feature.
In line with this pattern, we divided the operation mode into
four types: the sketch mode, feature mode, selection mode, and
view change mode (for manipulating). A few buttons were provided to switch the mode on the screen. We allowed duplicate
gestures according to the mode to reduce the number of gestures
involved. Hence, for modeling tools, only two gestures for the
sketch mode and four for the feature mode are needed.
When making a sketch, the user simply can use one finger
i.e. a pen; therefore, the gestures for the sketch functions were defined as sketch with one finger. The system recognizes the shapes of

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

233

Table 3
Descriptions of gestures.
Function

Gesture

Sketch line
Sketch circle
Extrusion
Cut extrusion
Sweep
Datum plane

Sketch w/ 1 finger
Sketch w/ 1 finger
Drag w/ 1 finger
Drag w/ 1 finger
Drag w/ 2 fingers
Drag w/ 3 fingers

Move
Rotate
Zoom in/out
Front/top/right view

Drag w/ 1 finger
Drag w/ 1 finger while holding 1 finger
Pinch w/ 2 fingers
X (using menus)

The defined gestures for each mode are shown in Table 3 and
Fig. 4.
Fig. 3. Move/rotate functions in 3D space.

sketches automatically (more details are in 4.1). For feature functions, fewer fingers were used in more frequently used functions,
such as the extrusion function.
When selecting the references of the feature, the user does
not select a point or an edge as a reference because it is difficult
to perform an accurate input on the touch screen of a smart
device. In this system, the user can select references on the feature
tree directly. Other functions that require accurate inputs are not
supported on the smart device. Alternatively, the user can create a
simplified model on the smart device and then modify it in the PC
environment.
3.2.2. Manipulating tool
With regard to manipulating functions (move, rotate, zoom
in/out), many studies have adopted similar approaches, involving,
for instance, drag and pinch in/out functions. We decided to use
similar gestures for manipulating as well. The move and rotate
functions are operated based on a 2D viewpoint, as shown in Fig. 3.
When the standard menu button provided by Android is
pressed, there are several viewpoint options. Examples are the
diagonal, top, front and right views. The users can change the viewpoint quickly and easily using the menu. It is also possible to minimize the switching mode to the view change mode while modeling.

3.3. Saving modeling information to macro file
As explained earlier, complex and fine modeling is difficult on
a smart device. Consequently, after modeling on a smart device, it
is necessary to perform modification on a PC. In this study, the system saves the entire modeling procedure (functions) to a macro
file. The file type is ASCII, and this file can be sent to a PC. Macro is
a computer terminology that denotes a method that supports the
simultaneous execution of various pre-saved commands [22]. In
commercial CAD systems, macros are used commonly for recreating 3D models.
The macro files defined for smart devices are translated to files
in a neutral format for PCs; these files can then be transferred to
PCs and converted for a commercial CAD system using a macroparametrics approach (MPA). MPA is a history-based parametric
model exchange method that uses macro information, which is
a recording of the modeling command sequence or the modeling
history, to exchange parametric models [23]. The research on MPA
has already defined a neutral function set from a common set of
functions used in commercial CAD systems such as CATIA, Pro/E,
UG, IDEAS, Solidworks and SolidEdge [17]. The modeling functions
defined in this research for smart devices are a subset of the neutral
function set of MPA. Therefore, we can easily convert macro files

Fig. 4. Defined gestures.

234

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

Fig. 5. Recognition of sketches.

for smart devices to macro files for commercial CAD systems on a
PC.
The MPA project [23] at KAIST in Korea offers translations between neutral formats and commercial CAD systems. We only need
to provide one translator from the application of the smart device
to the neutral format. Then, we can exchange the editable 3D models from the smart device to the commercial CAD systems using
MPA. Currently, the MPA method can support the export of meshes
from CAD only (one-way exchange), as it is difficult to convert the
mesh data into a parametric feature-based CAD model [24]. Although 3D models are created with polygonal meshes in this research, the system records the modeling command sequence to
a macro file every time. Conversion from the mesh model of the
smart device to a commercial CAD model is thus fully possible.
Users can easily exchange 3D models from smart devices to
their commercial CAD systems running on their PCs using our
system. After the exchange, the simple and inaccurate model from
the smart device can be modified into a detailed model with precise
dimensions.
4. Implementation and case study
4.1. Gesture recognition
2D point coordinates are acquired on the touch panel of the
smart device as the inputs for each gesture. However, we have to
create a 3D model based on this 2D information. This is similar to
the input of PC-based CAD via a mouse.
Some information should be added to convert 2D data to 3D
data. Therefore, there is a high potential for errors. To minimize the
number of errors, the converting algorithm should be as simple as
possible.
First, when entering the sketch mode, the user’s view is moved
to an angle perpendicular to the selected reference surface, and
the user draws 2D sketches on the reference face. The two possible sketch types are a circle and a line. They can be recognized
by the following method. When the user touches the screen with
a finger, an onPress event occurs. When the user moves the finger
on the screen, onMove events occur frequently. When the user releases the finger from the screen, an onRelease event occurs. The
coordinates of the onPress point are denoted as (x1 , y1 ), the coordinates of the onMove point are denoted as (xcurrent , ycurrent ), and
the coordinates of the onRelease point are denoted as (x2 , y2 ). The

sketch inputs are recognized as follows:
Lfin
Lmax

=

(x1 − x2 )2 + (y1 − y2 )2
< ε.
(x1 − xmax )2 + (y1 − ymax )2

Here, (xmax , ymax ) denotes the coordinates of the point furthest
from (x1 , y1 ) with the squared distance Lmax . The squared distance
between (x1 , y1 ) and (x2 , y2 ) is Lfin . This is detailed in Fig. 5.
If the left term is less than a user-defined factor ε , the input is
recognized as a circle. If not, the input is recognized as a line.
If the sketch input is recognized as a circle, the center is
( xmax2+x1 , ymax2+y1 ), and the radius is ( xmax2−x1 , ymax2−y1 ). In the case
of a line, the start point and the end point are (x1 , y1 ), (x2 , y2 ).
When creating features, some features are created according to
the amount of the movement in the gestures, such as protrusion
extrude, cut extrude, and datum plane features. When entering the
feature mode, these functions also change the current view to the
side view according to the selected reference. Then, (xcurrent − x1 )
is considered to be the distance factor of the features. To reduce the
number of errors, y coordinates are ignored, and the current result
of the feature is shown on the screen during the onMove phase. On
the other hand, the sweep feature is independent of the moving
distance of gestures. Accordingly, if x2 − x1 > σ after an onRelease
event occurs, the system creates a sweep feature. In addition, σ is
a user-defined factor.
It is not necessary to compute 3D coordinates from 2D touch
inputs in this application due to the smart change capability of
modeling views.
4.2. System workflow
This study was implemented on Android devices. The target
device is the Samsung Galaxy Tab 10.1; it has a 10 in display with
a resolution of 1280 × 800, and runs on Android 3.2 Honeycomb.
The buttons for switching the four modes (sketch, feature, selecting, and view mode) are on each corner, and the current mode
is shown on the upper side of the screen. The left side of the screen
shows the feature tree of the current model; the right side provides
a selection list that shows the names of the selected features during
the select mode. In the center of the screen, the current 3D model
is visualized. The lower part provides the save button and the load
button for saving and loading macro files on the smart device.
Fig. 6 shows the workflow of the entire system. The modeling procedure is similar to that of a commercial MCAD system.

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

235

• The second model: (on the smart device) head sketch – extrude
– 2 rectangular hole sketches – cut extrude – body profile sketch
– guide curve sketch – select the profile and the guide curve –
sweep – datum plane – tail sketch – extrude – hole sketch on the
tail – cut extrude – (on the PC) select head holes – edit sketches
– select the sweep feature – edit profile sketch – select edges of
the head – fillet.
• The third model: (on the smart device) bottom rectangular
sketch – extrude – cylinder sketch – extrude – datum plane –
top cylinder sketch – extrude – hole sketch – cut extrude – linear pattern – (on the PC) datum plane – sketch a circle – cut
extrude – repeat several times – select edges – chamfer.
Most simple models can be generated, but complex shapes
should be simplified for modeling because many modeling functions were excluded during the development of the CAD system
that is usable on a smart device. While it is technically feasible
to add other operations and functions existing in commercial CAD
systems, some problems may arise, as outlined below:
1. a geometric kernel for CAD on a smart device does not exist.
The implementation of a kernel that supports the complex
geometry is necessary.
2. the number of gestures is increased according to the increase
in the number of functions. This also leads to an increase in the
learning time of users, an increase in the likelihood of operational errors, and an increase in the degree of algorithm complexity for recognizing gestures.

Fig. 6. The workflow of the entire system.

However, the main differences compared to commercial CAD systems are that the user can use the defined gestures only for modeling, after which they must send the saved macro file to a PC for
modifying or additional modeling. This workflow is a result of considering the characteristics of smart devices, as this is the first system design of a 3D CAD system on a smart device. An example of
detail modeling and the related editing procedures are shown in
Figs. 7 and 8.

Mesh handling is an important issue for decreasing the computation complexity and increasing the processing speed of 3D models [25]. However, designing a 3D modeling kernel for a CAD system
is another difficult problem, and it is not covered here. Instead of a
building 3D modeling kernel for smart devices, the system makes
simple 3D models with polygonal meshes, or loads pre-stored
primitive shapes in the form of meshes and then resizes the shapes.
In this research, we allowed duplicate gestures according to on
operation mode to reduce the number of gestures and avoid confusion. Most users were able to input the defined gestures easily on
our system. However, a few users were confused about how many
fingers they should use to enter the cut, sweep, and datum plane.
When users were confused, we provided a table containing information on the number of fingers necessary for each function. This
helped the users to create a proper model. There appears to be a
problem caused by the mapping of multiple functions to a single
motion for simplified input. Therefore, a study that determines the
optimal numbers of gestures and functions is needed in the future
to enhance user understanding.
Above all, the primary benefit of this research lies in the availability of conceptual modeling anytime and anywhere. The system
also supports the exchange of inaccurate models from smart devices to PCs for more detailed and accurate editing.
5. Conclusion

4.3. Case study
In this chapter, the actual modeling process of test cases using this application is shown. We created several models with the
smart device application developed here, as shown in the upper
figures in Fig. 9, and then performed additional editing in the PC environment, as shown in the lower figures in Fig. 9. There are many
ways to create a model with the same shape, but one example of a
modeling sequence got these models is shown below.

• The first model: (on the smart device) L-shaped sketch – extrude – datum plane – hole sketch – cut extrude – (on the PC)
select an edge – fillet.

In this study, we developed a CAD modeling system for the
smart device environment. The major contribution of this study is
that we proposed the first-ever system design of a 3D CAD system
available on a smart device with consideration of the characteristics and shortcomings of smart devices. Implementation methods
and case studies using a prototype system with examples were also
introduced in order to demonstrate the applicability of the proposed system. This system opens up the possibility of 3D modeling
on smart devices based on simple multi-touch gesture commands.
Moreover, the system supports additional fine modifications of 3D
models in the PC environment based on a macro-parametrics approach.

236

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

Fig. 7. An example of the modeling procedure on a smart device.

Fig. 8. An example of the editing procedure on a PC.

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

237

Fig. 9. Models on the smart device (top) and the models after additional editing on the PC (bottom).

Using the proposed system, users can create 3D models on
smart devices quickly and easily. In addition, engineers can model
their ideas in collaborative environments or even outdoors, making it possible to shorten the design time. More general industrial models should be tested in further experiments. In addition,
user tests should be performed, including comparisons of the running/modeling times of CAD systems on smart devices with those
on PCs.
This research has some limitations. For this reason, additional
work is required.
First, the system does not offer a function to modify models
after modeling. Currently, there is no B-rep information pertaining
to the models in the smart device. As a result, the user cannot
edit the models after modeling, even if a feature tree exists. Given
this shortcoming, more studies are needed to allow models to be
modified after the initial modeling.
Second, the current system adopts ‘familiar’ multi-touch gestures that are similar to the pre-defined gestures of the iPad. With
these popular (commonly used) gestures, we tried to minimize the
learning time for users and the complexity of the gestures, thereby
reducing the number of mistakes made by user and the gesturerecognition errors made by the system. In practice, however, an
additional study of the capabilities of these gestures is needed. It
is necessary to analyze the level of comfort felt by the user when
making these gestures. In particular, this system can be used more
easily if the optimal number of gestures and intuitive gesture actions for feature creation are defined, allowing the user to learn
and utilize the gestures without confusion. Furthermore, considering how smart devices are equipped with several built-in sensors,
such as an inertial sensor and camera, a multi-modal method can
be used to input a wider variety of operations with greater accuracy and ease.
Unlike CAD models of commercial CAD systems in the PC environment, the models created using the current system contain only
relative dimensions and do not handle constraints. Currently, this
issue can be resolved with users’ manual additions of constraints
and actual dimensions after converting models for the PC environment. If the system provides a user interface for constraints and
actual dimensions on a smart device, the amount of necessary further user modifications on the PC can be decreased. To realize this,
the methodology for saving numeric data with B-rep information
mentioned earlier should be developed for smart devices.

In addition, if real-time synchronization among multiple devices can be enabled while the system is running on a network,
a more effective collaboration environment for design can be expected.
Acknowledgments
This work was supported by the Human Resources Development of the Korea Institute of Energy Technology Evaluation and
Planning (KETEP) grant funded by the Korea government Ministry
of Knowledge Economy (No. 20114030200040).
This research was supported by WCU (World Class University) program through the National Research Foundation of Korea
funded by the Ministry of Education, Science and Technology (R312008-000-10045-0).
This research was supported by Development of Integration &
Automation Technology for Nuclear Plant Life-cycle Management
grant funded by the Korea government Ministry of Knowledge
Economy (2011T100200145).
References
[1] Evolution of smart phone market, Deloitte telecommunications predictions.
2009. http://www.deloitte.com/tmtpredictions2009.
[2] Kang Y, Kim H, Han S, Suzuki H. Feature-based 3D CAD modeling on smart
device using multi-touch gesture. In: Proceeding of Asian conference on design
and digital engineering. 2012.
[3] Zeng Y, Horvath I. Fundamentals of next generation CAD/E systems. Comput
Aided Des 2012;44(10):875–8.
[4] Modi S, Tiwari MK, Lin Y, Zhang WJ. On the architecture of a human-centered
CAD agent system. Comput Aided Des 2011;43(2):170–9.
[5] Esfahani ET, Sundararajan V. Classification of primitive shapes using
brain–computer interfaces. Comput Aided Des 2012;44(10):1011–9.
[6] Fuge M, Yumer ME, Orbay G, Kara LB. Conceptual design and modification
of freeform surfaces using dual shape representations in augmented reality
environments. Comput Aided Des 2012;44(10):1020–32.
[7] Vizireanu DN, Halunga SV. Simple, fast and accurate eight points amplitude
estimation method of sinusoidal signals for DSP based instrumentation. J
Instrum 2012;44:P04001.
[8] Cheon SU, Kim BC, Mun D, Han S. A procedural method to exchange editable
3D data from a free-hand 2D sketch modeling system into 3D mechanical CAD
systems. Comput Aided Des 2012;44(2):123–31.
[9] Company P, Piquer A, Contero M. On the evolution of geometrical reconstruction as a core technology to sketch-based modeling. In: The proceedings of
EUROGRAPHICS workshop on sketch-based interfaces and modeling. 2004.
[10] Kim DH, Kim MJ. A new modeling interface for the pen-input displays. Comput
Aided Des 2006;38(3):210–23.

238

Y. Kang et al. / Computer-Aided Design 59 (2015) 229–238

[11] Sharma A, Madhvanath S, Shekhawat A, Billinghurst M. MozArt: a multimodal
interface for conceptual 3D modeling. In: Proceedings of the 13th international
conference on multimodal interfaces. 2011. p. 307–10.
[12] SpaceClaim, 2010. http://www.spaceclaim.com/en/Resources/VidPlayer.aspx.
[13] Ranglani S. 3D shape manipulation using multi-touch gestures, Master thesis
of The University of Tokyo, Japan; 2012.
[14] Zhao S, Tan W, Wu C, Liu C, Wen S. A novel interactive method of virtual reality
system based on hand gesture recognition. In: Proceedings of the ’09 control
and decision conference. 2009. p. 5879–82.
[15] Hoang TN, Thomas BH. Distance-based modeling and manipulation techniques
using ultrasonic gloves. In: Proceedings of the IEEE international symposium
on mixed and augmented reality. 2012. p. 287–8.
[16] Choi GH, Mun D, Han S. Exchange of CAD part models based on the macroparametric approach. Int J CAD/CAM 2002;2(2):23–31.
[17] Mun D, Han S, Kim J, Oh Y. A set of standard modeling commands for the
history-based parametric approach. Comput Aided Des 2003;35:1171–9.
[18] Ju T, Zhou QY, Hu SM. Editing the topology of 3D models by sketching. ACM
Trans Graph 2007;26(3): Article 42.

[19] Saul G, Lau M, Mitani J, Igarashi T. SketchChair: an all-in-one chair design
system for end users. In: Proceedings of the fifth international conference on
Tangible, embedded, and embodied interaction. 2011. p. 73–80.
[20] Lin J, Igarashi T, Mitani J, Liao M, He Y. A sketching interface for sitting pose
design in the virtual environment. IEEE Trans Vis Comput Graphics 2012;
18(11):1979–91.
[21] Shin HJ, Igarashi T. Magic canvas: interactive design of a 3-d scene prototype
from freehand sketches. In: Proceedings of the graphics interface conference
2007. 2007.
[22] [in Korean] http://terms.co.kr/macro.htm.
[23] Li J, Han S, Shin S, Lee S, Kang Y, Cho H, et al. CAD data exchange using the
macro-parametrics approach: an error report. Int J CAD/CAM 2010;10.
[24] Kim H, Han S, Suzuki H. Converting a CAD mesh model into a feature
feature-based CAD system based on planar edge loop analysis for a macroparametrics approach. In: Proceeding of Asian conference on design and digital
engineering. 2012.
[25] Li J, Lu G, Ye J. Automatic skinning and animation of skeletal models. Vis
Comput 2011;27:585–94.

