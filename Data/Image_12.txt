Image and Vision Computing 40 (2015) 49–64

Contents lists available at ScienceDirect

Image and Vision Computing
journal homepage: www.elsevier.com/locate/imavis

Editor’s Choice Article

Hallucination of facial details from degraded images using 3D
face models☆
Matthaeus Schumacher 1, Marcel Piotraschke 1, Volker Blanz ⁎
Institute for Vision and Graphics, University of Siegen, Germany

a r t i c l e

i n f o

Article history:
Received 24 December 2013
Received in revised form 23 October 2014
Accepted 13 June 2015
Available online 24 June 2015
Keywords:
Face hallucination
3D models
Model-based deblurring
Occlusions
Faces

a b s t r a c t
The goals of this paper are: (1) to enhance the quality of images of faces, (2) to enable 3D Morphable Models
(3DMMs) to cope with severely degraded images, and (3) to reconstruct textured 3D faces with details that
are not in the input images. Details that are lost in the input images due to blur, low resolution or occlusions,
are ﬁlled in by the 3DMM and an additional texture enhancement algorithm that adds high-resolution details
from example faces. By leveraging class-speciﬁc knowledge, this restoration process goes beyond what general
image operations such as deblurring or inpainting can achieve. The beneﬁt of the 3DMM for image restoration
is that it can be applied to any pose and illumination, unlike image-based methods. However, it is only with
the new ﬁtting algorithm that 3DMMs can produce realistic faces from severely degraded images. The new method includes the blurring or downsampling operator explicitly into the analysis-by-synthesis algorithm.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
Difﬁcult imaging conditions due to blur, low resolution, noise, partial
occlusions or non-uniform lighting are frequently encountered in many
real-world applications, for example in law enforcement if a suspect has
to be recognized in low quality image material. A number of image processing algorithms recover and enhance information that is present in
the image, yet mostly invisible to the human eye. For example,
deconvolution strives to invert the effect of blurring. Often the blur kernel (or point spread function, PSF) is unknown and has to be estimated
from the image. A survey of this so-called Deblurring by Blind
Deconvolution problem can be found in [1], and some more recent publications in this ﬁeld include [2] and [3]. For low resolution video,
deconvolution can be combined with methods to merge data from all
frames [4,5]. On the other hand, structures that are degraded due to occlusion can be partly recovered with image inpainting methods [6].
These methods can be applied to any image material, because they
make only very general assumptions about the image content. However, if it is known that part of an image shows a human face, it is possible
to add new information that was not in the image to begin with. If the
lower half of a face is occluded, we can still safely assume that a
mouth and a chin have to be added, and we can estimate their pose

☆ Editor's Choice Articles are invited and handled by a select rotating 12 member Editorial
Board committee. This paper has been recommended for acceptance by Shishir Shah.
⁎ Corresponding author. Tel.: +49 271 740 2035.
E-mail addresses: schumacher@informatik.uni-siegen.de (M. Schumacher),
piotraschke@nt.uni-siegen.de (M. Piotraschke), blanz@informatik.uni-siegen.de (V. Blanz).
1
Tel.: +49 271 740 2036.

http://dx.doi.org/10.1016/j.imavis.2015.06.004
0262-8856/© 2015 Elsevier B.V. All rights reserved.

angle and lighting from the upper half of the face. The same is true for
blurred regions: even if the eye and eyebrow are only dark spots in
the input image, we can ﬁll in eyeball, iris, eyelashes and all other details
of human eyes. Fig. 3 shows how this can be done with our proposed
algorithm.
Given a mathematical model of the expected content of the image,
such a model-based image enhancement can be achieved. In this
paper, we rely on the 3D Morphable Model (3DMM) [7] as a statistical
description of the natural shapes and textures of faces. It is important
to stress that the added image detail cannot be more than an educated
guess, based on the prior information about faces on the one hand,
and all the remaining information that is in the image on the other hand.
One solution would be to ﬁll in the details from the average face, or
any other random face. Our algorithm goes one step further by
exploiting correlations in the set of human faces: After ﬁtting the
3DMM to the degraded image, we obtain a best ﬁt, then from this best
ﬁt we take the details and render these into the image, both in the
case of blurred and partly occluded faces (Fig. 2). This idea is along the
lines of 3D shape reconstruction from single images using 3DMMs [7],
where the model is ﬁtted to colors of pixels, and gives an estimate of
depth. Recently, it has been shown that this inference is consistent
with human expectation: when viewers see a frontal view of a face
and then a choice of proﬁles that are all geometrically consistent with
the front view, they tend to prefer those proﬁles that were calculated
by the 3DMM [8], even if the choice includes the ground truth proﬁles
of the face.
With this caveat, model-based inference of missing information may
be a useful tool to obtain high quality images or 3D face models from degraded input data.

50

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

(a)

(b)

(c)

(d)

Fig. 1. a shows the blurred input image and the ﬁnal result of the 3D reconstruction after applying our new approach. b to d are zoomed in views to the eye and mouth regions of the
reconstructed 3D face model. While in 1b only the original texture was extracted from the photo, in 1c the deblurring described in Section 4.1 was applied and in 1d the high-resolution
texture transfer of Section 5 was added to the results of column 1c.

For images of human faces, the model-based ﬁll-in of facial details
has become known as Hallucinating Faces [9]. For a recent survey, see
[10]. For low resolution images, AAMs have been used to ﬁll in missing
details [11].
In contrast to this work, Baker et al. proposed a model that does
not account for shape differences explicitly, and that uses a maximum a posteriori (MAP) estimator to estimate the high-resolution
levels of a Gaussian Pyramid of registered images [9]. In a two-level
approach based on global Eigenfaces and a local patch-based nonparametric Markov network, Liu et al. achieved very signiﬁcant improvements in image resolution [12,13]. A separation of global face
hallucination and local feature hallucination has been proposed in
[14]. For face hallucination in video, Dedeoglu et al. use spatiotemporal consistencies and a domain-speciﬁc prior [15].
Soon after their initial development, Active Appearance Models
(AAMs) were used to reconstruct missing structures in occluded regions
[16]. Reconstruction of facial images both in case of partial occlusion and
low resolution using a 2D Morphable Face Model, which bears similarities with AAMs, has been presented in [17,18].
Using separate reconstruction modules for 2D shape and texture
that account for global structure and local detailed texture [19] can reconstruct occluded regions in images of faces. Another approach that
is able to ﬁll in occluded regions uses asymmetrical Principal Component Analysis (PCA) [20].
All of these algorithms use a statistical model of 2D faces restricted to
poses that are close to frontal. Larger variations in pose have been handled
by using a Gabor wavelet decomposition of faces and a set of linear

(a) occluded input

mappings between wavelet features in different poses [21], or by
exploiting large datasets, recent image matching techniques and MAP
estimation [22].
Unlike these image-based methods, this paper proposes a 3D approach that is intrinsically invariant to changes in pose, size, illumination and other image parameters. Our strategy is to ﬁt a 3DMM
[7] to the input image with a novel ﬁtting algorithm that is robust
to the effects of blurring by explicitly simulating image blur in an
analysis-by-synthesis. The algorithm works on any blur level, and estimates the appropriate level automatically. In addition to the facial
details estimated by the 3DMM reconstruction (equivalent to an
MAP estimate [23]), the algorithm adds details, such as eyelashes
and pores, from other faces to obtain high resolution results.
A method for hallucinating 3D facial shapes from low resolution
3D scans using an radial basis function (RBF) regression that predicts
curvatures and displacement images at high resolution from their
low resolution version was presented by [24]. Unlike our algorithm,
their input data are 3D, and no texture is used.
A major challenge in using a 3DMM for face hallucination or 3D
reconstruction from low resolution images is to adapt the cost function to blurred input data. This challenge also occurs for generative
face models in 2D, such as AAMs. An algorithm to make AAMs applicable to low resolution images was presented as Resolution Aware
Fitting (RAF) [25]. Similar to our method, they include an explicit
model of the downsampling or blurring in their cost function, and
they compute the image difference in terms of pixels of the input
image space and not in the shape-free texture space, as standard

(b) 3D face

(c) reprojection

Fig. 2. 3D reconstruction of non-frontal input image with occluded face regions. Parts of the input image are occluded due to glasses and facial hair. These have to be marked manually. b
shows the 3D reconstruction with compensation of occluded areas. After reprojection and relighting of the reconstruction, a hallucination of occluded facial regions is possible (c).

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

51

image with those inferred by the 3DMM. Note that the occluded pixels
have to be marked manually, similar to image inpainting [6].

(a)

(b)

The input data for our algorithm are an image, 5-7 feature point coordinates and, if the face is partially occluded, a binary occlusion mask.
The output is a textured 3D face and a rendering of this face into the
input image. Please note that the feature point coordinates can also be
from automated feature detectors [26].
2. 3D face reconstruction

Fig. 3. a shows a close-up of an eye in a blurred input image, whereas b shows the close-up
of the deblurred reconstruction.

AAM would do. However, besides being a 2D approach which is restricted to frontal views, their way of treating the blur function is
fundamentally different from ours: RAF is based on a Taylor expansion of the effect of the degrees of freedom of the AAM on the blurred
image, so these degrees of freedom are a ﬁrst order perturbation of
the blurred image. In contrast, our approach treats the effect of
blurring as a perturbation of the imaging process.
In this paper, Sections 2 and 4 summarize components that were introduced in earlier versions of the 3DMM ﬁtting algorithm [7,23], but
they include a number of relevant, yet unpublished implementation details. On this technical basis, the entire Sections 3, 4.1, 5 and 6 describe
new algorithms (Fig. 4.)
Novel contributions of this paper are:
• A 3D model-based algorithm for face hallucination at any pose and
illumination,
• A method for handling blur in 3D analysis-by-synthesis,
• A self-adapting estimate of blur levels,
• An algorithm that combines low spatial frequency information from
the input image with mid-level details of the model,
• Transfer of high spatial frequency details across faces for face hallucination on the level of eyelashes and pores,
• An algorithm that treats occlusions in the ﬁtting process, and that produces seamless textures that combine details extracted from the

2.1. 3DMM
The 3DMM, of 3D faces ([7]) is a statistical model that captures the
range of natural faces in terms of 3D shapes and textures. It is derived
from a dataset of 3D scans of faces. The crucial step is to establish
dense point-to-point correspondence of all faces with a reference face.
Then, shapes and textures of all m individual faces i ∈ {1, ..., m} in the database are represented by shape and texture vectors [7]
Si ¼ ðX 1 ; Y 1 ; Z 1 ; X 2 ; …; X n ; Y n ; Z n ÞT

ð1Þ

Ti ¼ ðR1 ; G1 ; B1 ; R2 ; …; Rn ; Gn ; Bn ÞT

ð2Þ

formed by the coordinates and colors of all n vertices of the reference
model. In our model, m = 200 and n = 75, 972.
In this face space representation, convex combinations of face
vectors are equivalent to morphs of the database faces, and will therefore have a natural face-like appearance:
S¼

ai ∈½0; 1

i¼1

T¼

m
X
ai ¼ 1

ð3Þ

i¼1

m
X
bi Ti ;

bi ∈½0; 1

i¼1

m
X
bi ¼ 1:

ð4Þ

i¼1

Using PCA, the distribution of database faces can be described in
terms of arithmetic means, unit-length eigenvectors and standard deviations for shape and texture:
s;

2D Input Image

m
X
ai Si ;

si ;

σ s;i

and t;

ti ;

σ t; i

ð5Þ

With this, we can rewrite Eqs. (3), and (4) in a new basis (see [7])
with the coefﬁcients α and β:

Occlusion
Handling

3D Face
Reconstruction

Non-local
Fitting

Section 6

Section 2

Section 3

S¼sþ

m−1
X

α i si ;

ð6Þ

i¼1

Seamless
Occlusion
Fill-In

Texture
Extraction
Section 4

Model-based
Texture
Enhancement
Section 4.1

Section 6

High-Resolution
Texture
Transfer
Section 5

T ¼tþ

m−1
X

β i ti :

ð7Þ

i¼1

The fact that PCA eigenvectors are orthogonal is not used here. The
relevance of PCA in our algorithm is to reduce the number of dimensions, and to obtain an estimate of the prior distribution – a multivariate
Gaussian – for regularization to avoid overﬁtting. For linear combinations (6) and (7) the prior probabilities are:
Xm−1 α2

−12

pðα 1 ; :: :; α m−1 Þ∼e

3D Face Reconstruction
Fig. 4. A schematic overview of our processing pipeline. New contributions are highlighted
in green.

i¼1

i
σ2
s; i

Xm−1 β2

;

−12

pðβ1 ; :: :; βm−1 Þ∼e

i¼1

i
σ2
t; i

:

ð8Þ

2.2. Fitting the model to images
In an analysis-by-synthesis loop, the ﬁtting algorithm [7,23] ﬁnds
the shape and texture vector from the Morphable Model that ﬁts the

52

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

image best in terms of pixel-by-pixel color difference between the synthetic image Imodel (rendered by computer graphics rasterization), and
the input image Iinput:
EI ¼

XÀ
Á2
I input ðx; yÞ−Imodel ðx; yÞ :

ð9Þ

x;y

The squared differences in all three color channels are added in EI.
We suppress the indices for the separate color channels throughout
this paper. The optimization is achieved by an algorithm presented in
[7,23]. The algorithm optimizes the linear coefﬁcients for shape and texture, but also 3D orientation and position, focal length of the camera,
angle, color and intensity of directed light, intensity and color of ambient light, color contrast as well as gains and offsets in each color
channel.
For the optimization to converge, the algorithm has to be initialized
with the 2D feature coordinates (qx, j, qy, j) of at least 5 feature points j.
The 2D distance between the initialization positions and the current positions ðxk j ; yk j Þ of the corresponding vertices kj of the model forms an
additional cost function:

 
2
X
 qx; j
xk j 


EF ¼
−
 qx; j
yk j  :

Iinput ðxk ; yk Þ−Imodel; k

Á2

ð13Þ

which involves the following calculations:

ð11Þ

ð12Þ

with weights μF, μreg that are important for initialization and stability,
and that are reduced during the process of the iteration. The optimization problem is solved iteratively using a Stochastic Newton Descent algorithm [7]. The following parameters are optimized:
•
•
•
•
•
•
•

XÀ

ð10Þ

where ρ denotes the rendering parameters such as pose angle, 3D translation, illumination and contrast. ρi is the starting value for these parameters, and σR, i an ad-hoc estimate of the expected standard deviation.
Together, these contributions form a cost function
Etotal ¼ EI þ μ F E F þ μ reg Ereg

ak is calculated in the starting position and once every 1000 iterations on the entire face. Triangles that are invisible due to selfocclusion (z-Buffer) obtain ak = 0. The approximated cost function is
then [7]

k∈K

A third contribution to the overall cost function is a regularization
term that avoids overﬁtting. The regularization term measures the
Mahalanobis distance of the current solution from the average face
using PCA:
X α 2 X β2 X ðρ −ρ Þ2
i
i
i
i
þ
þ
;
σ 2s;i
σ 2t;i
σ 2r;i
i
i
i

As in [7,23], we chose option 3 for two reasons:
1) The analysis-by-synthesis requires surface normals and their derivatives to account for shading effects, and these normals are the easiest
to compute on centers of triangles.
2) Each triangle k can be assigned an area ak in the image space, and by
setting the probability of choosing k proportional to ak in the random
selection procedure, the expectation value of the approximated cost
function is equal to EI.

EK ¼

j

Ereg ¼

• Select a random subset of triangles k of the 3DMM, calculate their centers (Xk, Yk, Yk) in 3D, project them to image positions (xk, yk) and evaluate EI there.

Shape coefﬁcients αi,
Texture coefﬁcients βi,
3D orientation and position,
Focal length of the camera,
Ambient light (RGB channels),
Direction and color of directional light (RGB channels),
Color contrast, gains and offsets in RGB channels.

2.3. Optimization on a subset of triangles
Calculating the image difference term EI(9) on the entire image or
on all pixels of the foreground in each iteration would be very time
consuming. The main idea of Stochastic Newton Descent is to consider only a random subset of points in each iteration, and proceed in
small steps towards the optimum. For this approximation of EI, we
could proceed in the following way:
• Render the image and select a subset of pixels (x, y), which would of
course bring no speedup,
• Select a random subset of vertices i of the 3DMM, calculate their image
positions (xi, yi) and evaluate EI there, or

• 3D position of center of triangle k, using Eq. (3) for all 3 vertices,
• Rigid transformation of triangle center, given the current estimate of
pose angles,
• Perspective projection, which yields the image position (xk, yk) of the
triangle center,
• Surface normal of the triangle, computed from the corner positions in
3D,
• Surface reﬂectance (i.e. RGB vertex color) using Eq. (4),
• Phong shading, including cast shadows (see below),
• Color space transformation (offset, gain, color contrast).
If the algorithm was used for rendering, the color value Imodel, k
would be rasterized to pixel (xk, yk). Please note that in Eq. (13) the
2D position (xk, yk) only appears in Iinput(xk, yk), not in Imodel, k. The latter
is the appearance of the model triangle no matter where it is located in
the image.
3. Non-local rendering effects in an analysis-by-synthesis algorithm
In this paper, we call the effects of the image formation process
local if the appearance of a pixel depends only on one surface point
of the mesh or perhaps its neighborhood on the mesh. In contrast,
non-local rendering effects occur whenever vertices that are far
apart on the mesh have inﬂuence on the same image point. In
this terminology, blurring is a non-local effect because the color
of a pixel depends not only on the shading of the 3D surface
point that is projected to that pixel, but also on its neighbors and
– this is the crucial point – also on other vertices of the mesh
that happen to be rendered close by. This is the case whenever
there is a depth discontinuity in the rendered image, for example
along the ridge of the nose in a half-proﬁle, or along the silhouette
of the face in front of a background. A more mathematical
formulation would be that the mapping from image positions to
a surface parameterization is not continuous. If it were continuous, the effect of blurring could be simulated by blurring the
surface texture with a spatially varying and non-isotropic ﬁlter
that accounts for the effects of perspective distortion. However,
due to its non-local nature, image blur is more challenging.
In the rendering pipeline described in the previous section (the
analysis part of the analysis-by-synthesis) blurring would be formulated
in the following way: Let Imodel(x, y) be the synthetic image of the

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

current 3DMM reconstruction. Then, a blurred image is obtained by an
image-space operator
Imodel ðx; yÞ↦φðImodel ðx; yÞÞ:

ð14Þ

Existing 3DMM ﬁtting algorithms, such as [7,23], cannot handle nonlocal, patch-wise image modiﬁcations.
Our strategy is to simulate the effect of φ on a rendered image
Imodel(x, y) in the current iteration step, compare it to the un-ﬁltered
rendered image and store the difference Δj for each vertex j and each
RGB channel. This is done prior to the optimization and once every
1000 iterations. The difference is precomputed since its value is based
not only on vertex j but rather on multiple, potentially non-adjacent
vertices of the 3D face model





Δ j ¼ φ Imodel x j ; y j −I model x j ; y j :

ð15Þ

Note that Δj is attached to a vertex j and not to a pixel position, because it is computed on the model and describes the change of the
model appearance in this vertex when it is rendered and modiﬁed by
φ. The screen position xj and yj of vertex j on the input image Iinput varies
during the reconstruction process, because of the adjustment of the
shape and pose of the 3D face model.
In the cost function described in Eq. (13), which is based on triangle
centers, Δk of the center is interpolated from the values Δj of the 3 triangle vertices. With this Δk , EK is
EK ¼

XÀ

À
ÁÁ2
Iinput ðxk ; yk Þ− Imodel; k þ Δk :

ð16Þ

k∈K

As mentioned above, Δk describes the color difference of triangle
center k and a non-local color value modiﬁcation of the same triangle
center. To verify Eq. (16), we can combine Eqs. (15) and (13). The triangle version of Eq. (15) is
Δk ¼ φðImodel ðxk ; yk ÞÞ−Imodel ðxk ; yk Þ:

ð17Þ

Note that there is no fundamental difference between the trianglebased and vertex based version. The triangle-based values are only interpolations of the vertex-based values for the corners of each triangle
(average with weight 1/3). The reason for dealing with triangles is
that in each iteration the surface normal has to be updated, and surface
normals cannot be computed on individual vertices without knowledge
about the neighbors.
After substituting Eq. (17) in Eq. (16) we can write
EK ¼

XÂ
À
À À
Á
ÁÁÃ2
Iinput ðxk ; yk Þ− Imodel; k þ φ I model; k −Imodel; k

53

directions can be treated. Other forms of anisotropic ﬁltering or PSFs
from motion blur are easy to implement in our approach by changing
φblur.
To account for a diffusion of the background color into the face region along the silhouette, we render the face into the input image before
blurring and computing Δj.
3.1. Reconstruction from small image sizes
Low spatial frequency images may be found if images of reasonable
size are blurred due to defocus or motion blur. More often, however,
they occur if the image size is small. In the previous section, the operator
φ() has simulated the ﬁrst case by means of a blur operation (convolution). In our algorithm, effects of image size, sampling and aliasing are
less obvious.
Unlike the cost function (Eq. (9)), which involves a summation over
pixels (x, y), the triangle-based functions in Eqs. (13) and (16) seem to
be independent of image size. For each triangle center, the estimated
color is computed and compared to the color in Iinput(xk, yk). If Iinput is
small, many triangles will be compared to the same pixel. Our novel algorithm (Section 3) alleviates this problem because Δk, which simulates
the effect of blurring and proper down-sampling, brings the colors of
model points closer to what is found in Iinput, so the multiple triangles
that are mapped to the same pixel will also have more and more similar
colors as b increases.
Still, we have found that the optimization algorithm works better if
small images are upsampled with a Gaussian kernel to a standard minimum size of 400 × 400 pixels for the face region. The reason is that the
calculation of image gradients is more reliable if the relevant structures
of Iinput are well above the pixel resolution. It is easy to determine how
much the image has to be scaled up since the positions of 5 feature points
are available for initialization of the ﬁtting algorithm anyway (Section 2).
3.2. Input blur estimation
As mentioned above, it is necessary to specify the blur level in the
vertex blur function φblur which is used by the new error function (see
Eq. (16)). To obtain the best reconstruction results, this blur level should
be equal to the blur level of the input image. Thus, an identiﬁcation and
measurement metric for the blur level of a given input image is required.
In this paper we use the blur metric proposed in [27]. This metric is
based on the smoothing effect of sharp edges by measuring the spread
of edges in an image. To detect the edges, a Sobel ﬁlter is applied to the
luminance of pixels. In order to separate the gradient image from noise
and insigniﬁcant edges, a thresholding is then applied. The start and
end positions of an edge are deﬁned as the locations of the local luminance extrema closest to the edge [27]. In other words, the distances

k∈K

EK ¼

XÂ
À
ÁÃ2
Iinput ðxk ; yk Þ−φ Imodel; k :
k∈K

This equation shows that the extension of the error function leads to
a non-local modiﬁcation of the analysis-by-synthesis loop.
In this paper we focus on blurred input images. With a new variable
b describing the blur level, let
À
Á
φblur Imodel; j ; b

ð18Þ

describe the effect of blurring the reprojected face model for each vertex
j (or triangle center k). Hence Δj is the vertex difference between the
rendered and blurred model and the non-blurred rendering. φblur is implemented by ﬁltering the rendered image Imodel with a 3-tap binomial
ﬁlter iteratively, and b is the number of iterations. The ﬁltering process
is separable, thus different blur levels for horizontal and vertical

(a)

(b)

Fig. 5. Sharpened texture used for blur estimation and blur compensated reconstruction
(a) and original texture (b) of the average head.

54

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

from blurred
our method

from blurred
no compensation

from original

Fig. 6. 3D face reconstruction from original unblurred image (right), blurred image without compensation (middle) and blurred image with the proposed method (left). The texture is
generated by the 3DMM due to linear combination of principal components of texture vectors.

between the nearest local maxima and minima to an edge are used as the
local blur measure for the current edge location. By averaging the local
blur values over all edges, the global blur metric of the image is calculated.
Given this blur metric on the input image, we still need to ﬁnd out
the appropriate blur level for the model. The measured overall blur depends on the size and the content of the images, so that two images with
the same blur level may have different overall blur measures. This could
be partially avoided by using machine learning algorithms on images
with controlled blur levels. In this paper we propose a novel self-

from blurred
our method

adapting blur measurement that has the potential to operate on a
wide variety of input images (different sizes, illuminations, blur levels).
After the ﬁrst set of iterations of the ﬁtting algorithm, we have a very
conservative ﬁrst estimate of head shape (estimate head) and an estimate of the rendering parameters (e.g. camera pose, focus, lighting conditions). Rendered into Imodel(x, y), this estimated head is already
roughly aligned with the input face in Iinput(x, y). A binomial 3-tap
low-pass ﬁlter is applied subsequently on the rendered image Imodel to
simulate different levels of blur. The 3DMM makes it easy to know

from blurred
no compensation
Fig. 7. For explanation see caption of Fig. 6.

from original

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

(a)

(b)

55

(c)

(d)

Fig. 8. Example input image with estimated blur level (by self-adapting blur estimation) of 4 (b), 12 (c), 34 (d) and the unblurred original image (a).

where facial regions with signiﬁcant edges in Imodel are, for example in
the eye and mouth region. The blur metric is calculated only in these regions of Imodel and Iinput.
To estimate the appropriate blur level b, the low-pass ﬁltering is applied sequentially b times to Imodel until the blur measures on both images are approximately equal.
Due to the limited texture resolution of the 3D scanner and residual
errors in the calculation of correspondences when the 3DMM was built,
the textures of our 3DMM faces tend to be blurry. For the self-adapting
blur calculation described above, we apply a sharpening operator to the
texture of the estimate head (Fig. 5). The sharpened texture is created
by using an unsharp masking ﬁlter [28].
Still, the resolution of the estimate head is limited. If the blur measure of the input image is equal to or even smaller than the blur measure
of Imodel, no blur compensation is done by the analysis-by-synthesis process: b = 0 and all Δj = 0.

and comparable to the reconstructions from the unblurred input
images.
For an objective evaluation of the reconstruction quality we use the
Mahalanobis distance. This distance compares 3D face reconstructions
in PCA space, taking into account how much the faces vary in different
directions in face space in terms of shape or texture.
As described in Section 2, the shapes and textures of 3D face reconstructions are described by linear combinations of eigenvectors (see
Eqs. (6) and (7)). The Mahalanobis distance calculates the difference
!
!
of two reconstructions by using the coefﬁcients α and β of the linear
combinations. Hence
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
À
Á2
m

 u
uX
α re f ; k −α sample; k
!
!
ds α re f ; α sample ¼ t
σ 2s; k
k¼0

ð19Þ

describes the Mahalanobis distance for shape and
3.3. Results of blur-compensated image reconstruction
To evaluate the reconstruction quality of the proposed non-local
3DMM ﬁtting algorithm with the unmodiﬁed method [23], input images with different blur levels are reconstructed and compared with
the reconstructions from unmodiﬁed input images. The Multi-PIE database [29] is used for this task, since it contains many different views of a
large number of persons. For evaluation, all images are blurred by ﬁltering and downsampling using a Gaussian image pyramid with different
levels [30] and expanding to the original image size. In this way, the
lowpass ﬁlter used to generate the input images differs from the binomial ﬁlter kernel used in the analysis-by-synthesis method.
If G0 is the unﬁltered image of the image pyramid, we measured the
blur intensity of the expanded images of levels G1, G2 and G3 [30] with
the method described in Section 3.2. The average of the estimated blur
levels for G1 is 4, for G2 is 12 and for G3 is 34. For a consistent terminology, we use this metric to describe the blur intensity of the input images
instead of the stage of the image pyramid.
Figs. 6 and 7 show examples of reconstructed 3D faces. The input images for the ﬁrst row of Fig. 6 are shown in Fig. 8c. The other examples of
Figs. 6 and 7 are also reconstructed from an estimated average blur level
of 12.
Fig. 6 illustrates the quality improvement with respect to 3D shape.
With no blur compensation, the reconstructions show obvious shape artifacts compared to the original and the blur-compensated reconstructions, for example a strong bulge above the eyebrows (ﬁrst row),
small chin, big lips and missing hump on the nose (second row).
Also the quality of the texture estimate is improved substantially by
the proposed method (Fig. 7). Obvious enhancements are visible especially in the eye regions (shape and color of pupils and eyebrows).
During a manual evaluation of about 500 3D reconstructions, no evidence was found that the proposed method generates reconstructions
with lower quality than the unmodiﬁed algorithm. In contrast, the quality of shape and texture was improved in most cases (as in Figs. 6 and 7)

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
À
Á2
m
!
 u
uX
βre f ; k −βsample; k
!
dt β re f ; β sample ¼ t
σ 2t; k
k¼0

ð20Þ

!
!
the distance for texture coefﬁcients. Here α re f and β re f are the shape
and texture coefﬁcients of the reference reconstruction (unblurred
!
!
image) and α sample and β sample are the coefﬁcients of the reconstruction
from blurred images. σs, i and σt, i are the standard deviations for shape
and texture from PCA calculation along every principal component. For
evaluation, we reconstructed all 249 images of persons in the ﬁrst session of the Multi-PIE database. This dataset includes faces from different
ethnic groups (Afro-American, Asian, Caucasian, Indian), and some are
wearing glasses. We reconstructed all persons from the original and
blurred images with 3 different blur levels (average estimated blur
level of 4, 12 and 34, see Section 3.2) without and with blur compensation incorporated by our model. Fig. 8 shows an example of the input
images.

estimated blur level

4

12

34

shape blur

6.2164

9.9793

13.2301

shape deblur

5.8823

8.5486

11.452

texture blur

4.7536

8.415

11.7068

texture deblur

4.4671

7.8392

11.0694

Fig. 9. The table illustrates the average Mahalanobis distance between the 3D reconstruction of the unblurred input images and the reconstructions from blurred input images
with and without blur compensation by the 3DMM. The evaluation was done for 3 levels
of image blur. The data indicates that blur compensation reduces the distance towards the
reference reconstruction (from unblurred images) for both shape and texture.

56

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

ground truth

blurred input

synthetic image

reprojected output

Fig. 10. Ground truth input image Iinput, blurred input image with estimated blur level of 12, reprojected reconstruction of Imodel without texture extraction (third) and reprojected and
deblurred reconstruction (IE) with enhanced texture extraction (Section 4.1).

(a) ground truth

(b) blurred input

(c) reprojected output

Fig. 11. Input image (left), blurred input image with estimated blur level of 12 (middle) and enhanced image IE (right) which is then used for texture extraction (Section 4.1). This image
pair illustrates the potential of our algorithm for image enhancement. To the best of our knowledge this is the ﬁrst face hallucination for side views.

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

57

For higher blur levels (estimated blur level n = 12, Fig. 8c) the blur
compensation becomes more visible. Thus for shape, in 95.58 %
(238 of 249 cases) the blur-compensated reconstructions are closer
to the reconstruction of the unblurred input image. Also the texture
coefﬁcients are in 81.92 % (204 of 249 cases) closer to the unblurred
reconstruction. In input images with strong blur (estimated blur
level n = 34, Fig. 8d) we decrease the Mahalanobis distance
concerning shape in 84.73 % (211 cases) and in 71.88 % (179 cases)
concerning texture.
4. Texture extraction

(a)

(b)

Fig. 12. The schematic a shows the result of Tdiff (u, v) based on Eq. (23) for the complete
facial texture map, while b is a magniﬁcation of the eye region. Note that both ﬁgures
are artiﬁcially intensiﬁed to enhance visibility.

The shape and texture coefﬁcients of the unblurred reconstruction
are used as reference, and we compare these to the coefﬁcients of the
uncompensated reconstruction and the proposed blur-compensated reconstruction using Mahalanobis distance.
The average Mahalanobis distance of all 249 reconstructions for
the 3 different blur levels for texture and shape is shown in Fig. 9.
In all cases the average distance between the blurred and the ground
truth image decreases with increasing blur levels. Especially the
shape reconstructions get closer to the unblurred input image.
Even in slightly blurred input images (estimated blur level n = 4,
Fig. 8b), the Mahalanobis distance decreases in 70.28 % (175 of 249
cases) of the 3D reconstruction concerning the shape coefﬁcients
and in 73.89 % (184 of 249 cases) concerning the texture coefﬁcients.

The result of the model ﬁtting algorithm is a textured 3D model of
the face. The texture vector T contains one set of RGB color values per
vertex, and it is the optimal linear combination of database vectors
(Eq. (4)).
However, it is desirable to have true u, v texture mapping with highresolution textures, for the following reasons:
• The resolution with texture vectors that have one color per vertex is
relatively low. The deﬁnition of these vectors was adapted to the resolution of the database scans. After ﬁtting the model to high quality
photos, details such as eyelashes can be captured only with a high resolution u, v texture.
• The linear combination (4) has only a limited number of degrees
of freedom and it can only reproduce structures that are found
in at least one of the database faces. Details (eyelashes, birthmarks) cannot be reproduced with the model-based approach
directly.
• Even on blurred images, there may be individual characteristics
on a low spatial frequency domain that are not in the degrees of

(a)

(b)

(c)

(d)

Fig. 13. In the left part of a to d the initial photo of four candidates from the high-resolution facial database [29] is shown. Whereas the right part is a rendering of the reconstructed 3D face
of the person in the input image (see Fig. 1a) using the resulting texture Ti, L → H(u, v) from the high-resolution texture transfer. While the Mahalanobis distance for a and b is minimal and
thus convincing facial details are added. 12 and 12 depict candidates where the distance is maximal. Thus the associated difference texture Tdiff (u, v) (see Eq. (23)) does not add proper
details to the 3D reconstruction: Facial hair is added where non is present in the input image and furthermore in d the eyebrows do not match.

58

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

(a)

(b)
Fig. 14. a shows an example of occasionally appearing artifacts in the area of the iris.
Whereas b is the result of the application of a warped difference texture Tdiff (u, v) based
on the optical ﬂow calculations between the input texture Ti, L(u, v) and the corresponding
high-resolution texture Tj, H(u, v) from the database. Tests showed an artiﬁcial sharpening
of Ti, L(u, v) before estimating the optical ﬂow enhances the ﬁnal results.

freedom of the 3DMM, for example larger blemishes, or facial
hair.

The linear combination of texture vectors cannot capture these individual details from the photo, so the following texture extraction procedure [7] maps them to the model.
Let TTE(u, v) be an RGB texture for the facial mesh. The resolution of
TTE may be any value that is appropriate to capture the details seen in
the image. For each vertex j, we deﬁne a texture coordinate uj, vj.
In order to extract TTE from an image, we rely on the fact that the
image position xj, yj of each vertex j is known after ﬁtting. The corners
of a triangle k of the mesh have x, y coordinates in image space and
u, v coordinates in texture space. For each texel (integer pair u, v) in
the texture triangle, we calculate the barycentric coordinates, and use
the same coordinates to calculate the corresponding point x, y in the triangle in image space. TTE(u, v) is then obtained by sampling the image in
the non-integer position x, y using bilinear interpolation between 4 adjacent pixels.

(a)

(b)

With the procedure described so far, all illumination effects in
I(x, y), including specularities and shadows, would simply be
mapped on the surface, so new illuminations and new poses could
not be rendered correctly. Illumination-corrected texture extraction
[7] solves this problem by inverting the effects of lighting in each
texel. After ﬁtting, the pose and the illumination of the face are
known, since pose and illumination are among the parameters that
are optimized. Also, the surface normal of each point is known.
Given I(x, y), the algorithm inverts the effect of color contrast, subtracts the specular reﬂection using the surface normal, and ﬁnally inverts the effect of Lambertian shading. As a result, the algorithm
outputs the reﬂectance values in each color channel and stores
them in T TE(u, v). Subsequent rendering will then again multiply
the reﬂectance with the Lambertian shading, add specularities and
change the color contrast to obtain a realistic view in new rendering
conditions. Note that the algorithm will exactly reproduce the input
image I(x, y) when the textured face is rendered with the estimated
pose and lighting of the photo. However, texture extraction from a
low resolution input image would remove the details introduced
by the 3DMM, so we need a modiﬁed procedure.
4.1. Model-based texture enhancement from low resolution images
As mentioned in this Section individual characteristics can occur on a
low spatial frequency domain that are not in the degrees of freedom of
the 3DMM. We use a modiﬁed texture extraction to improve the texture
estimated by the 3DMM (vector T in Eq. (4)).
In the ﬁrst step, we calculate an enhanced input image
IE ðx; yÞ ¼ IInput ðx; yÞ þ ðIModel ðx; yÞ−φðIModel ðx; yÞÞÞ

ð21Þ

that contains all the image details that are in IModel but are washed out in
φ(IModel). These are texture details, for example the iris or the sharp edge
of the eyebrows, but also details due to shading, for example at the nose.
Both are missing in IInput and φ(IModel). Examples of IE are shown in
Figs. 10 and 11.
With this enhanced input image, we perform illumination-corrected
texture extraction as described in the previous section:
IE ðx; yÞ↦T ETE ðu; vÞ:

ð22Þ

Note that the illumination correction inverts the effects of shading, so the color of the nose will be constant skin color again, as
desired.

(c)

(d)

Fig. 15. a shows the blurred input image (top) and the ﬁnal result of the 3D reconstruction after applying our new approach (bottom). b to d are zoomed in views to the eye and mouth
regions of the reconstructed 3D face model. While in b only the original texture was extracted from the photo, in c the deblurring described in Section 4.1 was applied and in d the highresolution texture transfer of Section 5 was added to the results of column c.

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

59

Image with occlusion

Ground truth

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 16. Example of an occlusion due to hair covering parts of a face: In a parts of the face are occluded and b depicts the unoccluded face with identical viewing angle and lighting conditions. The 3D reconstructed shape and texture due to linear combination are shown in the second row. c is reconstructed without occlusion handling and d with our proposed method.
For comparison the reconstruction from the unoccluded input image is depicted in e. The third row shows the result of the 3D reconstruction with texture extraction: f without occlusion
handling, g with proposed method, h the ground truth.

(a)

(b)

(c)

Fig. 17. b shows an example of an occluder image mask. The binary mask has the same size as the input image (a). A pixel value of 0 denotes that the current pixel is occluded and a value of
1 marks a non-occluded pixel in the input image. For comparison c shows an overlay of the occluder image mask (in gray) in the input image.

60

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

5. High-resolution texture transfer
Although the methods described in the previous sections already
provide substantial enhancements to blurred input images, they cannot
go beyond the level of detail represented in the 3DMM (see Section 2).
The ﬁne structures of hair or skin, including pores and slight dermal irregularities, are not recovered.
To cope with this drawback, we propose an additional method
which adds details above the spatial frequencies captured by the
3DMM. These facial details are derived from a database of highresolution photos of faces and are transferred to new individuals
during a postprocessing step of the 3DMM ﬁtting. The basic idea is
to ﬁnd one matching face (see Section 5.2) in a high-resolution
photo collection for each low-resolution input image and to use it
as a basis for the transfer of skin features. In Section 5.3 a modiﬁcation to this approach is described, which does not search for an entire
face that matches the one in the input image, but to ﬁnd matching facial regions and thus combines the skin information of different
faces. Currently our high-resolution database contains 221 individual faces (79 female and 142 male persons) from the Multi-PIE face
database [29].
While Scherbaum et al. [31] applied the makeup of one person to
the face of another one, the fundamental idea of our approach is to
transfer facial details, so that the formerly low resolution texture
Ti, L(u, v) of person i gets transformed into a high-resolution texture.
In the following, the result of this process is denoted as Ti, L → H(u, v).

(a)

(b)

Fig. 18. Comparison between the occluder image mask and the occluder texture mask: a
shows a gray overlay of the occluder image mask on the input image. b shows the projection of the occluder texture mask of a into the texture map as a gray overlay.

face texture of the input image are compared with the coefﬁcients of
!
each low resolution sample β db of our facial database. For this compar! !
ison we calculate the Mahalanobis distance dt ð β in ; β db Þ (see Eq. (20))

5.1. Extraction of skin features
Before any facial details can be transferred, it is necessary to extract them from a low resolution texture Tj, L(u, v) and a corresponding high-resolution texture Tj, H(u, v) of a person j. We assume that
the facial details are equivalent to the difference between the low
and the high-resolution textures of the same person. These high spatial frequencies are stored in the texture Tdiff(u, v), so that
T diff ðu; vÞ ¼ T j; H ðu; vÞ−T j; L ðu; vÞ;

ð23Þ

where Tj, H (u, v) is the extracted texture from a high-resolution
image of person j, while Tj, L(u, v) originates from a low resolution
image of the same person j. Both images are stored in our facial database. Nevertheless, the low resolution image is mostly just a Gaussian blurred high-resolution image. The details of the texture
extraction of our 3DMM are described in Section 4.1.
Since the 3DMM guarantees a dense point-to-point correspondence
between each texel (u, v) of all ﬁtted individuals, which is a key prerequisite for a successful transfer of details, the difference texture Tdiff (u, v)
can be added to the extracted (low resolution) texture Ti, L(u, v) of any
other face to create a convincing high-resolution texture Ti, H(u, v).
Therefore the transferred texture Ti, L → H(u, v) can be simply written as
À
Á
T i; L→H ðu; vÞ ¼ T j; H ðu; vÞ−T j; L ðu; vÞ þ T i; L ðu; vÞ
¼ T diff ðu; vÞ þ T i; L ðu; vÞ:

(a)

(b)

(c)

(d)

(e)

ð24Þ

An example of a typical Tdiff(u, v) texture is shown in Fig. 12. While in
Fig. 12a the whole cylindrical texture mask is displayed, Fig. 12b allows
a more detailed look at the region of the right eye as well as its eyebrow.
Not only the patterns of the eyelashes and brows were extracted, but
also the high frequency differences of the dermal texture.
5.2. Search for matching faces
Since it is unusual that the extracted information of one face perfectly represents the missing details of another face, in many scenarios it is
necessary to locate pairs of textures that are similar to each other.
!
Therefore the PCA coefﬁcients β in which belong to the low resolution

Fig. 19. Example of occluded texture reconstruction methods. a shows an input image
with an artiﬁcially generated occlusion. The corresponding 3D model with occluded texture reconstruction by using the calculated texture from the 3DMM (ﬁrst method) is
depicted in b and the texture in c. d and e shows the texture reconstruction with
mirroring.

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

61

5.3. Search for matching facial regions

(a)

(b)

Sometimes it is challenging to ﬁnd a face where all facial regions
(eyes, nose, mouth, ears, etc.) are similar enough at once to be useful
for the texture transfer. For example there may be strains of hair on
the forehead of some individuals from the image database while there
is no hair on the forehead of the person in the input image. Therefore
we implemented an approach to assemble the details of different facial
regions by looking for best matching areas and combine the details of
several individuals in the process of the high-resolution texture transfer.
This is done by computing the Mahalanobis distance not only for the
whole face but also for each region separately. To avoid visible transitions at the border of the different regions a texture blending is applied.
Searching for matching facial regions instead of looking for similar
looking faces provides an additional advantage: If we compare whole
faces like in Section 5.2 local differences were sometimes overruled by
global similarities. Sometimes this led to errors like the transfer of
beard stubble into a female face. By labeling the high-resolution images
based on the person's gender, we could avoid this kind of artifacts to
some extent. Nevertheless, applying a region based approach to detect
the best matches provides a solution to this problem without the necessity of labeling the images of the database.
5.4. Enhancing the image quality by utilizing optical ﬂow and warping

(c)

(d)

Fig. 20. Examples of visible seams along transitions between extracted and reconstructed
texture: a and b show a close-up of the 3D reconstruction in Fig. 19b and the texture in 19c.
c and d are the corresponding close-ups to Fig. 19d and e. A seam around the left eye is visible in both reconstruction methods.

!
!
between β in and every available β db and transfer the facial details from
that individual, where the computed distance is minimal. Quantifying
the distances in PCA space to detect the nearest neighbor instead of
computing the absolute differences ensures that the most signiﬁcant
features are regarded for the similarity measurement.
To preclude that similar eye, nose and mouth regions between a
female and a male face lead to a transfer of beard stubble into the female face, all images in the database are labeled, so that only the
details from individuals of the same gender are taken into
consideration during the distance measurements and ﬁnally for the
transfer of facial details. Please note that the region based approach
presented in Section 5.3 does not presuppose a labeling of the
high-resolution images of our database.
The best ﬁtting candidates based on the computed Mahalanobis
distance between the low resolution input image Ti, L (u, v) and
every face in the database are shown in Fig. 13a and b while those
candidates which differ a lot from the input image are shown in
Fig. 13c and d. The left part of each image in Fig. 13 shows the original
photo of each candidate. The ﬁnal result Ti, L → H(u, v) after transferring the facial details by adding the difference texture Tdiff(u, v) to the
input image is depicted in the right part.
Even though the transfer of details in Fig. 13c and d for images
with a high Mahalanobis distance adds unwanted facial features to
the input image, the strong point-to-point correspondence which is
guaranteed by the 3DMM (see Section 2) prevents completely
unconvincing results. As can be seen for example in Fig. 13c, where
the photo on the left, which is used as high-resolution image to
extract the facial details, differs a lot from the low resolution input
image in Fig. 1a.

Nevertheless, artifacts may appear if a global similarity of two segments is co-occurring with strong local differences. This was occasionally observed in the region of the eyes or the eyebrows. An example of this
artifact is shown in the close-up of the iris in Fig. 14a, where the artiﬁcially added details look like misplaced contact lenses.
To handle these remaining imperfections we apply an image
warping to the difference texture Tdiff (u, v) so that it ﬁts better to the
low resolution input texture Ti, L(u, v). As a basis for this transformation

(a)

(b)

(c)

(d)

Fig. 21. Poisson image editing example: a and b show the reconstruction from Figs. 19b, c
and d the reconstruction of Fig. 19d after stitching with Poisson image editing algorithm.

62

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

the optical ﬂow between an artiﬁcially sharpened image of Ti, L(u, v) and
the corresponding high-resolution texture Tj,H(u, v) from the database is
estimated. The result after the application of the warped difference texture is shown in Fig. 14b.
5.5. Results of the high-resolution texture transfer
The result of the described transfer of texture details is shown in
Fig. 15. While we obtain already strong enhancements with the model
based deblurring (see Section 4.1) as is presented in Fig. 15c, the transfer of details improves the visual quality even more (see Fig. 15d).
Although the transferred details for the small hairs of the eyebrow
do not match perfectly to the blurred eyebrows of the input image,
our algorithm provides a visually convincing result as can be seen at
the top of Fig. 15d. Furthermore the ﬁne skin structure of the lips is
well transferred into the formerly blurred input image (see Fig. 15d).
Nevertheless we would like to point out that it is not invariably the
case that all features in the face are perfectly restored. For example, in
Fig. 14a the recovery of the iris is not as good as in Fig. 15d, even though

the corresponding Mahalanobis distances for the eye regions do not
differ signiﬁcantly from each other. Even after the application of the
optical ﬂow and image warping operation (see Fig. 14b), which has
been described in Section 5.4, the reconstruction in Fig. 15d is superior.
Accordingly a future improvement might be to reduce the size of the
different regions and to combine all regions in a tree structure from coarse
to ﬁne. This should allow our approach to adapt better to even small
differences between the facial textures.
6. Reconstruction of occluded regions
Another common problem that inﬂuences the quality of the reconstructed 3D models is occlusion of facial regions, for example due to
sunglasses, hats, scarfs, beards or hair covering parts of the face. The
presence of such facial occlusions is quite common in real-world applications and lead to visible artifacts in the 3D reconstruction (see Fig. 16).
They affect the reconstruction and the texture extraction step.
Without explicit handling of occlusions, the ﬁtting algorithm tries to
simulate the color of occluded areas, which differ in most cases

(a) ground truth

(b) occluded input

(c) reprojection

(d) 3D face

(e) ground truth

(f) occluded input

(g) reprojection

(h) 3D face

(i) ground truth

(j) occluded input

(k) reprojection

(l) 3D face

Fig. 22. Results with artiﬁcial occluded regions.

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

signiﬁcantly from skin-colors, by changing the lighting conditions and
choosing a linear combination of textures that reproduces the appearance of the occluder as good as possible (see Fig. 16c). Since lighting estimation is a crucial step for the 3DMM, the reconstructed texture from
input images with occluded facial regions have poor quality.
If relevant regions like eyes or mouth are completely or partially hidden, the estimation of the 3D shape may also be affected. In the postprocessing step of texture extraction, the occluding object is mapped directly on the 3D shape and generates wrong texture maps as a result. In
this paper, we take occlusions into account both in the ﬁtting algorithm
and in texture extraction.
The occluded regions of the face have to be detected and marked. In
our approach, this is done manually using a paintbrush tool. We have
experimented with automated methods to detect occlusions, but due
to the fact that the 3DMM can partly adapt to non-face pixels even
with relatively conservative settings (regularization), it remains unclear
how an automated criterion could distinguish what is part of a human
face and what is not [32].
The occluded pixels are stored in a binary occluder image mask that
has the same size as the input image. Fig. 17 shows an example. Note
that it is not a problem if pixels of the background are also marked as
occluders, because these are not considered in the cost function
(Eq. (13)).
6.1. Occlusion handling
The algorithm has to be initialized with the feature coordinates of at
least 5 feature points and – this is new – with an occluder image mask.
In the reconstruction algorithm the binary mask image is taken into account during the calculation of the image difference term EK (see
Eq. (13)).
The pixel position of each of these triangles is calculated by rendering and rasterizing the color value Imodel, k to pixel (xk, yk) (see
Section 2.3). Then, the visibility of a subset of triangles is tested consecutively. If the pixel position of one of these triangles is occluded, meaning that this pixel is marked in the occluder image mask, the current
triangle is rejected and another triangle is chosen randomly. This is

63

repeated until a non-occluded triangle is found. Since the visibility test
is done for all triangles of the subset, in the end the cost function in
Eq. (13) is determined only on visible triangles.
An explicit handling of occluded facial regions is also necessary for
the texture extraction algorithm, to prevent occluding objects from
being mapped on the reconstructed 3D shape.
As described in Section 4.1, the texture map assigns a 2D texture coordinate to every vertex in the 3D face reconstruction. In a ﬁrst step, we
determine the occluded regions on the texture map, by generating an
occluder texture mask for the texture map automatically from the
given occluder image mask. The occluder texture mask is very similar
to the occluder image mask explained above. The only difference is
that the occluder image mask describes which pixel of the input
image is occluded and the occluder texture mask describes whether a
vertex in the texture coordinate is visible or not (see Fig. 18 for comparison). To calculate an occluder texture mask, the rendering parameters
estimated by the ﬁtting algorithm are used to reproject the reconstructed 3D face into the original image space. Afterwards we have calculated
every pixel position (xn, yn) in the input image of each of the n = 75, 972
vertices. With the occluder image mask we can check the visibility of
every vertex and can mark it in the occluder texture mask.
After classiﬁcation whether a vertex in the 2D texture coordinate is
visible or not, it is necessary to ﬁll up the missing data with plausible
texture information. For this we propose two methods for occluded texture hallucination.
The ﬁrst algorithm uses the calculated texture from the 3DMM to ﬁll
up the missing texture data (see Fig. 19b and c). One drawback of this
method is the lower resolution compared to the extracted texture
from the input image. Especially in highly textured regions, such as
the eyes or the mouth, the decreased quality of the estimated texture
becomes salient. To avoid this, the second method utilizes the high symmetry of faces by mirroring texture from the visible half to the occluded
regions, if possible (Fig. 19d and e). In cases where it is not possible to
mirror texture the ﬁrst algorithm is used as fallback option.
One remaining problem in both occluded texture reconstruction
methods are visible seams along transitions between extracted and reconstructed texture (see Fig. 20). These artifacts originate from slightly

(a) occluded input

(b) reprojection

(c) 3D face

(d) occluded input

(e) reprojection

(f) 3D face

Fig. 23. Results with occluded regions.

64

M. Schumacher et al. / Image and Vision Computing 40 (2015) 49–64

different color, structures and overall brightness. We address this by
using Poisson image editing [33] for the reconstruction of the texture.
The principle of this gradient-based stitching algorithm is fusing the derivatives of signals instead of stitching the signals themselves. An advantage of this method is that the intensity differences between the
derivatives are relative, and not absolute as in the original signals.
Thus, differences in the amplitude of the two signals have no inﬂuence
in their gradient ﬁelds.
In our texture reconstruction approach, we use Poisson image
editing to stitch the extracted texture and the reconstruction of the occluded texture (either the calculated texture from the ﬁtting algorithm
(see Section 2) or the mirrored texture). See Fig. 21 for an example.

6.2. Results of occlusion handling
Figs. 22 and 23 show typical results of 3D face reconstructions with
hallucination from occluded input images. The examples consist of artiﬁcially generated occlusions and non-artiﬁcial occlusions (e.g. hair covering parts of the face or glasses). Unoccluded input images (ground
truth) are depicted in the ﬁrst row in Fig. 22. The second row shows
the related input images with artiﬁcially generated occlusions. 3D reconstructions of the occluded input images are shown in the last row
and the reprojected and relighted reconstructions are shown in the
third row. Fig. 23 illustrates reconstructions from image with natural occlusions. Shape and texture can be reconstructed despite occlusions. The
ﬁrst row shows input images with occlusions and the second row shows
the reprojected and relighted 3D reconstruction. The 3D reconstruction
is depicted in the third row.

7. Conclusion
We have presented an algorithm that can reconstruct detailed 3D
models of faces even from images with substantial blur or partial occlusions. It has the potential to allow 3D Morphable Models to be used for
face recognition even on low quality images, and it provides a very robust and general way for ﬁlling in missing details in images of faces.
The technical core of our approach is an explicit treatment of image
blur or other non-local image-space operators in the analysis-bysynthesis algorithm.
Our results on model-based estimation of facial details, and the
transfer of details on the highest level of resolution (eyelashes, pores)
pave the way for a very general tool that helps to enhance existing
low-quality image material.
As we have pointed out, we cannot claim to predict the true appearance of facial details, but the algorithm makes an educated
guess based on the correlations between features in faces. These
correlations are captured by the 3DMM due to the fact that it uses
global face vectors of entire faces, so constraints on some of the vector components (vertices of the face mesh) will inﬂuence the shape
and texture of the other vertices. With this statistical inference, the
algorithm provides a useful method to reconstruct details beyond
the visible structures in the image. Previous work on 3D depth
estimation [8] indicates that correlation-based inferences are plausible to human viewers if these viewers do not know the individual
person shown in the picture. In future work, we are planning to
test this hypothesis in psychological experiments. The motivation
behind these future experiments is not only to evaluate our
algorithm, but also to investigate the mechanisms on how the
human visual system hallucinates faces.

References
[1] D. Kundur, D. Hatzinakos, Blind image deconvolution, IEEE Signal Process. Mag. 13
(3) (1996) 43–64.
[2] R. Fergus, B. Singh, A. Hertzmann, S.T. Roweis, W.T. Freeman, Removing camera
shake from a single photograph, ACM Trans. Graph. 25 (2006) 787–794.
[3] Z. Hu, M.-H. Yang, Good Regions to Deblur, European Conference on Computer Vision (ECCV) 2012, pp. 59–72.
[4] B. Bascle, A. Blake, A. Zisserman, Motion Deblurring and Super-resolution from an
Image Sequence, European Conference on Computer Vision (ECCV) 1996,
pp. 573–582.
[5] R.R. Schultz, R.L. Stevenson, Extraction of high-resolution frames from video sequences, IEEE Trans. Image Process. 5 (1996) 996–1011.
[6] M. Bertalmio, G. Sapiro, V. Caselles, C. Ballester, Image Inpainting, ACM Transactions
on Graphics (SIGGRAPH) 2000, pp. 417–424.
[7] V. Blanz, T. Vetter, A morphable model for the synthesis of 3D faces, ACM Transactions on Graphics (SIGGRAPH) 1999, pp. 187–194.
[8] M. Schumacher, V. Blanz, Which facial proﬁle do humans expect after seeing a frontal view? A comparison with a linear face model, ACM Trans. Appl. Percept. 9 (3)
(2012) 11.
[9] S. Baker, T. Kanade, Hallucinating Faces, International Conference on Automatic Face
and Gesture Recognition (FG) 2000, pp. 83–88.
[10] J.H. Liang, Y. Lai, W.S. Zheng, Z. Cai, A Survey of Face Hallucination, Biometric Recognition — 7th Chinese Conference (CCBR) 2012, pp. 83–93.
[11] G. Edwards, T. Cootes, C. Taylor, Face Recognition Using Active Appearance Models,
in: Neumann Burkhardt (Ed.),European Conference on Computer Vision (ECCV)
1998, pp. 581–595.
[12] C. Liu, H.-Y. Shum, C.-S. Zhang, A two-step approach to hallucinating faces: global
parametric model and local nonparametric model, IEEE Computer Vision and Pattern Recognition (CVPR) 2001, pp. 192–198.
[13] C. Liu, H.-Y. Shum, W.T. Freeman, Face hallucination: theory and practice, Int. J.
Comput. Vis. 75 (1) (2007) 115–134.
[14] B. Li, H. Chang, S. Shan, X. Chen, W. Gao, Hallucinating Facial Images and Features,
International Conference on Pattern Recognition (ICPR) 2008, pp. 1–4.
[15] G. Dedeoglu, T. Kanade, J. August, High-Zoom Video Hallucination by Exploiting
Spatio-Temporal Regularities, International Conference on Pattern Recognition
(ICPR), vol. 2 2004, pp. 151–158.
[16] A. Lanitis, C. Taylor, T. Cootes, Automatic interpretation and coding of face images
using ﬂexible models, IEEE Trans. Pattern Anal. Mach. Intell. 19 (7) (1997) 743–756.
[17] B.W. Hwang, S.W. Lee, Reconstruction of partially damaged face images based on a
morphable face model, IEEE Trans. Pattern Anal. Mach. Intell. 25 (3) (2003)
365–372.
[18] S.-W. Lee, J.-S. Park, B.-W. Hwang, How Can We Reconstruct Facial Image from Partially Occluded or Low-Resolution One? Sinobiometrics 2004, pp. 386–399.
[19] C.-T. Tu, J.-J.J. Lien, Facial Occlusion Reconstruction: Recovering Both the Global
Structure and the Local Detailed Texture Components, Proceedings of the 2nd Pacific Rim Conference on Advances in Image and Video Technology, PSIVT 2007,
pp. 141–151.
[20] U. Söderström, H. Li, Asymmetrical Principal Component Analysis: Theory and Its
Applications to Facial Video Coding, Effective Video Coding for Multimedia Applications, InTech Open 2011, pp. 95–110.
[21] Y. Li, X. Lin, Face hallucination with pose variation, International Conference on Automatic Face and Gesture Recognition (FG) 2004, pp. 723–728.
[22] M.F. Tappen, C. Liu, A Bayesian Approach to Alignment-based Image Hallucination,
European Conference on Computer Vision (ECCV) 2012, pp. 236–249.
[23] V. Blanz, T. Vetter, Face recognition based on ﬁtting a 3D morphable model, IEEE
Trans. Pattern Anal. Mach. Intell. 25 (9) (2003) 1063–1074.
[24] G. Pan, S. Han, Z. Wu, Hallucinating 3D Facial Shapes, IEEE Computer Vision and Pattern Recognition (CVPR) 2008, pp. 1–8.
[25] G. Dedeoglu, S. Baker, T. Kanade, Resolution-Aware Fitting of Active Appearance
Models to Low Resolution Images, European Conference on Computer Vision
(ECCV) 2006, pp. 83–97.
[26] P. Breuer, K.I. Kim, W. Kienzle, B. Schölkopf, V. Blanz, Automatic 3D face reconstruction from single images or video, International Conference on Automatic Face and
Gesture Recognition (FG) 2008, pp. 1–8.
[27] P. Marziliano, F. Dufaux, S. Winkler, T. Ebrahimi, Perceptual blur and ringing metrics:
application to JPEG2000, Signal Process. Image Commun. 19 (2) (2004) 163–172.
[28] L. Levi, Unsharp masking and related image enhancement techniques, Comput.
Graphics Image Process. 3 (2) (1974) 163–177.
[29] R. Gross, I. Matthews, J. Cohn, T. Kanade, S. Baker, Multi-PIE, Image Vis. Comput. 28
(5) (2010) 807–813.
[30] E.H. Adelson, C.H. Anderson, J.R. Bergen, P.J. Burt, J.M. Ogden, Pyramid methods in
image processing, RCA Eng. 29 (6) (1984) 33–41.
[31] K. Scherbaum, T. Ritschel, M. Hullin, T. Thormählen, V. Blanz, H.-P. Seidel, Computersuggested facial makeup, Comput. Graphics Forum 30 (2) (2011) 485–492.
[32] P. Breuer, Automatic Model-based Face Reconstruction and Recognition, University
of Siegen, 2010. (Dissertation).
[33] P. Pérez, M. Gangnet, A. Blake, Poisson Image Editing, ACM Transactions on Graphics
(SIGGRAPH) 2003, pp. 313–318.

