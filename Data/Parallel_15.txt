Parallel Computing 48 (2015) 125–142

Contents lists available at ScienceDirect

Parallel Computing
journal homepage: www.elsevier.com/locate/parco

Intel Cilk Plus for complex parallel algorithms: “Enormous Fast
Fourier Transforms” (EFFT) library
Ryo Asai∗, Andrey Vladimirov
Colfax International, 750 Palomar Ave, Sunnyvale, CA 94085, United States

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 19 September 2014
Revised 2 May 2015
Accepted 19 May 2015
Available online 3 June 2015

In this paper we demonstrate the methodology for parallelizing the computation of large
one-dimensional discrete fast Fourier transforms (DFFTs) on multi-core Intel Xeon processors. DFFTs based on the recursive Cooley–Tukey method have to control cache utilization,
memory bandwidth and vector hardware usage, and at the same time scale across multiple
threads or compute nodes. Our method builds on a single-threaded Intel Math Kernel Library
(MKL) implementation of real-to-complex DFFT, and uses the Intel Cilk Plus framework for
thread parallelism. We demonstrate the ability of Intel Cilk Plus to handle parallel recursion
with nested loop-centric parallelism without tuning the code to the number of cores or cache
metrics. The result of our work is a library called EFFT that performs 1D DFTs of size 2N for
N ≥ 21 faster than the corresponding Intel MKL parallel DFT implementation by up to 1.5 × ,
and faster than FFTW by up to 2.5x. The code of EFFT is available for free download under the
GPLv3 license.

Keywords:
Fourier transform
DFT
Multi-threading
Intel Cilk Plus
Nested parallelism
Bandwidth-bound

This work provides a new eﬃcient DFFT implementation, and at the same time demonstrates
an educational example of how computer science problems with complex parallel patterns
can be optimized for high performance using the Intel Cilk Plus framework.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
The discrete Fourier transform (DFT) is a universal tool in science and technology. From image processing [1] to ﬁnding
distant astronomical objects [2], uses for the Fourier transform are countless and diverse. Research efforts into eﬃcient numerical
Fourier transform algorithms, dubbed discrete fast Fourier transforms (DFFTs), are numerous (e.g., [3]) and have led to great
improvements in the performance of Fourier transforms in a wide range of conﬁgurations on many architectures (e.g., [4,5]).
The deﬁnition of one-dimensional discrete Fourier transform (1D DFT) is given by the expression
N−1

Fk =

xn · exp −i
n=0

2π kn
,
N

(1)

where Fk is the complex kth coeﬃcient in the transform, xn is the nth data point, and N is the length of the transformed sequence.
If we were to construct a computer algorithm for DFT calculation using Equation (1) directly, the number of arithmetic operations (and memory accesses) would scale as O(N2 ). This quickly becomes ineffective even for small N, and for our domain of

∗

Corresponding author. Tel.: +1 408 730 2275; fax: +1 408 730 2274.
E-mail address: ryoasai408@gmail.com, ryo@colfax-intl.com (R. Asai).

http://dx.doi.org/10.1016/j.parco.2015.05.004
0167-8191/© 2015 Elsevier B.V. All rights reserved.

126

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

interest, N ࣡ 109 (see below), direct application of Equation (1) is completely useless. In order to improve the performance, practical implementations of DFTs use optimizations that weaken the complexity scaling and qualify the algorithm as DFFT (discrete
fast Fourier transform).
Most practical applications require either one-dimensional (1D), or multi-dimensional DFFTs with up to a few thousand data
values in each dimension. In this paper, we focus on the relatively exotic domain of 1D DFTs of data sets with a few billion data
values (i.e., N ࣡ 109 ). This domain of DFTs has application in astronomy (e.g., [2]), but because of its rare occurrence in other
ﬁelds, existing tools for performing such DFTs are not tuned for multi-core CPUs and perform sub-optimally.
In this paper we describe a new eﬃcient tool for such large 1D DFFTs in shared memory. We call this tool the EFFT library
(the acronym stands for “Enormous Fast Fourier Transforms”). This library is optimized for use on multi-core Intel processors.
EFFT uses the parallel framework Intel Cilk Plus in combination with the single-threaded implementation of small DFFTs from
the Intel Math Kernel Library (MKL) [8]. EFFT performs signiﬁcantly faster than the two leading DFFT libraries that we tested:
FFTW [9] and MKL with little increase in memory footprint and with no loss in accuracy. However, unlike those libraries, EFFT
supports only 1D transforms of size N = M × 2s , and is optimized only for very large values of N.
In addition to the practical importance of the EFFT library, its development process has great educational value. That is because
the parallel DFFT algorithm is an excellent example of a problem with complex pattern of parallelism. As such, it is an ideal
problem for demonstrating the strength of Intel Cilk Plus framework in effectively parallelizing diﬃcult problems with little
need for programmer input. Furthermore, our implementation of EFFT showcases multiple techniques for optimization for Intel
Xeon processors, which are effective for not only DFFT, but a diverse set of algorithms.
In Section 2 we discuss the sub-optimal performance of large multi-threaded 1D DFTs in MKL and the possible route to
resolution of this ineﬃciency through the use of the Cooley–Tukey recursive algorithm in combination with parallel framework
Intel Cilk Plus. In Section 3 we discuss the implementation of EFFT and the optimization consideration that allow us to achieve
better performance than MKL. Section 4 contains brief instructions on using the EFFT library, and Section 5 reports the tuning
methodology and performance benchmarks for a range of array sizes on a multi-core processor. Discussion in Section 6 touches
on the utility of Intel Cilk Plus for complex parallelism and on the applicability of our approach to the Intel Xeon Phi coprocessor
architecture.
The result of our work, the EFFT library, is available for free download at the Colfax Research web site [10] under the GPLv3
license.
2. Parallelizing the 1D DFFT calculation
2.1. Motivation behind EFFT: Parallel Intel MKL DFT performance
Intel MKL is a highly optimized, industry-leading mathematics function library. MKL supports DFTs in serial, multi-threaded
and cluster implementations. For a brief introduction refer to Appendix A. As we are mostly interested in multi-threaded 1D large
transforms, in this section we benchmark the MKL performance for this problem domain. Note that all reported benchmarks for
MKL DFT are of real-to-complex transforms.
Assuming that the user application must process multiple DFTs, there are two approaches to parallelizing the application:
1. Implementing multiple single-threaded Intel MKL DFTs. In this approach, each thread has its own DFTI_DESCRIPTOR_HANDLE.
The parallelization is done by calling multiple instances of single-threaded DftiComputeForward() from multiple user
threads.
2. Taking advantage of the internal threading functionality of the Intel MKL DFT. In this approach, only one DFTI_DESCRIPTOR_
HANDLE is created, and the value of parameter DFTI_NUMBER_OF_USER_THREADS is set to the desired number of threads. In this
case, the parallelization is done internally by MKL, and multiple cores work in parallel on a single transform.
The ﬁrst approach is applicable when (a) all DFTs needed by the application are independent from each other, and therefore
can be computed concurrently, and (b) the problem size is small enough so that T data arrays can ﬁt in memory, where T is
the number of threads used by the application. The second approach has the advantage of using far less memory, because only
one data array must be stored in memory at any given time. This approach also allows one to compute each DFT as fast as
possible in terms of wall clock time, which is useful when the result of one DFT determines the subsequent operations in the
parallel application.
Fig. 1 shows the performance of the two approaches as a function of the number of threads for DFTs of size N = 228 . The
red solid line with square markers reports the performance of multiple single-threaded DFTs, and the dotted blue line with
circular markers shows the performance of a single multi-threaded DFT. In both cases, the total number of threads is set using
the OMP_NUM_THREADS environment variable. Furthermore in the former case, the number of threads for individual DFT is set to
one using mkl_set_num_threads() as well as by setting DFTI_NUMBER_OF_USER_THREADS in the descriptor handle.
Based on performance alone, multiple single-threaded DFTs approach has a clear advantage in scalability over the internal
threading approach. However, running multiple single-threaded DFTs is entirely impractical in the domain of large transforms
due to its enormous memory requirement. The bottom panel of Fig. 1 shows the memory usage as a function of number of
threads for carrying out Intel MKL DFT. With multiple concurrent single-threaded DFTs running, the required memory increases
linearly with the number of threads T. Thus, in processors with high core count and (N ࣡ 109 ), the memory requirement for the
multiple single-threaded DFTs approach may exceed the practical amount of usable RAM.

Memory
(GiB)

Performance
(GFLOP/s)

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

60
50
40
30
20
10
0
100
80
60
40
20
0

127

T concurrent single-threaded MKL DFTs
One parallel MKL DFT with T threads
Transform size n=228

0

5

10
15
Threads, T

20

25

Fig. 1. Top: Performance of two parallelization approaches as the function of the number of threads. Bottom: Memory usage in the two approaches. (The system
conﬁguration is the same as in Section 5.1). (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this
article.)

EFFT resolves this diﬃculty by partially applying the FFT algorithm called Cooley–Tukey (CT) method to decompose the large
1-D FFT into many small FFTs, which are then processed in parallel by the single-threaded FFT algorithm. In that sense the EFFT
algorithm is not exactly an FFT algorithm, but a pre-processing and post-processing algorithm for large FFTs that converts a suboptimal workload, one large multi-threaded FFT, to an optimal workload, many smaller single-threaded FFT. As our benchmarks
in Section 5 will show, performance gain from switching to an optimal workload outweighs the overhead of the preprocessing
and postprocessing.
2.2. Radix-2 Cooley–Tukey algorithm
The CT algorithm originally presented in [11] is the archetype of most DFFT algorithms and is widely used and studied in
order to improve the performance of Fourier transforms. In EFFT, we only use the Radix-2 version, which is a recursive algorithm
that breaks down a transform of size N into two smaller transforms of size N/2 at each step. Note that this algorithm requires
that the size N = M × 2s , where s is the number of times we intend to apply the recursive splitting.
The core idea of the Radix-2 CT algorithm is to re-write Equation (1) as a sum of the elements at even values of n and odd
values of n:
N/2−1

Fk =

x2n · exp −i
n=0
N/2−1

=

x2n · exp −i
n=0

2π k(2n )
N
2π kn
N/2

≡ Ek + e−i2π k/N Ok .

N/2−1

+

x2n+1 · exp −i
n=0

+ e−i2π k/N

2π k(2n + 1 )
N

N/2−1

x2n+1 · exp −i
n=0

2π kn
N/2

≡
(2)

By comparing Equation (2) with (1), one can see that the ﬁrst sum is just the Fourier transform of the even-numbered elements and the second sum is the transform of the odd-numbered elements, multiplied by e−i2π k/N . The multiplication factor
e−i2π k/N is often referred to as the “twiddle factor”. Hereafter, we denote the even part of the transform as Ek and the odd transform as Ok .
Values of Ek and Ok need to be found only for 0 < k < N/2, because from the periodicity of the complex exponential functions
follows the symmetry

Fk = Ek + e−i2π k/N Ok ,

(3)

Fk+N/2 = Ek − e−i2π k/N Ok .

(4)

The CT algorithm repeats this decomposition recursively to produce smaller and smaller DFTs. The number of arithmetic operations (FLOPs) required to compute a transform of size N using the CT recursion is

FLOPs =

5
× N log2 N
2

(5)

This deﬁnition of FLOPs conﬂates the low-latency operations of ﬂoating-point addition and multiplication with the long-latency
transcendental arithmetic operations, and therefore cannot be easily related to the hardware performance metrics. However, for
simplicity, we use this scaling to express the performance of the implementation in terms of GFLOP/s.
For DFTs of real data (i.e., xn = x∗n ), the transform coeﬃcients also have the following symmetry;

Fk = F(∗N−k−1) .

(6)

128

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

Original Data

0 1 2 3 4 5 6 7
0 4 2 6 1 5 3 7
E

O

O

E
O

E
F

Process Name

Threads

Scatter

Multi-Threaded

Processing
(MKL DFFT)
Reassembly
Step 1
Reassembly
Step 2

Multiple
Single-Threaded
Multi-Threaded
Multi-Threaded

Output
Fig. 2. Workﬂow of EFFT with s = 2 splits and b = 2 = 4 bins. One “scatter” stage is performed with a multi-threaded loop. It is followed by b single-threaded
“processing” calls to MKL DFT function. Each pair of consecutive processed bins is “reassembled” by a multi-threaded loop. Multiple “processing” and “reassembly” calls may be running concurrently.
s

It is trivial to demonstrate that for all DFTs, F0 and FN/2 are both real. This means that only coeﬃcients F1 to FN/2−1 need to be
stored in their full complex form, and the coeﬃcients F0 and FN/2 can be stored as real values. This property allows the result of
a real data transform, Fk , to be stored in the same sized memory array as the input data xn .
2.3. Parallel algorithm of EFFT
For our purposes, the value of the CT algorithm is not only in the reduction of arithmetic complexity from O(N2 ) to O(Nlog N),
but also in its ability to decompose a large Fourier transform into a number of smaller Fourier transforms. By decomposing a
large transform into many smaller independent transforms, we can execute each of these transforms serially (i.e., with a single
processor core) using an existing highly optimized DFT implementation. Parallelism can be achieved by distributing the small
serial DFTs across multiple processor cores.
However, for the purposes of optimization, parallelism also has to be achieved in the procedures that precondition the data
layout for parallel DFTs. Parallelism is also necessary in the application of Equations (3) and (4). This makes the parallelization
much more challenging.
In the EFFT library we express the parallel DFFT as three stages:
I. “Scattering” of xn into multiple contiguous arrays (“bins”), which are thereafter transformed,
II. “Processing” (i.e., performing a serial DFT on) each of the small “bins”, and
III. “Reassembly”, i.e., the application of Equations (3) and (4) to the “bins” in order to produce the transform coeﬃcients Fk .
This workﬂow is depicted in Fig. 2.
Parallelizing the scattering phase across multiple CPU cores is discussed in Section 3.2. As for the processing stage, data locality considerations call for a recursive algorithm, as discussed in Section 3.3. Parallelizing the processing is done by distributing the
single-threaded DFTs in each bin across the CPU cores. The diﬃculty with parallelizing the processing stage is that the number
of bins in the simplest algorithm is always a power of 2, but the number of CPU cores is not necessarily a power of 2.
Furthermore, data locality considerations suggest that reassembly must be performed as soon as possible after processing.
Also, because toward the end of the algorithms, the number of reassembled arrays decreases and eventually gets as low as 2,
scalability considerations require that the reassembly operation itself contain thread parallelism inside.
It can be very diﬃcult in a low-level parallel framework to overlap the multiple independent serial processing tasks with
multiple multi-threaded reassembly tasks. The processing and reassembly phase has two layers of parallelism; the outer layer
consists of independent process/reassemble tasks, and the inner layer is the parallel for-loop over the multiple data elements in
each reassembly task. In the beginning of the calculation, there are more tasks than threads, so it makes sense to schedule the
single-threaded processing jobs across multiple threads, and to give few threads to the inner layer of parallelism. Toward the end
of the calculation, there are fewer tasks than threads, so some of the inner parallel loops must start working to keep the threads
busy. Because of this diﬃculty, we were not able to come up with a satisfactory implementation using OpenMP, a framework
in which the programmer is responsible for assigning speciﬁc work items to hardware threads. Even though OpenMP supports
dynamic adjustment of the number of threads, we could not achieve good performance with it. At the same time, balancing the
amount of parallelism in the inner and the outer loop occurs naturally in the framework Intel Cilk Plus, which relies on work
stealing for scheduling. Appendix B gives a brief overview of the Intel Cilk Plus framework.
3. Optimizing EFFT
This section focuses on the steps we took to parallelize and optimize the EFFT library.
Before we examine the code, we will ﬁrst describe some of the terminology used in this text and in the code. The parameter
we call “number of splits”, represented in codes by variable numofsplits, and in the text by the variable s, is the number of
Radix-2 CT algorithm steps applied to the transform. For example, with three splits, we apply the Radix-2 CT algorithm three
times, thus decomposing the transform of size N into b = 23 = 8 smaller transforms of size N/8. These smaller transforms are
stored in segments of a larger “scratch array”. We refer to these segments as “bins”, which in the code is represented by variable
numofbins and in the text by b. The number of bins is related to the number of splits as b = 2s or s = log2 b.

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1
2
3

129

// Data a l l o c a t i o n on 64 byte bou ndary
dataPtr = ( float *) _mm_malloc ( size * sizeof ( float ) , 64) ;
scratchPtr = ( float *) _mm_malloc ( size * sizeof ( float ) , 64) ;

4
5
6
7

// Handle creation
D F T I _ D E S C R I P T O R _ H A N D L E * fftHandle =
( D F T I _ D E S C R I P T O R _ H A N D L E *) malloc ( numthreads * sizeof ( D F T I _ D E S C R I P T O R _ H A N D L E ) ) ;

8
9
10
11
12
13
14
15
16
17
18
19

// Spawning a parallel region
cilk_for ( int i = 0; i < numworkers ; i ++) {
m k l _ s e t _ n u m _ t h r e a d s (1) ;
// P r e p a r i n g small MKL FFT
MKL_LONG fftsize = binsize ;
// One handle per worker
DftiCreateDescriptor (& fftHandle [ i ] , DF TI_SINGLE , DFTI_REAL , 1 , fftsize ) ;
DftiSetValue ( fftHandle [ i ] , DFTI_NUMBER_OF_USER_THREADS , 1) ;
DftiSetValue ( fftHandle [ i ] , DFTI_PAC KED_FORMAT , DFTI_PERM_FORMAT ) ;
DftiCommitDescriptor ( fftHandle [ i ]) ;
}
Fig. 3. Data allocation and handle creation and initialization.
1
2
3
4

for ( int i = 0; i < N ; i +=2) {
temparray [ i ] = data [2* i ];
temparray [ i + N /2] = data [2* i +1];
}
Fig. 4. Basic recursive scatter.

3.1. Initialization
EFFT is a C++ library that implements a class EFFT_Transform. The constructor of this class takes care of data allocation and
initialization for the transform. Majority of the initialization code is self-explanatory, but we wanted to highlight two important
details related to performance optimization.
The ﬁrst detail is data allocation. The initialization function in EFFT automatically allocates both the data storage array as well
as the necessary temporary storage array. The storage array exists as long as the encompassing EFFT class exists. We retain this
array to reduce the overhead of the Transform() method. Additionally, by allocating the memory internally, we ascertain that (i)
the data container is aligned on a 64-byte boundary, and (ii) that the ﬁrst touch to the data container is performed in a parallel
region by the MKL DFT initialization function. Both of these details are important for subsequent performance of the FFT.
The second important aspect of EFFT initialization is MKL DFT handle creation. Since the time required to create a handle is
often longer than the actual transform itself, it is important to create the handle once and retain it for all subsequent transforms.
Furthermore, in order to implement the most eﬃcient MKL usage mode, we create as many Handles as there are Intel Cilk Plus
workers.
Fig. 3 shows the portions of the initialization that we have discussed.
3.2. Scattering stage
The purpose of the scatter phase of the transform is to reorganize the data set into a temporary data array, where the elements
for small DFTs are listed contiguously. In terms of the CT algorithm, this is the phase where we separate the even-numbered
elements and odd-numbered elements.
Fig. 4 shows a basic, unoptimized scatter that is equivalent to one step in the Radix-2 CT algorithm.
This basic version of scatter can be applied recursively to arrive at the reorganized array we require, but has extremely poor
performance due to high number of memory accesses. In order to carry out the Radix-2 CT algorithm s times, or to do s splits,
this recursive scatter will have to read 2 · s · N memory addresses. Thus, in order to gain performance it would be ideal to do the
full scatter in a single pass.
In our EFFT code, we use the fact that s is generally small and precompute the pattern of mapping from the original data array
to the scratch array. This mapping is based on the bit reversal principle[11]. Fig. 5 shows a (still unoptimized) variant of our that
version. In that code, we also use cilk_ f or to parallelize the scattering process.
There are several techniques we can employ to optimize this scatter code for parallelism and improve its performance. First
technique is to permute the j and the i-loops. This is helpful because in a target parameter domain of EFFT (N ࣡ 109 ), practical
values of b ≡ numofbins are much smaller than N/s ≡ binsize. Therefore, in modern multi-core processors, parallelizing the
j-loop with b iterations can potentially lead to the problem of not having enough parallelism. For instance, if we had T = 24
workers and b = 32 bins, the runtime system would ﬁrst distribute 24 bins across the 24 workers. Once this work is done, only

130

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1
2
3
4
5

int scatter_index [ numofbins ];
f i n d _ s c a t t e r _ i n d e x ( scatter_index , numofsplits ) ;
cilk_for ( int j = 0; j < numofbins ; j ++)
for ( long i = 0; i < binsize ; i ++)
tempArray [ scatter_index [ j ]* binsize + i ] = dataArray [ i * numofbins + j ];
Fig. 5. Unoptimized scatter.

1
2
3
4
5
6
7
8

int scatter_index [ numofbins ];
f i n d _ s c a t t e r _ i n d e x ( scatter_index , numofsplits ) ;
const long iTILE =16;
long ii ;
cilk_for ( ii = 0; ii < binsize ; ii += iTILE )
for ( long j = 0; j < numofbins ; j ++)
for ( long i = ii ; i < ii + iTILE ; i ++)
tempArray [ scatter_index [ j ]* binsize + i ] = dataArray [ i * numofbins + j ];
Fig. 6. Optimized scatter code with tiling.

32 − 24 = 8 of the 24 workers will be utilized for the remainder of the calculation. Therefore, permuting the loops to bring the i
loop outside and parallelize it ensures that there is enough parallelism in the code.
However, the loop permutation has the negative effect of losing unit-stride access in i in the inner loop. It is true that we
gain unit-stride in j in exchange for this loss, but for this particular implementation of the scatter, unit-stride in i is of greater
importance. This is due to the fact that the variable i is multiplied by b ≡ numofbins, while j (or, rather, scatter_index[j]) is
multiplied by binsize ≡ N/b. Again, the b is normally much smaller than N/s, so memory accesses with consecutive values of i
are located closer to each other than accesses consecutive in j. Thus having i in the inner loop is more optimal.
In order to get parallelism in i while keeping a unit-stride access in i, we implement the technique called loop tiling. This
technique involves strip-mining the inner loop and permuting the outer two loops. The strip-mining operation expresses a
single loop as two nested loops, of which the outer loop iterates with a stride referred to as “Tile”, and the inner loop iterates
incrementally within the “Tile”. Fig. 6 shows our ﬁnal optimized code. The value iTile=16 is obtained empirically and provides
the best performance in most cases.
In order to understand why Fig. 6 is optimal, consider the locality of data accesses. Each worker operates on a different value
of ii (in practice, each worker will get a contiguous chunk of values of ii to process). For each value of ii and each value of j,
the worker reads iTile==16 elements from dataArray with a stride of numofbins. Then these elements are written contiguously
into tempArray. Contiguous access to tempArray is good, however, in the process of reading dataArray, the worker had touch 16
cache lines to read only 16 words. Luckily, the loop in i is short enough so that for the next value of j, these cache lines are reused.
Therefore, the application can achieve a signiﬁcant fraction of memory bandwidth. At the same time, the outer parallel loop has
enough iterations to scale across tens of threads available in modern CPUs.
Loop tiling is not the only way to optimize memory traﬃc in nested loops like in lines 3 and 4 of Fig. 5. These loops effect
data movement similar to that of out-of-place matrix transposition. For optimizing cache reuse in transposition operation, other
studies have shown that a cache-oblivious recursive algorithm has asymptotically optimal cache traﬃc [see, e.g., 6, 7]. However, the cache-oblivious algorithm does not beneﬁt the “scatter phase” in our case. This is because, as our results show (see
Section 5.3) the typical optimal value for the number of splits is 3–5, which corresponds to a matrix with 8–32 elements in one
of the dimensions. If our algorithm followed the recursive method of [7], the recursion would be terminated when the sub-matrix
size is 16 or 32, so that sub-matrix rows are one or two cache lines long. At that point, a tiled loop would be applied. With that in
mind, we must recognize that our matrix is one dimensional in terms of tiles. Thus we did not implement cache-oblivious global
transpose for our “scatter phase”.
3.3. Processing/parallel recursion
In order to parallelize the processing and reassembly phases, we have combined them into a single parallel recursion tree.
This framework is shown in Fig. 7.
The processing call is initially applied to the array of size n=N. This call subsequently recurses into two instances of itself
with n reduced by a factor of 2. One of the recursive calls is placed in an Intel Cilk Plus task using the keyword cilk_spawn,
thus effecting parallel recursion. Recursion stops when n is reduced to the target binsize, and processing with serial MKL DFT is
applied in parallel to the multiple small “bins”. After the processing of two adjacent bins is complete (as indicated by the return of
the cilk_sync call), the reassembly function is called on these two bins. Reassembly has thread parallelism inside, as explained
in Section 3.4.
To see why we chose to incorporate reassembly in the same parallel recursion tree as processing, consider a case where the
architecture allows for T = 24 threads, and we do s = 5 splits to get b = 32 bins. The code starts the ﬁrst 24 serial DFTs in parallel,
as desired. However, after the ﬁrst 24 bins are processed (transformed), the rest of the DFTs can occupy only 8 threads. In order
to utilize all available CPU resources, the remaining 16 threads should work on reassembly while the other 8 ﬁnish the rest of the

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1

131

void P r o c e s s A n d R e a s s e m b l e (
D F T I _ D E S C R I P T O R _ H A N D L E * fftHandle ,
float * array , float * temp ,
const long n , const long binsize ) {
const long size = n /2;

2
3
4
5
6
7

if ( n > binsize ) {
// Create parallel task
cilk_spawn ProcessAndReassemble ( fftHandle , array , temp , size , binsize ) ;
P r o c e s s A n d R e a s s e m b l e ( fftHandle , & array [ size ] , & temp [ size ] , size ,
binsize ) ;

8
9
10
11
12

// Wait for spawned task
cilk_sync ;

13
14
15

// CT a l g o r i t h m to combine evens and odds . R e a s s e m b l e P a i r () is
multithreaded
ReassemblePair ( temp , size ) ;
} else {
// Process the small enough array with serial MKL i m p l e m e n t a t i o n of DFT
int wid = _ _ c i l k r t s _ g e t _ w o r k e r _ n u m b e r () ;
D f t i C o m p u t e F o r w a r d ( fftHa n dl e [ wid ] , temp ) ;
}

16
17
18
19
20
21
22

}
Fig. 7. Parallel recursive processing and reassembly in EFFT.
1
2
3
4
5
6

void B as ic Re a ss embl e ( float * evens , float * odds , float * target , const long
int size ) {
const float trigconst = -3.14159265359 f / size ;
target [0
]= evens [0] + odds [0];
target [1
]= evens [0] - odds [0];
target [ size
]= evens [1];
target [ size +1 L ]= - odds [1];

7
8

for ( long int k = 1 L ; k < size /2 L ; k ++) {
float cosk = cosf ( k * trigconst ) ;
float sink = sinf ( k * trigconst ) ;
target [2* k ]= evens [2* k ]+ odds [2* k ]* cosk - odds [2* k +1]* sink ;
target [2* k +1]= evens [2* k +1]+ odds [2* k ]* sink + odds [2* k +1]* cosk ;
target [2* size -2* k ]= evens [2* k ] - odds [2* k ]* cosk + odds [2* k +1]* sink ;
target [2* size -2* k +1]= - evens [2* k +1]+ odds [2* k ]* sink + odds [2* k +1]* cosk ;
}

9
10
11
12
13
14
15
16

}
Fig. 8. Basic (not optimized) reassembly of two arrays into one using Equation (3).

DFTs. Thus, in order to utilize all available hardware in a parallel setting, it is beneﬁcial to combine reassembly and processing in
the same tree.
3.4. Reassembly
The purpose of the reassembly phase is to apply Equations (3) and (4) to assemble the results of smaller DFTs to
produce the result of the larger decomposed FFT. This stage is performed by the function ReassemblePair() called from
ProcessAndReassemble() in Fig. 7.
The reassembly component uses the Radix-2 CT algorithm to recombine the results of the smaller DFTs. Fig. 8 shows a basic
(i.e., unoptimized) implementation of reassembly that applies a single step of Radix-2 CT algorithm to combine two bins into
one.
In Fig. 8, evens and odds are the FFT result of even- and odd-numbered elements, respectively (Ek and Ok ), target is the target
location to store the combined result (Fk ), and size is the size of the evens and odds arrays. Note that the ﬁrst two elements
are dealt with as special cases, because those are the real parts of F0 and FN/2 instead of the real-imaginary components of
Fk ≡ Rk + iIk like the rest (see Equation (A.2)).
Following Equation (3), the code adds the kth element of the evens to the kth element of the odds multiplied by the twiddle
factor. The second half of the target array, which corresponds to k ≥ N/2 (which are not in the evens and odds arrays), is calculated
by taking advantage the symmetry expressed by Equation (6).

132

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1

const long kTILE = 64 L ;

2
3

// ... omitted the code handling 0 and N /2

4
5
6
7

for ( long k = 1; k < kTILE ; k ++) {
// E l e m e n t s from 1 to kTILE -1 are handled s eparately
}

8
9
10

# define ALIGNED __attribute__ (( aligned (64) ) )
cilk_for ( long kk = kTILE ; kk < size /2 L ; kk += kTILE ) {

11
12

// Unfused loop to p r e c o m p u t e t r i g o n o m e t r i c
// f u n c t i o n s with vector o p e r a t i o n s
float coslist [ kTILE ] ALIGNED , sinlist [ kTILE ] ALIGNED ;
for ( int i = 0; i < kTILE ; i ++) {
coslist [ i ] = cosf (( kk + i ) * trigconst ) ;
sinlist [ i ] = sinf (( kk + i ) * trigconst ) ;
}

13
14
15
16
17
18
19
20

// Iterating within the tile
for ( long k = kk ; k < kk + kTILE ; k ++) {
target [2* k ]= evens [2* k ]+ odds [2* k ]* coslist [k - kk ] - odds [2* k +1]* sinlist [k - kk ];
target [2* k +1]= evens [2* k +1]+ odds [2* k ]* sinlist [k - kk ]+ odds [2* k +1]* coslist [k - kk ];
// ...
}

21
22
23
24
25
26

}
Fig. 9. Improved (still not optimal) reassembly code. Strip-mining helps to vectorize the trigonometric functions.

At ﬁrst glance this appears to be a problem that is easily vectorized, since the result is simply a superposition of elements
of two arrays multiplied by trigonometric functions, which can be vectorized. However, because the arrays contain complex
numbers with real and imaginary parts interleaved, access to the potentially vectorizable data has a stride of 2. This situation is
diﬃcult for the compiler to handle, and we have found that the following code modiﬁcation improves automatic vectorization
and, along with it, application performance.
First, in order to vectorize the calculation of trigonometric functions, we strip-mine the array in k. This operation expresses
the loop in k as two nested loops in kk (iterating with a stride of 64) and a loop in k (iterating with a stride of 1 through 64
iterations). The second step is un-fusing the loop over the trigonometric functions from the loop over array elements (see Fig. 9).
The reason for strip-mining the loop is to restrict the size of the container for precomputed trigonometric functions to a small
enough value that does not cause the eviction of evens and odds from caches. Note that with the strip-mined loop, we have to
handle k = 0 as an exception (see output format given by Equation (A.2)). This also necessitates a separate loop for processing
elements from k = 1 to k = kT ILE − 1, because the automatically vectorized loops in lines 15 and 21 of Fig. 9 is built for complete
tiles of size kTILE. A separate loop for the ﬁrst tile introduces redundant code, however, it allows us to avoid branches in the main
loops in lines 15 and 21, which would ruin the overall performance of the application.
The second optimization of vectorization relies on the Intel Cilk Plus array notation to assist the vectorization of operation on
strided data. Indeed, the for-loop in line 21 of Fig. 9 mixes data with a stride of 1 (in sinlist and coslist) and stride 2 (evens,
odds and target). We have found that re-writing this loop with array notation as shown in Fig. 10 improves the eﬃciency of
automatic vectorization. Array notation is not a standard part of the C++ language; it is a language extension provided by the
Intel Cilk Plus framework. It is supported by the Intel C++ compiler and by GCC [18].
Finally, one more target of optimization in the reassembly code is memory access. The unoptimized code in Fig. 10 combines
two elements from input arrays and writes out the result into a separate output array. This means that in this code, reassembly
is done out-of-place. Indeed, the array index of the destination element Fk is not the same as the index of the source elements Ek
and Ok . Fig. 11 demonstrates this problem.
However, this diﬃculty can be overcome by performing reassembly in pairs of elements. Due to the symmetry of the operation, by combining two different steps in reassembly, it is possible to get an assembly step where the memory location for the
input is the same as the destination memory location for the output. Indeed, upon the assembly of Ek and Ok , we have to write
Fk and FN−k−1 . If at the same time we assemble EN/2−k−1 and ON/2−k−1 , then we can write FN/2−k−1 and FN−(N/2−k−1 )−1 = FN/2+k .
The offset of the latter element coincides with the offset of Ok . Therefore, it is possible to read the values of E and O elements and
write the values of the corresponding F elements into the same memory locations. Fig. 12 shows this assembly-in-pair scheme.
Using this method, in the optimized version of element combination function (Fig. 13) we perform reassembly in-place. By doing
this, we reduce the number of memory accesses by a factor of 2.
The code shown in Fig. 13 is the ﬁnal version that we adopted in EFFT. It is optimized with the following methods:
1. Strip-mining to precompute the trigonometric functions with vector operations,
2. Array notation to help the compiler vectorize loops with non-unit stride access, and

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1

133

cilk_for ( long kk = kTILE ; kk < size /2 L ; kk += kTILE ) {

2
3

float coslist [ kTILE ]
sinlist [ kTILE ]
evenrek [ kTILE ]
evenimk [ kTILE ]
// ...

4
5
6
7

ALIGNED ,
ALIGNED ,
ALIGNED ,
ALIGNED ,

8
9
10
11
12
13
14

# pragma simd
# pragma vector
for ( int i =
sinlist [ i ]
coslist [ i ]
}

aligned
0; i < kTILE ; i ++) {
= sinf ( theta + i * trigconst ) ;
= cosf ( theta + i * trigconst ) ;

15
16

// Iterating within the tile ( array notation )

17
18

// Gather elements with a stride of 2
evenrek [:] = evens [ kk : kTILE :2];
evenimk [:] = evens [ kk +1: kTILE :2];
oddrek [:] = odds [ kk : kTILE :2];
oddimk [:] = odds [ kk +1: kTILE :2];

19
20
21
22
23
24

// R e a s s e m b l e & s c a t t er into stride -2 array
target [ kk : kTILE :2] = evenrek [:] +
coslist [:]* oddrek [:] - sinlist [:]* oddimk [:];
target [ kk +1: kTILE :2] = evenimk [:] +
sinlist [:]* oddrek [:]+ coslist [:]* oddimk [:];

25
26
27
28
29

// ...
}

Fig. 10. Further optimized (still not optimal) reassembly code. Array notation helps the compiler to vectorize operations with strided data accesses.

Assembled

Evens

Odds

Fig. 11. Diagram showing the memory locations in a CT assembly of an element. Because of the location of the elements, this algorithm cannot be done in-place.

Assembledt

Evens

Odds

Fig. 12. Diagram showing the memory locations in a CT assembly in pairs. With this method the memory locations overlap, and in-place CT assembly is possible.

3. Performing reassembly on symmetrically located pairs of elements in each iteration in order to perform the job in-place,
reducing the number of memory accesses.
Fig. 13 does not contain the code for the processing of elements k = 0 and k = N/2, because they are stored in a different way
from the rest of the elements (see Equation (A.1)). It also does not contain the code for processing of elements k = 1 through
k = kTILE−1. These elements are processed separately because they do not comprise a full “tile”, and processing them together
with the rest of the tiles would require protection with branches, which would ruin vectorization performance. For the complete
code listing, refer to the source code available at [10].

134

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1
2
3
4

cilk_for ( long kk = kTILE ; kk < halfsize /2 L ; kk += kTILE ) {
float coslist [ kTILE ] ALIGNED , sinlist [ kTILE ] ALIGNED ,
evenrek [ kTILE ] ALIGNED , evenimk [ kTILE ] ALIGNED ,
// ...

5
6
7
8
9
10
11
12
13

// V e c t o r i z e d twidd le factor p r e c o m p u t a t i o n
const float theta =( float ) ( kk /2) * trigconst ;
# pragma simd
# pragma vector aligned
for ( int i = 0; i < kTILE ; i ++) {
sinlist [ i ] = sinf ( theta + i * trigconst ) ;
coslist [ i ] = cosf ( theta + i * trigconst ) ;
}

14
15

// Gather stride -2 data for first pair
evenrek [:] = evens [ kk : kTILE :2];
evenimk [:] = evens [ kk +1: kTILE :2];
oddrek [:] = odds [ kk : kTILE :2];
oddimk [:] = odds [ kk +1: kTILE :2];

16
17
18
19
20
21

// R e a s s e m b l e first pair
// and scatter results into stride -2 array
evens [ kk : kTILE :2] =
evenrek [:]+ coslist [:]* oddrek [:] - sinlist [:]* oddimk [:];
evens [ kk +1: kTILE :2] =
evenimk [:]+ sinlist [:]* oddrek [:]+ coslist [:]* oddimk [:];

22
23
24
25
26

// Start gather second pair ( mirror )
oddmirrek [:] = odds [ size - kk : kTILE : -2];
oddmirimk [:] = odds [ size - kk +1: kTILE : -2];

27
28
29
30

// Finish r e a s s e m b l y of first pair
odds [ size - kk : kTILE : -2] =
evenrek [:] - coslist [:]* oddrek [:]+ sinlist [:]* oddimk [:];
odds [ size - kk +1: kTILE : -2] =
- evenimk [:]+ sinlist [:]* oddrek [:]+ coslist [:]* oddimk [:];

31
32
33
34

// Finish g a the r ing second pair ( mirror )
evenmirrek [:] = evens [ size - kk : kTILE : -2];
evenmirimk [:] = evens [ size - kk +1: kTILE : -2];

35
36
37
38

// R e a s s e m b l e secon d pair of el e m e n t s
// and scatter results into stride -2 array
evens [ size - kk : kTILE : -2]=
evenmirrek [:] - sinlist [:]* oddmirr ek [:]+ coslist [:]* oddmirimk [:];
evens [ size - kk +1: kTILE : -2]=
evenmirimk [:] - coslist [:]* oddmirrek [:] - sinlist [:]* oddmirimk [:];
odds [ kk
: kTILE : 2]=
evenmirrek [:]+ sinlist [:]* oddmirrek [:] - coslist [:]* oddmirimk [:];
odds [ kk +1
: kTILE : 2]=
- evenmirimk [:] - coslist [:]* oddmirrek [:] - sinlist [:]* oddmirimk [:];

39
40
41
42
43
44
45
46
47
48

}
Fig. 13. Optimized main loop in the reassembly function.

4. Using the EFFT library
EFFT is a C++ library that implements class EFFT_Transform:
In order to perform a DFT, an object of type class EFFT_Transform must be initialized (Fig. 14). Its constructor requires two
inputs: size (N) and splits (s). The size argument is the size of the full transform of type long. Splits is the number of Radix-2 CT
that EFFT applies on the data set. Note that our current implementation of EFFT only accepts sizes that are multiples of 2(s+8 ) .
This restriction is the consequence of the Radix-2 CT algorithm, as well as the tiling optimization we have implemented.
The initialization of an EFFT_Transform object may take some time, especially for large arrays. However, once it has been
created, it can be reused as many times as the user needs. As discussed in Section 3, the EFFT_Transform internally allocates the
memory space it needs, including the data array.

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1

135

class EFFT_Transform {

2
3

// Private members not shown

4
5
6
7

public :
EFFT_Transform ( const long size , const int splits ) ; // C o n s t r u c t o r
~ EFFT_Transform () ; // D e s t r u c t o r

8
9

void RunTransform () ; // Main method
float * Data () ; // Accessor to input
float * Result () ; // Accessor to output

10
11
12

};
Fig. 14. Declaration of EFFT_Transform.
1

EFFT_Transform myTransform (n , numofsplits ) ;

2
3
4

// Getting the pointer to the data
float * A = myTransform . Data () ;

5
6
7
8
9

// Populating with random data
srand (0) ;
for ( long int i = 0; i < n ; i ++)
A [ i ] = (( float ) rand () /( float ) RAND_MAX ) -0.5 f ;

10
11
12

// Applies FFT to the data
myTransform . RunTransform () ;

13
14
15

// Getting the pointer to the result
float * B = myTransform . Result () ;
Fig. 15. Using the EFFT library.

After initialization of the main class, the user populates the data array using the accessor EFFT_Transform::Data(). Note that
this is a pointer to an array of type float; EFFT currently does not support double precision.
After the array was populated with data, the user should simply call the method EFFT_Transform::RunTransform() to carry
out the DFT on the data array. The pointer to the result of the DFT can be obtained with the EFFT_Transform::Result() accessor method. The current implementation of EFFT does the Transform out-of-place, so the data array will retain the input and
EFFT_Transform::Result() and EFFT_Transform::Data() will return different pointers.
Fig. 15 shows a sample implementation of a 1D DFT using the EFFT library.
Number of splits, s, is one of the two tuning parameters available in EFFT. Generally, the good performance is achieved when
2s is nearest to the number of total available threads. The other tuning parameter, number of workers, is set externally by setting
the environment variable CILK_NWORKERS or invoking __cilkrts_set_param(“nworkers”,“Wkrs”) where Wkrs is the number
of workers to use. For example, in order to use eight workers, either set export CILK_NWORKERS=8 in the terminal or invoke
the function __cilkrts_set_param(“nworkers”,“8”) in the C++ code. Generally, the good performance is achieved when the
number of workers is equal to the physical core limit. The optimal value for the two tuning parameter depends on variety of
factors, such as the model of the CPU, the performance of the memory subsystem, and the DFT size. The process of this one-time
optimization is discussed further in Section 5.2.

5. Benchmarks
5.1. System conﬁguration
All of the benchmarks presented in this section were taken on a Colfax ProEdge SXP8600 workstation1 based on two-way Intel
Xeon E5-2697 v2 processor (12 cores per socket, 24 cores total). For our tests, the hyper-threading feature was enabled and we
have disabled the Intel SpeedStep technology so the clock speed of the cores remained constant at the processor base frequency
of 2.70 GHz. We used the Intel C++ compiler version 15.0.0.90 and Intel MKL version 11.2 on a CentOS 6.5 Linux OS. Again, note
that all reported benchmarks for MKL DFT are of real-to-complex transforms. For comparing with FFTW, we used FFTW version
3.3.4 compiled with the Intel C compiler with conﬁguration arguments –enable-avx, –enable-single and –enable-openmp.

1

http://www.colfax-intl.com/nd/workstations/sxp8600.aspx

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

12
11
10
9
8
7
6
5
4
3
2
1

50
40
30
20
10

Performance, GFLOP/s

Splits, s

136

0
1

2

4

6

8

12

16

24

32

48

Threads

48
32
24
16
12
8
6
4
2
1

10
9
8
7
6
5
4
3
2
1

EFFT splits
EFFT workers

210

215

220

225

Optimal number of splits

Optimal number of workers

Fig. 16. Heat map showing the optimal number of workers and splits vs size. The areas that are “Hot” have better performance.

230

Transform size, N
Fig. 17. The optimal number of workers and splits vs size. They are different for each size, and thus the parameter scan must be performed for every size to
achieve the optimal performance.

5.2. Parameter tuning
As discussed in Section 4, the tuning of EFFT is done by scanning the 2D parameter space of the number of splits, s, and
the number of Intel Cilk Plus workers. Fortunately, this optimization scan is a one-time requirement; the optimal values for the
tuning parameters will not change for a given DFT size and hardware. Additionally, the optimization scan is not a requirement
to use the EFFT library; it is an option for users who needs the best possible performance. EFFT package comes with a simple
benchmarking code which can be used to scan the parameter space.
Fig. 16 shows an example of this parameter space scan. Color represents the performance, in GFLOP/s, for each point in the
parameter space for a DFT of size N = 230 . In this case, the optimal number of splits is s = 4 (corresponding to b = 2s = 16 bins),
and the optimal number of workers is 16. Here and elsewhere, performance is calculated from the wall clock run time of method
EFFT_Transform::RunTransform() using Equation (5).
Fig. 17 shows the optimal number of workers and splits for EFFT on our system as a function of array size. We use the data
shown in this ﬁgure to obtain tuned performance measurements in Section 5.3.
In Section 5.3, in addition to EFFT results, we show results of multi-threaded MKL DFT implementation and of the multithreaded FFTW library. We tuned both of these libraries by tuning the number of threads and thread aﬃnity for each array
size. In the parameter range of interest (N ࣡ 109 ), 16 or 24 threads and KMP_AFFINITY=scatter yielded the best performance.
Note that for other parameter ranges, other aﬃnity settings and thread count may yield better results. For FFTW, we planned
the transforms in the FFTW_MEASURE mode and subsequently reused the “wisdom” generated in these measurements (see [9] for
more information on FFTW planning and wisdom).

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

137

DFT performance (GFLOP/s)

70
60
50

EFFT
FFTW
MKL

40
30
20
10
0
210

215

220

225

230

Transform size, N

Peak memory usage (MiB)

Fig. 18. Comparison of performance of our implementation (EFFT), Intel MKL and FFTW, as a function of array size, for N = 2q . The number of threads, T, for MKL
is set to the optimal value for each point (T = 16 for most points), and for FFTW, T = 24 at all points. EFFT performance is reported with the optimal values of
tuning parameters T and s at each point.

105

104

EFFT
FFTW
MKL
Data

103

102
210

215

220

225

230

Transform size, N
Fig. 19. Peak memory usage as a function of array size. We found that for most sizes the memory consumption of EFFT is comparable to memory consumption
of MKL DFT.

5.3. Performance, memory usage and accuracy
Fig. 18 shows the tuned performance, in GFLOP/s, of EFFT (this work), MKL DFT and FFTW as a function of the array size. The
numbers of threads for MKL and FFTW are ﬁxed at, respectively, 16 and 24, which corresponds to values yielding the best results
for most array sizes. For EFFT, the number of threads and splits vary from point to point in this plot, and are always set to the
optimum value. For FFTW, we did not measure performance for arrays greater than N = 230 because the planning time in the
FFTW_MEASURE mode is very long (hours to days).
According to our measurements (Fig. 18), EFFT outperforms FFTW for all sizes by more than 2 × . It also performs better than
MKL for array sizes N > 222 , achieving around performance between 45 and 55 GFLOP/s, which is 1.1 × to 1.5 × faster than MKL.
Fig. 19 shows the peak virtual memory usage for each of the libraries. Memory usage was determined by querying the ﬁle
/proc/(pid)/status and reading the line beginning with VmPeak. Here, (pid) is the process ID of the DFT implementation, and
sampling rate was set at 1 kHz. For small array sizes, measurements were not consistent from run to run due to short run times.
Solid black line in Fig. 19 shows the amount of data in the input DFT array.
Fig. 19 shows that for small array sizes, all libraries have memory usage around 2 GB. For larger arrays (N > 228 ), EFFT uses
less memory than MKL. However, both EFFT and MKL require 3 × to 4 × as much memory as the input data array contains. FFTW
has consistently lower memory consumption than EFFT for large arrays, around 2 × the data size.
In order to ensure fair comparison of the three implementations, we estimate the accuracy of the transforms. This is done
using the methodology proposed by the FFTW project [9]. The data array is initialized with random values from −0.5 to 0.5
and transformed. Then the computed result, F, is compared to the “exact” solution Fe obtained using a single-threaded “inﬁnite
precision” calculation with the help of the GNU Multiple Precision (GMP) library. The comparison metric is the so-called L2 norm
of the difference between the computed and “exact” DFT. Equation (7) deﬁnes the L2 norm.

||F − F e || =

(Fk − Fke )2
Fke

2

(7)

138

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

10-4

L2 norm

10-5

EFFT
FFTW
MKL

10-6
3.10-7
2.10-7
1.10-7
0
210

215

220

225

230

Transform size, N

CPU Time (s)

Fig. 20. Accuracy of DFTs as a function of array size.

Scatter
Process
Reassemble
Idle
Total

512
256
128
64
32
16
8
4
2
0

2

4

6

8

10

12

Splits
Fig. 21. CPU time on each phase for N = 230 .

Fig. 20 reports the L2 norm of the deviation from the “exact” solution for power of 2 transform sizes with EFFT, MKL and
FFTW.
All codes produce comparable L2 norms of order a few times 10−7 , except for MKL for large transforms. Starting at the size
N = 222 , the accuracy of MKL degrades and continues to degrade linearly toward greater sizes. We have contacted Intel’s MKL
team, and they informed us that this is a bug in their code and that it will be ﬁxed in a future update.

5.4. CPU time analysis
Fig. 21 reports the amounts of CPU time spent in different sections of the code as a function of splits for N = 230 .
The reported processing and reassembly times were measured by timing each instance of the two respective functions from
each thread, and is the proper measured CPU time. The scatter CPU time was estimated by multiplying the total time spent on
the scatter phase by the number of threads, 24. We chose to use an estimated value for two reasons. First, we did not want to
introduce overhead from timing and change the execution time of the “scatter phase”. Second, we believe this to be a relatively
accurate representation of the actual CPU time because scatter phase is highly parallelized. We deﬁned the total time as the wall
clock execution time multiplied by the number of threads. Therefore, our total time is the time taken by all threads including
those that are idling. Finally, the idling time was estimated by subtracting the CPU times of the three phases from the total time.
One important result from Fig. 21 is that the amount of time “scatter phase” takes is near optimal for most values of splits. On
average, the scatter phase takes about 0.2 s of wall clock time for N = 230 (recall that the values in the ﬁgure are CPU times). In
the STREAM “copy” test [19], our system has a memory bandwidth of 60 GB/s, which translates to 8GB/60 = 0.133 s to read 4 GB
of data and to write the same amount. The scatter phase takes 1.5 times longer than that because our platform is a dual-socket
NUMA system, and accessing memory in a remote socket is slower than accessing memory on the local socket. This issue can be
resolved by proper ﬁrst touch allocation, where we allow scatter to operate on the allocated but uninitialized array. This ensures
that the virtual memory region touched by each thread is placed in the NUMA-local physical memory. Indeed, by doing that, we
were able to achieve the theoretical 0.133 s performance for the “scatter phase”. However this allocation negatively impacted the

Serial Performance, GFLOP/s

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

139

20.0
FFTW
MKL/EFFT

15.0
10.0
5.0
0.0
210

215

220

225

230

Transform size, N
Fig. 22. Single-threaded FFTW performance as a function of size.

other parts of the algorithm. Because the “scatter phase” takes up a small portion of the total computation time, we have decided
to keep the ﬁrst-touch allocation pattern that beneﬁted processing and reassembly.
The ﬁgure also demonstrates the reasoning behind the effects of splits on the performance of EFFT. Higher number of splits
expose parallelism at the cost of more post-processing. This is evident from the sharp decrease in idling time as parallelism
is exposed, and a linear increase in reassembly as the number of splits increase. Lower number of splits do not offer enough
parallelism to keep all cores busy in the processing stage. Finding the best value for splits is a balancing act between the two.
5.5. Notes on FFTW
As stated in Section 2.1, EFFT is a pre-processing and post-processing algorithm that partitions the problem in such a way that
a single-threaded library implementation of small FFTs can be used for the bulk of the computation. Thus it is possible to implement EFFT with single-threaded FFTW, rather than MKL, functions processing the bins. However, we chose to not implement
EFFT with FFTW for several reasons.
One issue is that with FFTW, the user of real-data transforms must choose between the complex and half-complex output
formats. The complex format has the output array size greater than the input array size, which is inconvenient for our algorithm.
The half-complex format does not have that problem, but it is not adequate for the in-place reassembly method discussed in
Section 3.4, because this method requires pairs of elements, which cannot be formed in the half-complex format.
Another issue is the practicality of the FFTW planning time. In our parameter range of interest (N ࣡ 109 ), FFTW planning takes
on the order of hours to days. Generating the FFTW wisdom once and storing it in a ﬁle can drastically speed up subsequent
planning, but even just the initial planning can make FFTW impractical. That is especially important if the user has to run the
application on different platforms (FFTW wisdom is not portable), or in an enterprise cluster environment where the initial
planning time could be costly. On the other hand, EFFT with MKL takes only a few seconds to initialize.
Finally, EFFT with FFTW will have lower performance. Fig. 22 shows the single threaded performance of FFTW.
The single threaded performance of FFTW is about 2–3 × worse than the MKL performance in the target parameter range. As
the Fig. 21 from Section 5.3 shows, the CPU time of processing (i.e., the time consumed by MKL) is around 30% of the total CPU
time, which is a signiﬁcant portion of the computation. Thus, EFFT with FFTW would require more processing time, leading to
lower performance.
5.6. Parallel scalability
While raw maximum performance reported in Section 5.3 is important for practical application, it is also informative to study
the parallel eﬃciency of the implementations. We deﬁne parallel eﬃciency, η, as the ratio of the actual performance with T
threads, P(T), to the projected performance assuming linear speedup:

η (T ) =

P (T )
.
T × P (1 )

(8)

In compute-bound workloads, η may remain constant up to values of T equal to the number of physical cores in the processor.
However, for bandwidth-bound workloads like DFFT, η is expected to decrease with increasing T. In order to evaluate the thread
scalability of EFFT, we compare η for EFFT with η for the STREAM benchmark [19].
Fig. 23 reports the performance and parallel eﬃciency of EFFT and of the STREAM benchmark. For EFFT, N = 230 was used. In
all tests, thread aﬃnity with T threads was set to KMP_AFFINITY=explicit,proclist=[0-N], where N = T − 1. This binds threads
to individual processor cores in such a way that for 1 < T ≤ 12, threads are placed on successive cores on CPU 0, and for T > 12,
threads start ﬁlling CPU 1. That said, for T ≤ 12, only one NUMA node is used in our two-way system, and for T > 12, two NUMA
nodes are used.
According to Fig. 23, the eﬃciency of EFFT at the optimal value of T = 24 threads is η = 0.39, while STREAM has better
eﬃciency, η = 0.47. EFFT is expected to be less eﬃcient than STREAM for T = 24. That is because in STREAM, threads read and

140

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142

Performance
(GFLOP/s)

50
40

1

2

4

6

8

12

16

24

32

48

EFFT

30

Two NUMA nodes

20
One NUMA node

10

Performance
(GB/s)

0
80 STREAM
One NUMA node
60
Two NUMA nodes

40
20

Efficiency, η

0
1
0.8
0.6
0.4
0.2
0

EFFT
STREAM

One NUMA node
1

2

Two NUMA nodes
4
6
8 12 16 24 32 48
Number of threads/workers

Fig. 23. Parallel scalability of EFFT and STREAM.

write only NUMA-local data (i.e., data mapped to memory banks local to the socket on which the thread is executing), while EFFT
has a more complex pattern of memory traﬃc with non-local NUMA accesses. However, for T = 12, when NUMA is not involved,
EFFT has slightly better eﬃciency better than STREAM (which is consistent with data reuse in caches in EFFT). This observation
indicates good parallel scalability of the bandwidth-bound EFFT algorithm.
6. Discussion
Our goal in this work was to implement a multi-threaded code for very large (N ࣡ 109 ) 1D DFTs. We achieved that goal by
developing the EFFT library based on a serial MKL DFT implementation and Intel Cilk Plus to parallelize and vectorize the multithreaded implementation. Our implementation performs better than multi-threaded MKL DFT for N > 222 by 1.1 × –1.5 × , with
better accuracy of the calculation and lower memory usage than MKL. We also benchmarked EFFT against FFTW and measured
2 × better performance with the former without any loss in accuracy; however, FFTW has lower memory footprint.
This paper also demonstrated multiple optimization techniques and discussed the reasoning behind the performance gain
that each technique produced:
•
•
•
•
•
•

improving temporal data locality via loop tiling,
improving spatial data locality (in-place algorithm),
strip-mining to vectorize transcendental math,
using array notation to vectorize stride-2 operations (“gather” and “scatter”),
using the cilk_spawn/cilk_sync extensions of Intel Cilk Plus to effect parallel recursion, and
using the cilk_for extension for loop parallelism.

Furthermore, this publication demonstrated the strength of Intel Cilk Plus for parallelizing a workload with a complex, multilevel pattern of parallelism. While the OpenMP standard offers similar functionality (tasking and dynamic number of threads in
parallel regions), in this application we were not able to achieve satisfactory performance results with OpenMP despite investing a greater development effort than we did with Intel Cilk Plus. In contrast, development with Intel Cilk Plus required little
programming effort and resulted in accelerated performance.
The good parallel scalability of EFFT (considering its bandwidth-bound nature) and its reliance on automatic vectorization
and portable Intel Cilk Plus parallel framework promise high chances of adapting this application to the Intel MIC architecture.
In a future publication, we will report on the possibility of accelerating what we call “enormous Fourier transforms” using Intel
Xeon Phi coprocessors.
The product of the publication, the EFFT library, is available for free download [10].
Acknowledgement
This research has been funded by Colfax International.

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1
2
3
4
5
6

141

// (1) cr e a t i n g the d e s c r i p t o r for single - pr e c i s i o n DFFT on real data
const int N = 1 < <28;
float * data = ( float *) _mm_malloc ( N * sizeof ( float ) , 64) ;
MKL_LONG fftsize = N ;
D F T I _ D E S C R I P T O R _ H A N D L E * fftHandle = new D F T I _ D E S C R I P T O R _ H A N D L E ;
D f t i C r e a t e D e s c r i p t o r ( fftHandle , DFTI_SINGLE , DFTI_REAL , 1 , fftsize ) ;

7
8
9
10
11
12

// (2) c o n f i g u r i n g d e s c r i p t o r : single - th r e a d e d t r a n s f o r m
//
packed permuted data format
DftiSetValue (* fftHandle , DFTI_NUMBER_OF_USER_THREADS , 1) ;
DftiSetValue (* fftHandle , DFTI_PACKED_FORMAT , DF TI _PERM_FORMAT ) ;
D f t i C o m m i t D e s c r i p t o r (* fftHand le ) ;

13
14
15

// (3) carrying out in - place DFT
D f t i C o m p u t e F o r w a r d (* fftHandle , data )
Fig. A.1. Basic MKL DFT code.

Appendix A. Intel MKL DFT
This section is designed to be a brief introduction to using the MKL DFT. For full documentation, refer to the Intel MKL
documentation [8].
Fig. A.1 shows a basic example code that carries out a single-threaded, one-dimensional, real data DFFT in single precision on
the data array of the size N = 228 .
The ﬂow of using the DFT functionality in MKL is as follows:
1. Create a DFTI_DESCRIPTOR_HANDLE. This object stores the parameters of the transform: precision, type, dimensions and size.
2. Set various options for the handle including the data format (layout) and the number of threads. After the change are made,
the handle must be ﬁnalized with the function DftiCommitDescriptor() before it can be used. Once it has been committed,
the handle can be used as many times as the user needs.
3. Carry out the transform using function DftiComputeForward(...). This runs an in-place forward Fourier transform of the
array data.
The format DFTI_PERM_FORMAT that we used is an output format that is speciﬁc to real-data transforms. With this format, the
input data array contains N data points in the following order:

{x0 , x1 , x2 , . . . , xN−1 },

(A.1)

and the output array contains N/2 + 1 complex coeﬃcients Fk ≡ Rk + iIk in the following order:

{R0 , R N2 , R1 , I1 , R2 , I2 , . . . , R N2 −1 , I N2 −1 }.

(A.2)

For k ≥ N/2, the values of Fk can be inferred using the symmetry property expressed by Equation (6).
Appendix B. Intel Cilk Plus
This section is designed to be a brief introduction to using Intel Cilk Plus [12,13].
Intel Cilk Plus is a powerful parallelization and vectorization framework that can effectively parallelize complex problems
with very little work required on the part of the programmer. In this framework, the programmer speciﬁes the components of
the application that can be run in parallel, and the runtime library takes care of assigning computing resources (cores) to the
parallel tasks. This is done via an internal scheduling mechanism based on “work stealing” to distribute parallel work-items
among “workers”. Refer to [14] and [15] for a more thorough discussion on the work stealing mechanism and its usage in Cilk.
Workers are a concept in Intel Cilk Plus similar to threads in other frameworks (for example, OpenMP). Because of the simplicity
of the API and high degree of behind-the-scenes automation, Intel Cilk Plus can dramatically reduce the development workload
and time while providing great performance.
There are only three keywords in Intel Cilk Plus: cilk_for, cilk_spawn and cilk_sync (Fig. B.1). The keyword cilk_for is
a replacement for the C++ for with a hint that parallel execution is possible in this loop. cilk_spawn is used to launch a task
represented by a function in parallel with the current program. Finally, cilk_sync is a barrier for all tasks spawned from the
current task. Complex patterns of parallelism, including nested parallel programs and parallel regions requiring different degrees
of parallelism, are fully supported.
However, note that this ease of use of Cilk functionality does not release the programmer from following the necessary precautions that apply to all parallelized loops. Avoiding issues such as race conditions is still the programmer’s responsibility. Intel
Cilk Plus provides C++ templates referred to as reducers in order to eliminate race conditions in parallel programs with certain
patterns (see, e.g., [16]).

142

R. Asai, A. Vladimirov / Parallel Computing 48 (2015) 125–142
1
2
3

cilk_for ( i = 0; i < n ; i ++) {
// I t e r a t i o n s of this loop are d i s t r i b u t e d b e t w e e n wo r k e rs
}

4
5
6
7
8

// Launch M y F u n c t i o n
cilk_spawn MyFunction (a , b , c ) ;
// This will run w i t h o u t w a i t i n g for M y F u n c t i o n () c o m p l e t i o n
MyOtherFunction (b , a , c ) ;

9
10
11

// Wait for c o m p l e t i o n of M y F u n c t i o n
cilk_sync ;
Fig. B.1. Summary of Intel Cilk Plus framework keywords.

The functionality of Intel Cilk Plus perfectly ﬁts with the parallel pattern of EFFT. Namely, cilk_for can be used to parallelize
the scatter phase and to effect parallelism inside of each reassembly task. cilk_spawn and cilk_sync can be used to achieve
parallel recursion.
We have limited our discussion of Intel Cilk Plus functionality to topics relevant to this work. For complete documentation,
visit the Intel C++ Composer Reference Manual [17] and the Intel Cilk Plus Web resources [12,13].
References
[1] C. Solomon, T. Breckon, Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab, John Wiley & Sons, 2011.
[2] W.B. Atwood, M. Zeigler, R.P. Johnson, B.M. Baughman, A time-differencing technique for detecting radio-quiet gamma-ray pulsars, Astrophys. J. Lett. 652
(2006) L49–L52, doi:10.1086/510018.
[3] M. Frigo, S.G. Johnson, The design and implementation of FFTW3, Proc. IEEE 93 (2) (2005) 216–231.
[4] J. Park, G. Bikshandi, K. Vaidyanathan, P.P. Tang, P. Dubey, D. Kim, Tera-scale 1D FFT with low-communication algorithm and Intel® Xeon Phi coprocessors,
in: Proceedings of SC’13, article No. 34, 2013.
[5] A. Nukada, K. Sato, S. Matsuoka, Scalable multi-gpu 3-d fft for tsubame 2.0 supercomputer, in: Proceedings of SC’12, article No. 44, 2014.
[6] H. Prokop, Cache-Oblivious Algorithms, Massachusetts Institute of Technology, 1999.
[7] D. Tsifakis, A.P. Rendell, P.E. Strazdins, Cache oblivious matrix transposition: Simulation and experiment, International Conference on Computational Science,
vol. 17, Springer, 2004.
[8] Intel Math Kernel Library. http://software.intel.com/en-us/intel-mkl (accessed 5.01.15 ).
[9] FFTW Library. http://www.fftw.org/ (accessed 5.01.15).
[10] Electronic preview of ‘Intel Cilk Plus for Complex Parallel Algorithms: “Enormous Fast Fourier Transforms” (EFFT) Library’.
http://research.colfaxinternational.com/post/2014/09/18/EFFT.aspx (accessed 5.01.15).
[11] J.W. Cooley, J.W. Tukey, An algorithm for the machine calculation of complex Fourier series, Math. Comput. 19 (90) (1965) 297–301.
[12] Intel Cilk Plus. https://software.intel.com/en-us/intel-cilk-plus (accessed 5.01.15).
[13] Cilk home page. http://www.cilkplus.org/ (accessed 5.01.15).
[14] R.D. Blumofe, C.E. Leiserson, Scheduling multithreaded computations by work stealing, J. ACM 46 (5) (1998) 720–748.
[15] M. Frigo, C.E. Leiserson, K.H. Randall, The implementation of the Cilk-5 multithreaded language, in: ACM Sigplan Notice, vol. 33, no(5), 1998, pp. 212–223.
[16] M. McCool, J. Reinders, A. Robison, Structured Parallel Programming: Patterns for Eﬃcient Computation, Elsevier, 2012.
[17] User and Reference Guide for the Intel C++ Compiler 15.0. https://software.intel.com/en-us/compiler_15.0_ug_c (accessed 5.01.15).
[18] Cilk Plus Array Notation for C Accepted into GCC Mainline. https://software.intel.com/en-us/articles/cilk-plus-array-notation-for-c-accepted-into-gccmainline (accessed 5.01.15).
[19] J.D. McCalpin, STREAM: Sustainable memory bandwidth in high performance computers. http://www.cs.virginia.edu/stream/ (accessed 5.01.15).

