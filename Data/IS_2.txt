j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

Available online at www.sciencedirect.com

ScienceDirect
journal homepage: www.elsevier.com/locate/jisa

Efficient suspicious URL filtering based
on reputation
Chia-Mei Chen*, Jhe-Jhun Huang, Ya-Hui Ou
Department of Information Management, National Sun Yet-sen University, Taiwan

article info

abstract

Article history:

Enormous web pages are visited each day over a network and malicious websites might

Available online 9 November 2014

infect user machines. To identify malicious web sites, the most reliable approach is honeypot, an execution-based method. The vast amount of http traffic makes sandboxing all

Keywords:

the web pages impossible. It is not practical in the real environments as it consumes too

Web attack

much computational resource and time. Only 1.3% of websites are malicious. Crawler-

Intrusion detection

based approach might be not easy to find malicious websites effectively, as it has no

Honeypot

clue if they are visited by users. Therefore, the proposed system examines only user web

Sandbox

requests, not from crawler, in order to catch the real drive-by download web attacks. The

Reputation

challenge is that the web traffic is huge in real networks and an efficient filtering is desired
to process large scale user requests efficiently.
Based on our observation, the domains of drive-by download attacks often are unreliable and exhibit distinct attributes from the normal. To classify massive volume of web
traffic in a real network, this study proposes a two-stage drive-by download attack
detection mechanism: first identifying suspicious websites based on domain reputation
and then sandboxing only the suspicious ones to reduce the detection time. Such detection
not only reduces the required computation resources and time, but also remains the efficiency benefited from sandbox-based detection. As WHOIS database is not reliable for not
every domain query can be resolved. Therefore, this study relies on queries from DNS
server and proposes novel reputation attributes to distinguish the benign and the suspicious. The experimental results show that the proposed filtering yields the accuracy of 94%
in simulated real network environment and efficiently saves more than 12 times of the
computing time with the comparison of an improved sandboxing approach. Such twostage detection system implemented in a real network environment with 560 thousand
URL requests per day demonstrates its practicality and efficiency under large scale web
requests. During the deployment on the real network, unknown malicious websites are
identified which are not listed in the public accessible blacklist websites.
© 2014 Elsevier Ltd. All rights reserved.

* Corresponding author.
E-mail address: cchen@mail.nsysu.edu.tw (C.-M. Chen).
http://dx.doi.org/10.1016/j.jisa.2014.10.005
2214-2126/© 2014 Elsevier Ltd. All rights reserved.

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

1.

Introduction

As web-based services increasingly prevail, more web applications are built for cloud or ubiquitous computing. People
highly depend on web applications for daily activities, such as
socializing, information searching and sharing, forum discussion, and online commerce services. The evolution of web
applications brings the convenience for everyone. As they
generate responses dynamically based on user requests, a
vulnerable web site might compromise information security.
Web application designers may develop the applications
with limited security knowledge and limited testing and
hence potential threats exist in many web applications.
Hacker has taken the advantage of web vulnerabilities to
inject malicious scripts into webpages. Malicious websites
may redirect the visitor to download malware into a computer
and such drive-by download attack may cause the victim loses
confidential information or becomes a stepping stone. Cenzic
security report (CENZIC) concluded that 66% of vulnerabilities
belong to web applications and 45% of web vulnerabilities can
be applied easily to contaminate a website. Therefore, web
attacks are prevailing and malicious website detection is
desired.
Most commercial anti-virus software or open-source solution (Netcraft) rely on large URL database or blacklist to
detect malicious or phishing websites. Drive-by download
attack is hard to detect by such blacklist or signature-based
detection system, as the URLs of malicious web page can be
easily changed or the web page can be obfuscated. Worm
Conficker (Porras et al., 2009) is one of examples frustrating
blacklist, where an infected machine would contact a list of 50
thousand domains created everyday. Malware could generate
a large number of randomly malicious URLs used for spam,
botnet servers each day to evade blacklist detection.
Execution-based detection is a reliable approach for preventing web attacks, examining web page in sandbox or virtual machine environment. Client honeypot applies a
vulnerable browser and interacts with the target website. A
drive-by download is detected if registry, file, or process is
contaminated. High-interactive client honeypot, such as
Honeymonkey (Ourston et al., 2003), CHP project (Wang et al.,
2006a), and Capture-HPC(Yuan, 2007), is commonly used to
detect malicious web sites. However, the efficiency issue
should be considered when the execution-based approach is
applied.
Users usually rely on search to retrieve information. Google
reported that only 1.3% of search results are malicious (Porras
et al., 2009; Seifert and Steenson, 2006; Gruener, 2008).
Examining randomly selected websites is time consuming and
inefficient, Crawler-based detection is inefficient, as only
small amount of the websites examined by crawler may be
malicious and might not be visited by users. Therefore,
crawler-based approach may waste most resources on
examining the normal contents. Web page is easy to obfuscate, and hence signature-based detection is not efficient,
either.
Some studies focus on crawler-based client honeypot approaches (Seifert and Steenson, 2006; Provos et al., 2008;
Moshchuk et al., 2006), but yield the inefficiency problem.

27

The total number of websites in the Internet is over eight
hundred millions and about ten millions are malicious.
Crawler-based searching and detecting malicious websites
throughout the whole Internet is like finding a needle in a
haystack.
Even though sandboxing is an efficient way to detection
malicious websites, according to Honeymonkey research
(Ourston et al., 2003), honeypot/sandboxing approach takes at
least 30 seconds to determine the legitimacy of a website.
Improving the efficiency of the reliable approach becomes
essential for the applicability of this approach in a real
network. As the average size of a web page has tripled and the
number of external objects doubled since 20081, the sandboxing solution may take more processing and resource in the
future. Two stage detection solution with filtering and sandboxing might be an efficient solution for examining a vast
amount of web traffic.
Inspecting either all user web requests or crawled websites
by honeypot to identify 1.3% of malicious is time consuming
and resource exhaustion. Malicious websites make no harm
unless visited. For large scale requests, only the visited websites need to be inspected to avoid wasting resources. The
proposed two-stage detection examines the domain of the
visited by reputation and screens out the legitimate with good
reputation score. The suspicious then are analyzed further
thru sandbox/honeypot. Such drive-by download attack
detection method allocates most of the computing resources
to the suspicious, while it eliminates the efficiency problem of
honeypot and retains the detection reliability.
The study has the following contributions. (1) It proposes
an efficient two-stage malicious website detection method. (2)
Instead of crawling throughout the whole Internet, the proposed solution examines user's web requests. (3) It proposes a
suspicious URL filtering mechanism with accuracy rate of
90.9%, largely reducing the processing resources required by
the next stage of the detection, sandboxing. (4) Any sandboxing solution can be used in the sandboxing stage where
only analyzes the suspicious URLs. (5) The proposed
reputation-based filtering online examines all the requests
and identifies the suspicious ones. It reallocates the
computing resources to client honeypot and largely improves
the detection efficiency. (6) This study proposes novel features
based on DNS queries but not WHOIS database, as it is not
available for all domains. (7) It can identify unknown malicious websites which are not listed in the blacklists.

2.

Literature review

Malicious web site poses a significant threat to Internet users
and becomes a serious security issue. Provos et al. (2007)
concluded four major types of drive-by download techniques: web applications, user contributed contents, advertising, and third-party widgets. Web 2.0 and mobile
technology provide more interactive and user-friendly web
applications, which imply more security threats within the
web pages.
1

http://www.websiteoptimization.com/speed/tweak/averageweb-page/.

28

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

Fig. 1 e Illustration of drive-by download attack.

Drive-by download attack illustrated in Fig. 1 consists of
the following steps: (1) user visits a malicious website (aka
land site); (2) he is redirected to a drive-by download server
(hop point, or jump site); (3) the hop point identifies the vulnerabilities of the victim's browser or plug-ins and selects the
best vulnerability to launch an attack; (4) the attack commands the browser to download malware from the distribution site; (5) malware is installed and operating automatically
without user notice. As mentioned above, about half of the
websites may exist vulnerabilities easily explored by hacker.
Hacker could try to contaminate as many websites as possible
to increase the possibility of successful web attacks. Therefore, most researches develop detection in early stages to
prevent the attacks.
Jim et al. (2007) proposed a mechanism, Browser-Enforced
Embedded Policies (BEEP), to prevent JavaScript injection. A
security policy is defined for each webpage. Therefore, browser
executes the script from the visited webpage based on the
specified policy. The mechanism works well if all the websites

enforce proper security policy and the policy cannot be
compromised easily. Hallaraker and Vigna (2005) presented an
audit system for JavaScript interpreter in browser which
monitors JavaScript code execution and compares to high-level
policies defined in IDS to prevent compromise. Implementing
audit system is a difficult task and this is the first attempt.
The embedded scripts from dynamic HTML webpages may
exploit user browser. Therefore, BrowserShield proposed by
Reis et al. (2006) rewrites web pages and the embedded scripts
into safe equivalents, inserts checks, and filters at run-time.
The run-time checks inspect dynamically generated web
content based on known vulnerabilities. Yu et al. (2007) also
proposed program instrumentation approach for defensing
malicious JavaScript code. Rewriting process identifies and
rewrites untrusted JavaScript code and requires the user's
decision to proceed further. User behavior needs to be
changed or such mechanism might not be workable in the
current environment. e Strider.
HoneyMonkey, a web patrol system, proposed by Wang
et al. (2006b) consists of a pipeline of virtual machines with
vulnerable browsers to seek out and classify the web sites
which exploit the browser vulnerabilities in the virtual machines. The system operated by Microsoft automatically constructed attack graphs based on web traffic redirection to
capture the relationship between the exploit sites. Large
amount of hardware, network, and computing resources are
required to facilitate such mechanism.
Some malware detectors used signatures to capture syntactic features of the malware, while Christodorescu et al.
(2005) proposed a semantics-based framework which adopts
a trace semantics to characterize the behaviors of malware and
ignores un-important behaviors. Lin et al. (2008) combined
semantic features with model checking classification to identify the malicious. Obfuscated webpages can be identified by
semantics-based approaches. This study summarized the semantics of obfuscated malicious webpages into a set of templates and can detect known obfuscated malicious web pages.

Fig. 2 e Proposed detection architecture.

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

Fig. 3 e The sequence of vulnerability scan by the exploit
toolkits (SECURELIST).

Spitzner (2002) observed that traditional server honeypot
cannot discover drive-by download attacks and proposed
client honeypot which emulates as a vulnerable browser and
identifies anomalous behaviors during the interaction with
the target server. As high-interaction client honeypots are
both resource-intensive and unable to handle the increasing
array of vulnerable clients, Seifert et al. (2008) presented a
hybrid system combining client honeypot and a classification.

29

It improved the efficiency problem of honeypot. The work
discovered that the number of domain name extensions and
that of DNS servers used to resolve the host names of the web
servers can be used to determine if a website is malicious or
benign. 50 malicious pages were tested and it yielded a true
positive rate of 0.745.
Sadan and Schwartz (2011) proposed social network based
parameters predicting the user trust for the visiting web page.
The reputation of the visiting web link and that of all of its
inner URLs are taken into count to determine if the web page is
trustful. Reputation system proposed by Antonakakis et al.
(2010) calculates the reputation score based on DNS query
and zone information. Notos (Sadan and Schwartz, 2011)
deployed a reputation system which considers the following
attributes: the DNS A-Type record information, malicious
domains collected from honeypots, IP addresses collected
from blacklists. Later work (Antonakakis et al., 2010) extended
to fast-flux domains and added TTL (Time To Live) attribute to
detect the malicious. However, in some researches, Chen et al.
(2013)and FluxBuster (Rajkumar and Jaganathan, 2013)
discovered that TTL might not be a reliable feature for
detecting fast-flux malicious domains, as some benign websites exhibit similar short TTL.
The reputation systems (Chen et al., 2013) relied on the information from WHOIS database. However, as the database is
distributed and the domain administration agencies may have
different information security policies, the query results might
not be consistent or complete and some information, such as

Fig. 4 e An illustration of a dig result.

30

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

domain registration date or expiration date, might be missing.
In this study, the proposed reputation-based filtering does not
depend on WHOIS database so that its applicability can be
improved. As the malicious domains collected from honeypots
or blacklists may be varied over the time, they are not taken
into consideration as the reputation attributes. Comparing
with the previous research, the proposed URL filtering considers DNS A-Type and NS-Type records as well as CNAME, and
proposes the reputation attributes from three aspects: the
reputations of a domain itself, its authority, and its ASN from
statistical point of view. The proposed URL filtering is efficient
to inspect the legitimacy of a domain and is suitable for
checking a large number of web requests in real time.

3.

Proposed two-stage detection

This study aims for identifying drive-by download attacks
using a two-stage approach combining the reputation-based
URL filtering and the sandboxing approach. Commercial
anti-virus software mostly is based on signature or blacklist
for stability. Comparing with the signature-based solution,
this approach can detect websites using zero-day vulnerabilities or obfuscation; comparing with the blacklist, it can
discover unknown malicious websites.
Client-honeypot or other sandboxing solution can be
adopted to examine malicious websites. Such detection is
reliable, but not efficient. At least half minute of processing
time is consumed for honeypot to examine a single website,
excluding the overhead and resources required for snapshots
and recovery. Based on the statistics, over 98% of the websites
are benign. Therefore, the computing resources should focus
on small amount of the suspicious to avoid wasting resources.
The proposed filtering develops a reputation system to
screen out a large amount of benign websites efficiently so
that the two-stage detection system could allocate most resources to honeypot/sandbox inspecting the suspicious. The
proposed detection architecture is illustrated in Fig. 2. The
reputation-based filtering is built by machine learning as
shown in the upper box of the figure. The training and
detection phases have similar preprocess modules. The first
module, Http Monitor, collects URL requests from users over
the network. Module of anomalous vulnerability scan check
inspects if the target URL possesses a number of various web

links which might be a sequence of exploits and polling attempts. After that, Module of Feature Extraction extracts the
reputation features based on the target domain's DNS query
and forwards them to the URL Filter. The suspicious URLs
reported by the filter will be sent to the next stage, sandboxing, for further examination.

3.1.

Anomalous vulnerability scan

As mentioned above that drive-by download consists of three
steps: browsing a malicious website (land site), redirecting to a
hop point site (or jump site), and then downloading and
executing malware (Porras et al., 2009). Landing sites often
store exploit toolkits (Dhanalakshmi et al., 2011; SECURELIST).
A malicious web site installed by the exploit toolkits uses less
number of web pages for scanning a victim host and then
applies a corresponding exploit code for each vulnerability
found in the victim in order to attack it efficiently and successfully. The sequence of vulnerability scan is shown in
Fig. 3.
A benign web site normally contains rich contents and
many web paths, while a landing site with exploit toolkits
only includes a small number of web pages. To identify if a
website possesses such anomalous behavior, Path Filter
Module inspects the number of web pages in the domain of
the target website to see if it exists many web links. A
landing site normally does not have many web links. However, our detection does not rely solely on path filter for
detecting drive-by downloads but could enhance the filtering
efficiency.

3.2.

Feature selection

To develop efficient reputation features, a preliminary
analysis was conducted and the test data used for the preliminary study was collected from the following sources:
Clean mx, Malwaredomainlist, and Phishtank for the malicious samples and Alexa and Open Directory Project (Dmoz)
for the benign. The results will be explained in the
following.
Some researches adopt WHOIS database for malicious
website detection. WHOIS database is not reliable and not
every domain query can be resolved. To reduce the consumption of computing resources, this study first examines

Table 1 e Proposed features.
Feature set
Domain-Based Features

Feature no.
F1
F2
F3
F4
F5

Authority-based Features

F6
F7

ASN-based Features

F8
F9
F10
F11
F12

Feature name
Number of CNAME records
Number of IP addresses
Numeric Ratio
Longest Meaningful Substring Ratio (LMSR)
Type of Level Domain
Number of name servers
Rogue
Numbers of ASNs
Number of countries
Mode Equivalence Ratio (MER)
Degree of Centralization (DoC)
Corresponding country

31

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

the domain of the target web request and issues command
‘dig’ to obtain its DNS records, where dig (domain information groper) is a tool of querying DNS server for domain
related information. The dig result of www.google.com.tw
illustrated in Fig. 4 is composed of three sections: Answer,
Authority, and Additional sections. The first one contains
the A record and CNAME record of the target host; the second includes the administrative name server of the domain;
the last, Additional Section, shows the A record of the name
server, such as its IP addresses. Since not all name servers
provide the last section, this study develops 12 features
based on the first two sections, no Additional Section, and
they are listed in Table 1. The reputation features consider
three aspects from statistical point of view: the reputations
of a domain itself, its authority, and its ASN. Machine
learning is applied to build a URL filtering mechanism based
on the three reputation aspects.

3.2.1.

Domain-based features

CNAME (canonical name) record specifies an alias, CNAME,
of a domain. For example, www.google.com.tw is an alias for
CNAME www.google.com, from the illustration shown in
Fig. 4. CNAME can be used for load balancing. Therefore, to
provide good quality of service, many famous web sites
apply CNAME to redirect traffic to multiple hosts or content
delivery network (CDN) service providers which deliver the
content to end users. However, some malicious web sites
may utilize such fast-flux domain technique to evade
detection. As mentioned above, our preliminary analysis
extracts various features based on DNS query results and
attempts to identify efficient ones. Based on our preliminary
analysis shown in Fig. 5, the use of CNAME reduces the
probability of malicious web sites. Therefore, the number of
CNAME records is defined as a feature for domain
reputation.
Botnet often adopts fast-flux domain technology to maintain a reliable communication channel between bots and
command and control server (C&C server) (SECURELIST). Such
domains normally have short expiration dates and meaningless domain names (Dmoz,; Real free websites, 2008), such as
0c7k29.co.cc. However, the information relies on the correctness and consistency of WHOIS database and the availability
of the query service to obtain the life time of a domain, and not
every DNS zone provides such query service, for example
edu.tw and gov.tw do not provide such service. To extend the
applicability of the proposed solution, instead of using WHOIS
query result, this study proposes two fast-flux domain

features based on meaningless domain names: Numeric Ratio
and Longest Meaningful Substring Ratio (LMSR), to identify
such malicious domains which often have meaningless
domain names combining random numbers and letters.
Numeric Ratio computes the ratio of the number of digits in
the target domain to the total length of the domain. Longest
Meaningful Substring Ratio indicates the ratio of the longest
meaningful token to the total length of the domain. The
calculation is defined as follows, where td is the target domain
to be examined.
td ::¼ ftoken:g
token ::¼ fcharacterg
character ::¼ letterjdigit
letter ::¼ “A”j“B”j:::j“Z”j“a”j“b”j :::j“z”j“ ”j“ À ”
digit ::¼ “0”j“1”j:::j“9”
PLENðtoken jÞ
Á
À
pðcharacteriÀ1 ; characteri Þ
Á
À
2 À Gram tokenj ¼ i¼2
LEN tokenj À 1


À
Á
Max
∪ 2 À Gram tokenj
j¼1…N
LMSRðtdÞ ¼
LENðtdÞ
As commercialization of the Internet is prevailing, many
businesses have the needs of obtaining higher level domain
and more freedom of naming rules. Therefore, DLDs (Delegation Level Domains) emerge and provide the above service
for businesses, but they also may provide ambiguous domains, for example, domain fr.pl using the country code of
France, ‘fr’, which might be considered as a domain in
France. According to our preliminary analysis shown in Fig. 6,
the rates of malicious web sites in ccTLDs and gTLDs are less
than 50%, while more than 60% of DLDs contain the
malicious.

3.2.2.

Authority-based features

Many domain name registration agents offer very cheap
membership fee (DominateSEO.net,; xtgem.com/Operator,;
Smallbusiness.yahoo). Hacker might buy many domain
names with low cost and performs malicious operations on
those many domains to evade blacklist detection. To discover
the effect of such domain registration agents, the authority
based features are extracted from NS-Type record whose information relates to domain registrar.
A domain d consists of a number of tokens connected by
“.”, where the string with the two rightmost tokens is called
0.7
Malicious website ratio

The probability of a
malicious web site

0.7
0.6
0.5
0.4
0.3
0.2

0.1
0
CNAME=0 CNAME=1 CNAME=2 CNAME=3 CNAME=4 CNAME=5

Fig. 5 e The probability of a malicious web site in different
numbers of CNAME records.

0.6
0.5
0.4
0.3
0.2
0.1
0
gTLD

ccTLD

DLD

Fig. 6 e Malicious website ratio in different level domains.

32

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

À Á
LD td ¼

&

2LDðtdÞ;
3LDðtdÞ;

Level domain type ¼ gTLD or DLD:
Level domain type ¼ ccTLD:

According to our preliminary analysis, the benign of the
same level domain are clustered together and so are the
malicious. By applying either partition-based or densitybased clustering, the results shown in Fig. 7 demonstrate
the clustering phenomena and that 74.8% of domains can be
clustered. Density-based clustering DBSCAN is adopted, as it
does not require for parameter k and is more resistant to
noise.
The proposed authority reputation feature, Rogue(td), is
defined as the ratio of the number of the malicious domains
to the total number of the domains in the level domain LD(td),
where td is the target domain to be examined. According to
the previous researches (Sadan and Schwartz, 2011), an
abused domain name registration agent contributes more
malicious domains and will result in a higher Rogue value in
our study. This novel authority-based feature is obtained
from statistic view point of registrar authority and has not
been applied before in the previous work (Sadan and
Schwartz, 2011; Antonakakis et al., 2010) which relied on
WHOIS database, TTL time (godaddy.com,; Caglayan et al.,
2009), or a large amount of malicious domains from the
blacklists. The amount of the required training data is less

0.6
Malicious website ratio

the second level domain (2LD), and that with the three rightmost tokens is called the third level domain (3LD). For
example, 2LD(www.nsysu.edu.tw) ¼ edu.tw and 3LD(www.
nsysu.edu.tw) ¼ nsysu.edu.tw. Level domain information
can be obtained from NS-Type record of dig query result. According to the type of level domain, this study defines a level
domain function, LD(td), used to find the norm of the level
domain, where the norm is used to indicate the reputation of
the domain registrar and td denotes the target domain to be
examined. Domains registered in ccTLD contain country code
and, hence, increase one level in the function. Therefore, the
third level domain is used for ccTLD; otherwise, the second
level domain is applied. The function of level domain is
formulated as follows.

0.5

0.4
0.3
0.2
0.1
0
LD(d)= mode ASN(d)

LD(d)≠ mode ASN(d)

Fig. 8 e Malicious domain ratio with respect to Mode
Equivalence Ratio.

than the previous work (Sadan and Schwartz, 2011;
Antonakakis et al., 2010) as well.

3.2.3.

ASN-based features

An Autonomous System Number (ASN) uniquely identifies
each network on the Internet (Tyagi and Aghila, 2012). The
number of ASNs is often used for detecting fast-flux domains,
as the IP addresses of a fast-flux domain, mainly victim hosts,
are usually scattered around the world and belonged to
different ASNs. An illustration can be found in (IANA).
Furthermore, this study discovered that, in a well-established
organization, its domain names cluster together in its ASN.
For example, National Sun Yat-sen University whose ASN is
17713. It composes of many websites, such as www.nsysu.
edu.tw, www.cs.nsysu.edu.tw, lis.nsysu.edu.tw, and etc.
Their level domains are the same, nsysu.edu.tw, and cluster
together. In this case, nsysu.edu.tw represents as the mode in
the ASN. Good reputed organizations use the same domain
name to represent their brand, while malicious domain
names tend to be dynamic, random, and diversified to evade
detection and exhibit no mode or less significant mode.
Therefore, a novel feature, Mode Equivalence Ratio (MER), is
proposed, which applies mode in statistic to calculate the
clustering effect of the level domains in the ASN. It is defined
as the ratio of the number of the malicious domains whose
level domain is not equivalent to the mode of the ASN to the
total number of the domains in the ASN. MER(td) can be

Fig. 7 e Domain clustering in the same level domain.

33

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

28.41%

30.00%

Table 3 e Some clusters with the size >100 and Rogue
>0.95.

Error rates

25.00%
20.00%

18.38%

19.03%
14.35%

15.00%

11.01%

9.10%

10.00%
5.00%
0.00%
FS1

FS2

FS3

Center of cluster

No. of
benign

No. of
malicious

Rogue

above.com
cnmsn.com
dsredirection.com
co.cc
cz.cc
ripside.com
shopco.com
t35.net
taloha.tk

5
4
8
1
1
2
2
3
0

244
142
148
769
460
104
506
183
558

0.98
0.97
0.95
1
1
0.98
1
0.98
1

16.05%

FS1+FS2 FS1+FS3 FS2+FS3

ALL

FS1: Domain-based feature set(F1, F2, F3, F4, F5)
FS2: Authority-based feature set(F6, F7)
FS3: ASN_based feature set(F8, F9, F10, F11, F12)
ALL: FS1+FS2+FS3

Fig. 9 e Error rates on different combinations of feature
sets.

formulated as follows, where td is the target domain, ASN(d)
denotes the ASN of domain d and mode(n) represents the
mode of ASN n.

samples, total of 63,225. The training data has been verified by
a third party online service, McAfee SiteAdvisor (Huang, 2008).
C4.5 decision tree based classification algorithm is applied to
train the filtering. C4.5 applies information gain (IG) to select
the best feature which can split samples effectively (McAfee),

À Á jfdjd is malicious&LDðdÞsmodeðASNðtdÞÞ&ASNðdÞ ¼ ASNðtdÞgj
:
MER td ¼
jfdjASNðdÞ ¼ ASNðtdÞgj

Fig. 8 shows the probability that a domain is malicious if its
level domain is equivalent/inequivalent to the filtering model
in the ASN. The preliminary analysis demonstrates that the
chance that a web site is malicious reduces if its level domain
equals to the mode. This novel feature has not been proposed
in any previous researches and the preliminary results indicate that it is reliable on identifying the malicious domains.
The level domains of all the domain names in an ASN can
be considered as a discrete probability distribution, so the
mode of the function has the maximum value. For an ASN
whose domains have the same level domain, such as 17713,
the ASN of nsysu.edu.tw, the distribution function is very
dense with a high peak at the mode. On the other hand, an illreputed ASN mostly contains random domain names and its
level domain exhibits an approximate uniform distribution
with a low peak at its mode. Feature Degree of Centralization
(DoC) indicates how density the distribution function is at its
mode. DoC of an ASN is defined as the ratio of the number of
the domains whose level domain equals to the mode to the
total number of the domains in the ASN.

3.3.

Reputation-based filtering

The labeled training data used in this study was collected from
the following sources: Clean mx, Malwaredomainlist, and
Phishtank for the malicious samples, total of 14,931 collected,
and Alexa and Open Directory Project (Dmoz) for the benign

Table 2 e The detection performance of the filtering.
Accuracy
90.9%

Precision
93.6%

and its criterion is information gain, the difference in entropy,
where entropy indicates the quantitative measure of disorder
in an attribute.

4.

Performance evaluation

According to the literature, only 1.3% of search results are
malicious. Even though client honeypot is a reliable approach
to identify malicious websites, identifying malicious websites
among enormous websites in the Internet is time consuming
and resource exhausting. The proposed two-step detection
firstly identifies the suspicious and then analyzes them
further through sandboxing. Such design reserves most resources inspecting the suspicious websites and retains the
advantage of reliable detection from honeypot solution. An
open source client honeypot solution is applied for performance evaluation. The performance evaluation has three
folds: (1) evaluating the efficiency of the filtering; (2)
comparing the performance of the proposed two-stage
detection with an improved execution-based solution; (3)
and evaluating the practicality of the system by deploying to a
real network.

4.1.

Validation of the proposed filter

Total of 18000 websites are chosen from the collected data:
50% of the benign and 50% of the malicious (Cho, 2002; Zheng
et al., 2013). Ten-fold cross validation, commonly used in
estimating the performance of a proposed model (Longadge
et al., 2013), is adopted to validate the filtering rate and the
results are given in Table 2. Besides, TN, TP, FP, and FN are
considered as performance indexes, accuracy and precision

34

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

03:50

Table 4 e The parameter settings of Capture-HPC.

03:21

Value

client-default
client-default-visit-time
group_size
vm_stalled_after_revert_timeout
revert_timeout
client_inactivity_timeout
vm_stalled_during_operation_timeout

iexplore
90
20
600
600
600
600

02:52
Time(hh:mm)

Parameters

02:24
01:55
01:26
00:57
00:28
00:00
1000

2000

3000

4000

5000
6000
7000
Number of URLs

8000

9000

10000

Fig. 11 e The process time of the filtering.

Divid and Conquer
2-stage Detection
120000

Time(sec)

100000
80000
60000
40000
20000
0
1000 2000 3000 4000 5000 6000 7000 8000 9000
Number of URLs

Fig. 10 e Performance comparison with divide-andconquer.

are also adopted and defined below. The filtering has the accuracy rate2 of 90.9% classifying correctly the websites, a high
precision rate (93.6%), and a low false positive rate (5.8%)
misclassifying benign to malicious. If the test data simulates
the real network traffic mixture, the proposed filtering yields
over 94% accuracy rate.
The effects of different combinations of feature sets were
evaluated and the error rates are shown in Fig. 9. The domainbased and authority-based feature sets, (FS1 and FS2), are
indistinct. According to the information provided by the NSType record, a domain belonged to a cluster of a higher
value of the authority feature, Rogue, implies that it has a
higher possibility to be malicious. Cheap domain registrar in
the Internet becomes criminal incubator where most malicious domains reside in. As illustrated in Table 3, the ratio of
malicious domains in a cluster from a cheap ill-reputed
domain registrar is high and so is feature Rogue. It proves
the significance of this novel authority feature proposed in
this study. It can be observed that such cluster mostly is
resided by the malicious domains.

4.2.

Performance comparison

In order to observe the performance contribution of the proposed two-stage detection, the experiment is divided into two
parts: Capture-HPC with the two-stage detection (SD) and that
with divide-and-conquer (DC), where DC is an improved
honeypot solution proposed by Seifert and Steenson (2006). To
better simulate a real network environment, the experimental
2

Accuracy
¼
Precision ¼ TP=TP þ FP

TP þ TN=TN þ FN þ FP þ TP;

data used in this comparison was collected from a real
campus network and sampled randomly in thousands of
distinct URLs. The experimental parameters of Capture-HPC
are set as indicated in Table 4.
The results shown in Fig. 10 indicate that, as the number of
HTTP requests increases, the base method (DC) consumes
much more time while the process time of the proposed twostage detection (SD) increases insignificantly. The proposed
two-stage detection improves the process time 12.9 times
better than the base method, as it filters out most benign web
sites efficiently and reduces the process time on verifying
legitimate websites. Fig. 11 shows the process time consumed
by the filtering stage, no sandboxing. The results demonstrate
that the process time increases linearly as the input increases.
Therefore, the proposed two-stage detection has better scalability and performs better in a network with a large amount
of web traffic.

4.3.

Real network deployment

The proposed system was deployed on a 10 class C real
network, where the pattern of http traffic volume is summarized in Fig. 12 and is repeated in weekly basis in the real
network. The successful http requests (with the return code
200) in a week were collected, about four million URLs with the
average of 560 thousand requests per day. Again, Capture-HPC
was selected for the sandboxing stage. From the results of the
process time per day shown in Table 5, the proposed solution
spent an average time of 14 hours per day to handle all the
web traffic and less than 21.3 hours during a high peak data
volume day. It demonstrates that the proposed system is
applicable to a real network and can handle a large amount of
HTTP traffic efficiently.
During the real deployment, unknown attacks were
discovered by the proposed detection system, which were not
able to identify by the blacklist or the current signature-based
intrusion detection system in the real network. One of the
identified cases was that www.111hola.com/showpic.asp was
detected as a drive-by download and it downloaded and
executed f.exe and created 26106750.dll. The target web site
was sent to VirusTotal for cross verification and the return
results show that several anti-virus vendors indicate its
maliciousness.
Further investigation confirmed that it was an unknown
malicious web site, as cleanmx, malware domain, phishtank,
and SiteAdvisor have no record of the target site and SiteAdvisor put it in the queue. SiteAdvisor's indication that it is in

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

35

Fig. 12 e Real http requests per day.

the waiting list for inspection implies that improper handling
of the URLs to be examined may stall or lengthen the detection
and the malicious sites might not be able to be identified in a
timely manner. The experimental results prove that the proposed solution can improve the time efficiency of the
execution-based approach and makes it applicable for the
network with large scale web traffic. In addition, it is able to
identify unknown malicious websites which is not be listed in
the public available blacklist websites.

5.

Conclusions

Sandboxing or honeypot is a reliable approach to examining
malicious web sites, but its time efficiency is poor. It is
impractical to deploy such execution-based approach in real
networks, because it consumes huge resources and
computing time. This study proposes a reputation-based URL
filter examining the reputation of the target domain and
filtering out most of the benign. The proposed reputation
system requires no WHOIS data, as some name servers do not
provide the query service to WHOIS database. The proposed
reputation system adopts A-Type and NS-Type records and
the experimental results demonstrate that the proposed
authority-base feature set is reliable.
The experimental results indicate that the proposed
approach saves a lot of process time and improves more as the
amount of web traffic increases. It outperforms the base
method, divide-and-conquer, by about 13 times faster. The

Table 5 e The process time of the two-step detection per
day.
Day

Process time
(in hh:mm:ss)

Day1
Day2
Day3
Day4
Day5
Day6
Day7

21:16:20
20:26:04
17:02:11
16:08:28
5:42:23
6:55:23
8:35:46

deployment on the real network discovered unknown malicious sites which have not yet listed in the commonly used
security web sites and have not detected by the detection
system or the blacklist used in the real network. Using the real
HTTP requests can observe actual user browsing habits and
discover unknown drive-by download sites efficiently.
Each URL request was regarded as an independent event in
this study. However, dependency relationship might exist
among the URL requests by the same user, as a user may visit
a sequence of web pages from a single web server. A malicious
web server might contain more than one contaminated web
pages. Further investigation can be conducted on correlating
the requests to improve the performance of the reputation
system.

references

Alexa. Alexa the Web Information Company, http://www.alexa.
com/.
Antonakakis M, Perdisci R, Dagon D, Lee W, Feamster N. Building
a dynamic reputation system for DNS. In: Proceedings of
USENIX Security Symposium; 2010.
Caglayan A, Toothaker M, Drapeau D, Burke D, Eaton G. Detection
of fast flux service networks. In: Proceedings of Cybersecurity
Applications & Technology Conference For Homeland
Security. IEEE; 2009.
CENZIC. Web Application Security Trends Report, http://www.
cenzic.com/downloads/Cenzic_AppSecTrends_Q3-Q4-2010.
pdf.
Chen CM, Cheng ST, Chou JH. Detection of fast-flux domains. J
Adv Comput Netw June 2013;1(No.2).
Cho SB. Exploring features and classifiers to classify gene
expression profiles of acute Leukemia. Int J Pattern Recogn
2002;16(No. 7):831e44.
Christodorescu M, Jha S, Seshia SA, Song D, Bryant RE. Semanticsaware malware detection. In: Proceedings of USENIX Security'
05; 2005. p. 32e46.
CLEAN MX. CLEAN MX realtime database, http://support.cleanmx.de/clean-mx/viruses.
Dhanalakshmi R, Prabhu C, Chellapan C. Detection of phishing
websites and secure transactions. Int J Commun Netw Secur
(IJCNS) 2011;1.
Dmoz. Open Directory Project, http://www.dmoz.org/.
DominateSEO.net. Buy Deleted Domain to Give You New Business
A Boost, http://dominateseo.net/deleted-domains.

36

j o u r n a l o f i n f o r m a t i o n s e c u r i t y a n d a p p l i c a t i o n s 2 0 ( 2 0 1 5 ) 2 6 e3 6

http://www.godaddy.com/domains/get-a-website-Globe-2.aspx?
isc¼gtnftw01.
Gruener W. Google: Anti-virus Software Needs to Share Up. 2008.
http://www.tomsguide.com/us/google-anti-virus,news-603.
html.
Hallaraker O, Vigna G. Detecting malicious Javascript code in
mozilla. In: Proceedings of the 10th IEEE International
Conference on Information, Communications and Signal
Processing (ICECCS 2005); 2005. p. 85e94.
Huang MZ. Hybrid botnet detection. Master thesis. National Sun
Yat-Sen University; 2008.
IANA. Internet Assigned Numbers Authority, http://www.iana.
org/numbers.
Jim T, Swamy N, Hicks M. Defeating script injection attacks with
browser-enforced embedded policies. In: Proceedings of the
International World Wide Web conference; 2007. p. 601e10.
Lin SF, Hou YT, Chen CM, Jeng B, Laih CS. Malicious webpage
detection by semantics-aware reasoning. In: Proceedings of
The International Conference on Intelligent Systems Design
and Applications; 2008. p. 115e20.
Longadge R, Dongre SS, Malik L. Class imbalance problem in data
mining. Int J Commun Netw Secur (IJCNS) 2013;2(1).
Malware Domain List. Malware Domain List, http://www.
malwaredomainlist.com/.
McAfee. MaAfee SiteAdvisor, http://www.siteadvisor.com/.
Moshchuk A, Bragin T, Gribble SD, Levy HM. A crawler-based
study of spyware on the web. In: Proceedings of the 2006
Network and Distributed System Security Symposium (NDSS);
2006. p. 17e33.
Netcraft, http://searchdns.netcraft.com/.
Ourston D, Matzner S, Stump W, Hopkins B. Applications of
hidden Markov models to detecting multi-stage network
attacks. In: System Sciences, Proceedings of the 36th Annual
Hawaii International Conference; 2003.
Phishtank. Phishtank, http://www.phishtank.com/.
Porras P, Saida H, Yegneswarn V. Conficker C analysis. 2009. SRI
International Technical Report.
Provos N, McNamee D, Mavrommatis P, Wang K, Modadugu N.
The ghost in the browser analysis of web-based malware. In:
Proceedings of the first workshop on hot topics in
understanding botnets, Cambridge; 2007.
Provos N, Mavrommatis P, Rajab MA, Monrose F. All your
iFRAMEs point to us. In: Proceedings of the 17th conference on
Security symposium; 2008.
Rajkumar N, Jaganathan P. A new RBF kernel based learning
method applied to multiclass dermatology diseases
classification. In: Proceedings of 2013 IEEE Conference on

Information and Communication Technologies (ICT 2013);
2013.
Real free websites. How to choose a domain name. 2008. http://
www.realfreewebsites.com/articles/how-to-choose-adomain-name/.
Reis C, Dunagan J, Wang HJ, Dubrovsky O, Esmeir S.
BrowserShield: vulnerability-driven filtering of dynamic
HTML. In: Proceedings of the 7th USENIX Symposium on
Operating Systems Design and Implementation; November
2006.
Sadan Z, Schwartz DG. WhiteScript: using social network analysis
parameters to balance between browser usability and
malware exposure. Computer & Security 2011;30(1):4e12.
SECURELIST. Exploit Kits e A Different View, http://www.
securelist.com/en/analysis/204792160/Exploit_Kits_A_
Different_View.
Seifert C, Steenson R. CaptureeHoneypot Client (Capture-HPC).
NZ: Victoria University of Wellington; 2006.
Seifert C, Welch I, Komisarczuk P, Aval CU, Popovsky BE.
Identification of malicious web pages through analysis of
underlying DNS and web server relationships. In: Proceeding
of the 33rd Annual IEEE Conference on Local Computer
Networks; 2008.
http://smallbusiness.yahoo.com/.
Spitzner L. Honeypots: Tracking hackers. Addison Wesley; 2002.
Tyagi AK, Aghila G. Detection of fast flux network based social bot
using analysis based techniques. In: Proceeding of Data
Science & Engineering(ICDSE) informational Conference; July
2012.
Wang Y-M, Beck D, Jiang X, Roussev R, Verbowski C, Chen S, et al.
Automated web patrol with strider HoneyMonkeys: finding
web sites that exploit browser vulnerabilities. In: 13th Annual
Network and Distributed System Security Symposium; 2006a.
Wang YM, Beck D, Jiang X, Roussev R, Verbowski C, Chen S, et al.
Automated web patrol with strider HoneyMonkeys. In:
Proceedings of the 2006 Network and Distributed System
Security Symposium; 2006b. p. 35e49.
http://mayoup555.xtgem.com/Operator%20GSM/Www.co.cc.
Yu D, Chander A, Islam N, Serikov I. JavaScript instrumentation
for browser security. In: POPL '07: Proceedings of the 34th
Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages; 2007. p. 237e49.
Yuan B. Client-side honeypots. University of Mannheim; 2007.
Master's thesis.
Zheng C, Chen S, Wang W, Lu J. Using principal component analysis
to solve a class imbalance problem in traffic incident detection.
Mathematical problems in engineering, Vol. 2013; 2013.

