Artiﬁcial Intelligence 175 (2011) 1815–1855

Contents lists available at ScienceDirect

Artiﬁcial Intelligence
www.elsevier.com/locate/artint

Solving conﬂicts in information merging by a ﬂexible interpretation
of atomic propositions
Steven Schockaert a,∗,1 , Henri Prade b
a
b

Ghent University, Department of Applied Mathematics and Computer Science, Krijgslaan 281, 9000 Gent, Belgium
Toulouse University, Université Paul Sabatier, IRIT, CNRS, 118 Route de Narbonne, 31062 Toulouse Cedex 09, France

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 20 July 2010
Received in revised form 21 April 2011
Accepted 23 April 2011
Available online 28 April 2011
Keywords:
Information fusion
Similarity-based reasoning
Prioritized knowledge bases
Possibilistic logic
Penalty logic

Although many techniques for merging conﬂicting propositional knowledge bases have
already been proposed, most existing work is based on the idea that inconsistency results
from the presence of incorrect pieces of information, which should be identiﬁed and
removed. In contrast, we take the view in this paper that conﬂicts are often caused
by statements that are inaccurate rather than completely false, suggesting to restore
consistency by interpreting certain statements in a ﬂexible way, rather than ignoring
them completely. In accordance with this view, we propose a novel approach to merging
which exploits extra-logical background information about the semantic relatedness of
atomic propositions. Several merging operators are presented, which are based on different
formalizations of this background knowledge, ranging from purely qualitative approaches,
related to possibilistic logic, to quantitative approaches with a probabilistic ﬂavor. Both
syntactic and semantic characterizations are provided for each merging operator, and the
computational complexity is analyzed.
© 2011 Elsevier B.V. All rights reserved.

1. Introduction
In applications where information from different sources needs to be combined, conﬂicts are often the rule rather than
the exception. The presence of conﬂicts requires special attention, as it casts doubt on the reliability of available information.
Even worse, if information is encoded in classical logic, the combined pieces of information become trivial, as anything can
be derived from contradiction. To accommodate the possibility of conﬂicts in a more useful way, a wide array of approaches
has been proposed in the literature, ranging from purely syntactic approaches to semantic operators that manipulate sets of
interpretations. In general, the problem of information merging has been studied both in logical and in numerical settings.
An example of the latter case are situations where different probability or possibility distributions need to be fused [1,2].
This paper focuses exclusively on merging in a logical setting.
A common idea underlying many approaches to logical information merging is to get rid of the least reliable pieces of
information. The exact mechanism being employed may, among others, be based on prior knowledge about the reliability
of different pieces of information and of sources [3,4], on discriminating between pieces of information according to the
number of supporting sources [5], or on the dialectical principles of argument and counter-argument [6]. The essential
point of view underlying such approaches is that conﬂicts are caused by errors that are in some sense arbitrary: any piece
of information has some chance of being wrong, and agreement between sources (or prior knowledge) is all that can help
us to decide which pieces are more likely to be correct. A closely related idea is to weaken the information that is provided

*
1

Corresponding author.
E-mail addresses: steven.schockaert@ugent.be (S. Schockaert), prade@irit.fr (H. Prade).
Postdoctoral fellow of the Research Foundation – Flanders (FWO).

0004-3702/$ – see front matter
doi:10.1016/j.artint.2011.04.001

© 2011

Elsevier B.V. All rights reserved.

1816

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

by each of the sources [7–10]. For instance, if some source claims p ∧ q, we may change this, among others to p ∨ q. This
may be motivated in several ways. If the sources express conﬂicting goals or preferences, for instance, we may consider that
each competing source needs to concede to arrive at a feasible global strategy. Alternatively, when sources express beliefs,
we may consider that weaker information is more reliable, and that by progressively weakening these beliefs we ultimately
end up with beliefs that are correct, and thus consistent. Without extra-logical information, however, approaches based on
this idea tend to be rather coarse, often depending on the assumption that all atoms in the language encode properties that
are approximately of equal importance in the domain being modeled.
A quite different approach to dealing with conﬂicts is to relax the assumption that sources need to be combined conjunctively. Indeed, if individual sources are consistent, combining them disjunctively trivially restores consistency. Starting
from this basic idea, more reﬁned techniques have been developed, e.g. based on disjunctively combining conjunctions of
maximal consistent subsets of knowledge bases [11,12]. Paraconsistent logics [13] offer yet another solution to the problem
of conﬂict. Rather than trying to modify the knowledge bases, the logic itself is changed such that only non-trivial conclusions can be derived, even in the face of logical contradiction. The fact that both p and ¬ p may be entailed by a non-trivial
theory may, in some paraconsistent logics (e.g. the logic of formal inconsistency [14]), be interpreted as evidence that the
property modeled by p is controversial (or ill-deﬁned, vague, etc.) in which case it is natural that different sources may
have a different standpoint regarding p. A consequence of this methodology is that, contrarily to most other methods, there
is no real loss of information when conﬂicts arise. Indeed, when p is asserted by some source and ¬ p by another source,
we do not give up our belief in p but rather gain the insight that p is controversial.
In this paper, we are exploring a new direction, which is motivated by the fact that in real-world applications, random
errors occur side-by-side with conﬂicts that are due to the use of properties that may be understood differently by different
sources (e.g. vague properties such as ‘tall’). The method being used to deal with conﬂicts is then not only determined by
the nature of conﬂicts, but also — and perhaps even especially — by the nature of available background knowledge. Logics of
formal inconsistency, for instance, require that some atoms are designated as uncontroversial, to ensure meaningful results.
In general, most methods assume no, or very little prior knowledge, making them widely applicable, but at the same
time limiting their ability to correctly identify the real cause of conﬂicts, and thus, ultimately, their usefulness. A similar
consideration applies to the problem of belief revision, for which it is well known that extra-logical information about the
epistemic state of an agent (e.g. in the form of an epistemic entrenchment ordering) is key to meaningful results [15]. For
the task of merging the beliefs expressed by different sources, however, the use and importance of extra-logical information
is less well-understood. Nonetheless, there are many situations where appropriate background knowledge is paramount in
correctly dealing with conﬂicts.
Example 1. Consider a situation where predictions are available about tomorrow’s weather from three well-reputed sources.
Predictions from all sources are expressed in a propositional language over the set of atoms {overcast, partiallyCloudy,
openSky}, subject to the integrity constraint that overcast, partially cloudy, and open sky are Jointly Exhaustive and Pairwise
Disjoint atoms (a property called JEPD in the following):

K 1 = {partiallyCloudy ∨ overcast}

(1)

K 2 = {openSky}

(2)

K 3 = {overcast}

(3)

If all three sources are considered equally reliable, classical merging strategies would either yield the trivial result overcast ∨
partiallyCloudy ∨ openSky, or conclude overcast, which is the only atom that is compatible with the majority of the sources.
Since all three sources are well-reputed, however, the extreme conclusions openSky and overcast seem less plausible than the
intermediate conclusion partiallyCloudy. However, without additional information, encoding this idea of being intermediate,
there are no reasons to prefer partiallyCloudy over overcast or openSky.
The core idea in this example is that atoms such as overcast should be understood in a ﬂexible way. The need for
ﬂexibility regarding the meaning of atoms may stem from different causes, including all of the following:
1. Sources are overconﬁdent, and the assertions they make are too precise, given the knowledge they actually have. In the
example above, it is clear that people prefer more informative weather reports (e.g. it will be sunny tomorrow), with
the risk of being slightly wrong from time to time, over completely honest but less informative or uninformative reports
(e.g. it may be sunny or cloudy).
2. Atoms refer to properties for which precise, generally accepted deﬁnitions are lacking. Typically these are properties
that depend on threshold values in some continuous domain, or properties whose deﬁnition may slightly vary with the
context. For instance, there may be situations that are described as an open sky by some people and partially cloudy
by others.
3. Atoms refer to terms with a well-understood meaning, which are nonetheless used in a more liberal, or more restrictive
way in certain contexts. It is not hard to imagine, for example, a person in a civil union answering aﬃrmatively to
the question “Are you married (yes/no)?”, e.g. when ﬁlling in a web form. As another example, the term Asian is often

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1817

reserved to refer to people from South Asia in the UK, or the people from East Asia in the USA, thus restricting the
actual meaning of the term.
4. Atoms refer to ambiguous terms, which may mean completely different things. The term “public school”, for instance, is
used with an entirely different meaning in the UK and USA; also, different people, places, or events are often described
by the same name; etc.
The underlying idea common to the aforementioned causes of conﬂict is that there are some countermodels of a statement that are in some sense similar to models of the statement. Given that the statement is asserted by a reliable source,
we may then consider that normally, the actual state of affairs is described by one of the models of that statement, although
exceptionally, it might also be described by one of these similar countermodels. The aim of this paper is to formalize this
intuition, leading to merging operators for propositional knowledge bases that effectively take available background information about the relatedness of atoms into account.
The paper is structured as follows. In the next section, we provide the required background on a number of wellknown approaches to information merging in the propositional setting: distance-based merging, conﬂict-based merging
and morpho-logical merging. Next, Section 3 introduces the basic ingredient of our approach, i.e. background information
about the semantic relatedness of atoms. After outlining the main idea, it is shown how relatedness can be represented
at the syntactic level and at the semantic level, and what the correspondences are between both representations. What
results is a general framework, in which different types of relatedness can be expressed. To cope with this generality, four
prototypical scenarios are presented in Section 4. By showing how the general framework can be instantiated to implement
each of these scenarios, we bridge the gap between the general, but abstract framework and practical applications. In
Section 5 we subsequently turn to the merging problem itself. We show how different merging operators naturally arise
from different interpretations of the weighted knowledge bases that encode how different atoms are related. After deﬁning
the merging operators at the syntactic level, we provide semantic characterizations that reveal close links with existing
merging operators. The computational complexity of the merging operators is studied in Section 6, after which we present
our conclusions.
2. Background on propositional merging
In the following, we consider a propositional language built from a ﬁnite set of atoms A and the connectives ∨, ∧, →,
≡, ¬ in the usual way. An interpretation I is deﬁned as a subset of atoms a in A, where I | a for an atom a iff a ∈ I . An
interpretation is said to be a model of a formula (resp. set of formulas) if it satisﬁes that formula (resp. every formula in
the set) in the usual sense. We write ❏φ❑ to denote the set of all models of a formula φ .
We will also need a few order-theoretic concepts. Given a relation
in a universe U , we write min(U , ) for the set of
elements from U that are minimal w.r.t. , i.e.

min(U ,

) = {u ∈ U | ¬∃ v ∈ U . v

u∧u

v}

Given a list of relations 1 , . . . , n in U , we write par(
graphic extensions respectively, i.e.

(u , v ) ∈ par(
and (u , v ) ∈ lex(

1, . . . ,

1, . . . ,

n)

n)

iff ∀i ∈ {1, . . . , n} . u

iff either (u , v ) ∈ par(

∃k ∈ {1, . . . , n} . ∀i ∈ {1, . . . , k − 1} . u

i

1, . . . ,

1, . . . ,

i

n)

and lex(

1, . . . ,

n)

to denote their Pareto and lexico-

v
n)

or

v ∧ (u <k v )

where u <k v is used as a shorthand for u k v ∧ v k u. Note that in the case of the lexicographic ordering, the numbering i
of the relations i encodes an ordering of their importance.
Let K 1 , . . . , K n be propositional knowledge bases that are individually consistent, and let C be a set of integrity constraints. The purpose of a merging process is to ﬁnd one knowledge base ( K 1 , . . . , K n ) which is consistent with the integrity constraints (i.e. ❏ ( K 1 , . . . , K n )❑ ⊆ ❏C ❑), and which integrates the information from the knowledge bases K 1 , . . . , K n
to the best extent possible.
A variety of methods to obtain a suitable knowledge base ( K 1 , . . . , K n ) have already been studied. For instance, there
is a long tradition in inconsistency management to restore consistency by identifying maximal consistent subsets [16]. In
particular, we may deﬁne ( K 1 , . . . , K n ) to be the disjunction of (a subset of) these maximal consistent subsets (where a set
of formulas is treated as a conjunction). Such approaches, however, mainly attempt to isolate the inconsistency, rather than
reconciling any conﬂicting views that different sources may hold. Our approach will therefore be based on a different line of
work, which tries to resolve inconsistencies using the idea that the models of ( K 1 , . . . , K n ) should be those interpretations
that are in some sense close to the models of the knowledge bases K 1 , . . . , K n . Next, we brieﬂy recall three important
classes of existing merging operators that are based on this idea, and will play a role in the remainder of this paper.

1818

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

2.1. Distance-based merging
A common strategy is to deﬁne merging operators based on a pseudo-metric d on interpretations [8,5,17]. Although d is
not required to satisfy the mathematical properties of a metric (e.g. triangle inequality or even symmetry), it is common to
refer to d as a distance. Given two interpretations I and J , the well-known Hamming distance dHam is often used to this
end:

dHam ( I , J ) = ( I \ J ) ∪ ( J \ I ) = | I \ J | + | J \ I |
i.e. dHam ( I , J ) is equal to the number of atoms on which interpretations I and J disagree. The Hamming distance is sometimes also called Dalal distance, as Dalal ﬁrst proposed to use it in the context of belief revision [18]. A given distance d
allows us to quantify the distance between an interpretation I and a propositional knowledge base K :

d( I , K ) = min d( I , J )
J ∈❏ K ❑

Given an appropriate aggregation operation f , this can be extended to lists of knowledge bases K = ( K 1 , . . . , K n ):

d( I , K) = f d( I , K 1 ), . . . , d( I , K n )

(4)

where f may, for instance, be the sum, a weighted sum, or (reﬁnements of) the maximum. Now, a preorder
interpretations can be deﬁned as

I

d

iff d( I , K)

I

d

between

d I ,K

and the result of the merging process can semantically be deﬁned as the knowledge base whose models are those interpretations that are minimal w.r.t. this preorder:

q

dist

(K 1, . . . , Kn ; C ;

②

d)

= min ❏C ❑,

(5)

d

Although this class of merging operators is quite general and has a strong intuitive appeal, in practical applications it is
almost exclusively applied for d = dHam . An important limitation of dHam and related distances is that they are very sensitive
to the particular translation of a given problem into propositional logic [19]. In addition, such distances do not allow us to
express that two atoms have a meaning which is in some sense related.
2.2. Conﬂict-based merging
The motivating idea behind conﬂict-based merging [20] is that many merging operators derive from manipulating the
conﬂict set diff ( I , J ) between two interpretations I and J in the following sense:

diff ( I , J ) = ( I \ J ) ∪ ( J \ I )

(6)

The Hamming distance dHam ( I , J ), for example, is equal to the cardinality of this conﬂict set. By comparing conﬂict sets
in other ways than by their cardinality, other merging operators may be obtained. Similar to the distance-based merging
scheme, from conﬂict sets between interpretations, we may deﬁne the conﬂict between an interpretation and a knowledge
base [20]:

diff ( I , K i ) = min diff ( I , J )

J ∈ ❏Ki ❑ , ⊆

Note that since conﬂict sets may be incomparable with each other (w.r.t. set inclusion), diff ( I , K i ) may contain multiple
conﬂict sets c i . The conﬂict between an interpretation and a list of knowledge bases K = ( K 1 , . . . , K n ) is then represented
by a set of conﬂict vectors, i.e. vectors of conﬂict sets [20]:

diff ( I , K) =

c 1 , . . . , cn

c i ∈ diff ( I , K i )

The challenge now arises to deﬁne, from such conﬂict vectors, the models of the knowledge base that results from merging
K 1 , . . . , K n under the integrity constraints C . First, we need a relation conﬂ comparing conﬂict vectors. Although many
alternatives can be conceived, we will only consider the case where conﬂ is par (⊆, . . . , ⊆) in this paper. Note that this
choice essentially corresponds to a qualitative counterpart of the Hamming distance. Given a choice of conﬂ , we may
E
E
consider the relation conﬂ
between interpretations, deﬁned by I conﬂ
J iff

∃c ∈ diff ( I , K) . ∀c ∈ diff ( J , K) . c

conﬂ

c

It is then proposed in [20] to use the following merging operator:

q

conﬂ1

(K 1, . . . , Kn ; C ;

②

conﬂ )

= min ❏C ❑,

E
conﬂ

(7)

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1819

following the same intuition as the distance-based merging scheme: the models of the resulting knowledge base are those
models of the integrity constraints that are closest to each of the given knowledge bases. The main difference is that the
notion of closest is now seen in a more general perspective.
When applied to conﬂ = par(⊆, . . . , ⊆), however, this merging operator appears to be too tolerant, allowing more models than is intuitively required.
Example 2. Consider the situation of merging the single knowledge base K 1 , whose only models are J 1 = {c , d, e } and
J 2 = {a, b, c }, with the integrity constraints C , whose models are I 1 = {b, c , d}, I 2 = {c , d} and I 3 = {b, c }. The relevant
conﬂict sets are given by

diff ( I 1 , J 1 ) = {b, e },

diff ( I 1 , J 2 ) = {a, d}

diff ( I 2 , J 1 ) = {e },

diff ( I 2 , J 2 ) = {a, b, d}

diff ( I 3 , J 1 ) = {b, d, e },

diff ( I 3 , J 2 ) = {a}

Among all these conﬂict sets, only {e } and {a} are optimal w.r.t. ⊆. This means that, of all models of C , I 1 is neither the
closest to J 1 nor the closest to J 2 . On the other hand I 2 is the closest to J 1 and I 3 is the closest to J 2 . Therefore, it seems
appropriate to take { I 2 , I 3 } as models of the merged knowledge base. Using (7), on the other hand, leads to { I 1 , I 2 , I 3 }. The
reason is that I 1 is a minimal element of ❏C ❑ since

{e }

{a, d} and {a, b, d}
E
I ,
conﬂ 1

which means I 2

{b, d, e }

and

{b, e } and {a}

which means I 3

{a, d}

{b, e }

E
I .
conﬂ 1

To address the issue illustrated in the previous example, we propose the following alternative:

I∈

q

conﬂ2

( K 1 , . . . , K n ; C ; diff ,

②

conﬂ )

iff

I ∈ ❏C ❑ ∧ ∃c ∈ diff ( I , K) . ∀ I ∈ ❏C ❑ . ∀c ∈ diff I , K . c ≮conﬂ c (8)

where c <conﬂ c iff c conﬂ c and c conﬂ c . Note that when we apply (8) to the scenario in Example 2, we indeed ﬁnd
the desired result { I 2 , I 3 }. The operator in (8) directly encodes the idea that models of the merged knowledge base should
be those that are closest to the given knowledge bases, treating the conﬂict sets as qualitative (partially ordered) distances.
Note that the operator is parametrized by the conﬂict-operator diff , which will also allow us to consider alternatives to (6).
In general, while providing extra ﬂexibility, the conﬂict-based merging scheme has similar advantages and disadvantages as the distance-based approach. In particular, straightforward implementations do not allow to deal with semantic
background information about the relatedness of atoms.
2.3. Morpho-logical merging
Another interesting view on propositional merging [10] is to weaken propositions using a logical counterpart of mathematical morphology [21]. In particular, the dilation D B (φ) of a formula φ is deﬁned by

q

②

D B (φ) = I ∈ 2 A B ( I ) ∩ ❏φ❑ = ∅

(9)

where B ( I ) ⊆ 2 A is a set of interpretations that are related to I in some way. Note that B, which is called the structuring
element, can be regarded as a relation between interpretations. Again the Hamming distance is most commonly used,
choosing

B ( I ) = I ∈ 2 A dHam I , I

t

(10)

for some t ∈ N. This particular choice makes it straightforward to syntactically characterize D B (φ) (see [10]). The knowledge
bases K 1 , . . . , K n may be merged, subject to the integrity constraints C , using the operator morph deﬁned by:

q

morph

②
q
②
( K 1 , . . . , K n ; C ; B ) = ❏C ❑ ∩ ❏ D B ( K 1 )❑ ∩ · · · ∩ D B ( K n )

where the structuring element B is chosen such that consistency is effectively restored, i.e. ❏ morph ( K 1 , . . . , K n ; C ; B )❑ = ∅.
For instance, when using (10), we may choose the smallest value of t that restores consistency. Interestingly, in [22], the
morpho-logical approach to merging has been generalized to ﬁrst-order logic. In the propositional case, however, it is clear
that morpho-logical merging is similar in spirit as distance-based and conﬂict-based merging.

1820

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

3. Modeling heterogeneous vocabulary usage
Our approach to merging is based on interpreting statements that are provided by the sources in a ﬂexible way. Conceptually this boils down to weakening each of the considered knowledge bases, i.e. increasing their set of models. Below
we present the main ideas of our procedure. First, Section 3.1 elaborates on the relationship between weakening knowledge bases and interpreting atoms in a ﬂexible way. Section 3.2 then discusses how this idea can be implemented at the
syntactic level. Subsequently, Section 3.3 provides the semantic counterpart of the syntactic mechanism for weakening our
understanding of statements.
3.1. Weakening knowledge bases
Let K 1 , . . . , K n be sets of statements (knowledge bases) that are asserted by distinct sources s1 , . . . , sn , where each
source is assumed to use a propositional language over the same set of atoms A. We furthermore consider an additional
knowledge base C containing the integrity constraints of the domain being modeled. Throughout this paper, we will tacitly
assume that the set of integrity constraints C is consistent, and that each knowledge base K i is individually consistent.
Our goal then is to weaken the statements from the knowledge bases K 1 , . . . , K n , resulting in knowledge bases K 1 , . . . , K n
such that K 1 ∪ · · · ∪ K n ∪ C is consistent. The term weakening reﬂects the fact that all models of K i are contained in the
set of models of K i . While this general idea of weakening knowledge bases underlies many existing merging strategies, our
approach is particular in its use of extra-logical information and the idea of similarity. Speciﬁcally, our point of departure
is that some sources may have a slightly different understanding of the meaning of atom a. To formalize this intuition,
let us write a@si to denote the understanding of atom a by source si . In other words, a@si is an artiﬁcial notation that we
introduce to precisely capture what can be assumed to hold when source si claims a. Depending on the application context,
the atom a itself could then correspond to the oﬃcial meaning of a given term (assuming one exists), or to the particular
way this term is to be understood w.r.t. the integrity constraints in C . Moreover, given a subset of atoms X ⊆ A, we write
@s
X @si for the set {x@si | x ∈ X }. Given a knowledge base K i , we write K i i to denote the knowledge base that results from
substituting each occurrence of an atom a ∈ A by the corresponding atom a@si . Thus we tacitly admit that (¬a)@si = ¬a@si ,
(a ∧ b)@si = a@si ∧ b@si and (a ∨ b)@si = a@si ∨ b@si .
Example 3. Consider again the weather example (1)–(3). Consistency can be trivially restored by assuming that the three
sources use a slightly different terminology:
@s1

= partiallyCloudy@s1 ∨ overcast@s1

@s2

= openSky@s2

@s3

= overcast@s3

K1
K2
K3

@s

@s

@s3

As each of the knowledge bases K 1 1 , K 2 2 , K 3
@s1

that K 1

@s2

∪ K2

@s3

∪ K3

and C contain occurrences from disjoint sets of atoms, we clearly have

∪ C is consistent.

To obtain more interesting conclusions than from the trivial solution in Example 3, we additionally encode how atoms
of the form a@si relate to the atoms from A. This idea of using a disjoint vocabulary for each of the sources, and subsequently imposing constraints that allow useful results without introducing inconsistency can also be found in [23]. This
latter approach, in our notation, boils down to adding a maximal set of equivalences of the form a@si ≡ a@s j which does
not introduce inconsistency (a procedure called belief set merging), or a maximal set of equivalences of the form a@si ≡ a
which does not introduce inconsistency (a procedure called belief set projection). The underlying idea is that the knowledge
expressed by particular sources about particular variables is ignored. In this sense, the approach from [23] is also similar to
the procedure proposed in [24], which is more explicitly based on this idea of ignoring variables.
In this paper, we propose a different solution, which may be understood as a reﬁnement of the belief set projection
approach from [23]. Our purpose in adding constraints, however, is not to ignore certain variables, but rather to encode in
which way they may be understood. Although we may not know with certainty how atom a is understood by source si , in
many contexts it seems reasonable to assume that information is available about which understandings are possible. In the
case of Example 3, the understanding of overcast by source s1 may be one of the following:
1. The intended meaning of overcast is adopted by source s1 , i.e. we have overcast@s1 ≡ overcast, or in other words
overcast@s1 → overcast and overcast → overcast@s1 .
2. Source s1 takes a more liberal interpretation of overcast which includes some of the cases that are normally described
as partiallyCloudy, i.e. overcast@s1 → overcast ∨ partiallyCloudy and overcast → overcast@s1 .
3. Source s1 takes a more restrictive interpretation of overcast which includes only some of the cases that are normally
described as overcast, the remaining cases being described as partiallyCloudy. We then have overcast@s1 → overcast and
overcast → overcast@s1 ∨ partiallyCloudy@s1 .

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1821

Assume furthermore that the ﬁrst situation is considered more plausible than the second situation, which is in turn considered to be more plausible than the last. Using possibilistic logic [25], for instance, we may describe our background
assumptions on the possible state of affairs as follows:

M=

oc@s1 ≡ oc ∨
oc

@s1

oc@s1 → oc ∨ pc ∧ oc → oc@s1

≡ oc ∨ oc

@s1

→ oc ∨ pc ∧ oc → oc

@s1

∨

oc@s1 → oc ∧ oc → oc @s1 ∨ pc @s1

, λ1 , oc

@s1

,1 ,

≡ oc, λ0

where we have abbreviated overcast and partiallyCloudy as oc and pc respectively, and 0 < λ0 < λ1 < 1. The possibilistic
knowledge base M encodes that we are certain that at least one of the three situations described above is correct. With
less certainty, we believe that at least one of the ﬁrst two situations is correct, and with even less certainty that the ﬁrst
situation is correct. It can be veriﬁed2 that the possibilistic knowledge base M may be equivalently expressed as

M=

oc@s1 → oc ∨ pc , 1 , oc → oc@s1 ∨ pc @s1 , 1 , oc → oc@s1 , λ1 , oc@s1 → oc, λ0

(11)

Note how the knowledge base M encodes more or less plausible ‘mistakes’ that may explain conﬂicts between different
sources. This idea of merging propositional knowledge bases by trying to identify the underlying mistakes that have been
made by the sources has been advocated in [26].
3.2. Syntactic encoding
Note that implications are used as the basic building blocks instead of equivalences in (11). This is due to the fact that
the exact meaning of e.g. oc@s1 may not be expressible using the available terminology (i.e. the atoms in A). All we can
express then, are necessary and suﬃcient conditions under which oc @s1 and ¬oc@s1 hold. This gives rise to four types of
implications, which are used to encode the relationship between the atoms in A @si and those in A:

a@s i →
a→

¬a@si →
¬a →

w w ∈ W al

(12)

x@si x ∈ X al

(13)

¬ y y ∈ Y al

(14)

¬ z@si z ∈ Z al

(15)

where l ∈ {0, . . . , k} and W al , X al , Y al , Z al ⊆ A are sets of atoms such that

{a} = W a0 ⊆ W a1 ⊆ · · · ⊆ W ak ,

{a} = X a0 ⊆ X a1 ⊆ · · · ⊆ X ak

{a} = Y a0 ⊆ Y a1 ⊆ · · · ⊆ Y ak ,

{a} = Z a0 ⊆ Z a1 ⊆ · · · ⊆ Z ak

Note that an arbitrary number of disjuncts may appear in the right-hand side of the implications. The disjuncts that appear
in the right-hand sides of the implications are denoted as W al , X al , Y al and Z al , where l is a tolerance parameter, i.e. rather
than considering one implication of each type, we consider a sequence of implications which allow for an increasingly more
liberal view. For l = 0, the implications (12)–(15) simply assert that a@si ≡ a. For larger values of l, these implications correspond to increasingly weaker constraints on the exact logical relationship between a and a@si . The underlying idea is that
the larger the value of l, the more certain we are that the logical relations expressed by (12)–(15) are valid. By considering
larger values of l, we effectively stretch the meaning of what is asserted by the source, until it becomes consistent with
what is asserted by other sources. As the meaning of propositions cannot be stretched indeﬁnitely, some ﬁxed upper bound
k is assumed. Note that in the approach presented in [23], we have either a@si ≡ a, or a@si ≡ when the literal a should be
ignored for the sake of consistency.
Note that (12) and (14) respectively correspond to necessary and suﬃcient conditions for having a@si true, expressed in
the standard usage of the vocabulary. Similarly, (13) and (15) respectively correspond to necessary and suﬃcient conditions
for having a true, assuming the vocabulary usage of source si . In practice, depending on the characteristics of the domain,
several of the implications (12)–(15) may be trivial. To allow us to trivialize the implications in a convenient way, we treat
and ⊥ as special atoms from A (rather than primitives in the language). We will tacitly assume that C contains at least
the formula ∧ ¬⊥, enforcing that
is contained in any model of the integrity constraints, and ⊥ is contained in no such
model, and furthermore that @si = and ⊥@si = ⊥ for every source si .
Example 4. In the case of (11), implications of the form (14) or (15) were not considered for l > 0. This is due to the fact
that the vocabulary does not allow us to express suﬃcient conditions for a or a@si , using respectively the vocabulary of
source si and the standard vocabulary. This leads to:
2

By repeated application of the possibilistic resolution rule (¬ p ∨ q, λ), ( p ∨ r , μ)

(q ∨ r , min(λ, μ)).

1822

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

0
W oc
= {oc},

0
X oc
= {oc},

0
Y oc
= {oc},

0
Z oc
= {oc}

1
W oc
= {oc, pc },

1
X oc
= {oc},

1
Y oc
= {oc, ⊥},

1
Z oc
= {oc, ⊥}

2
W oc
= {oc, pc },

2
X oc
= {oc, pc },

2
Y oc
= {oc, ⊥},

2
Z oc
= {oc, ⊥}

l
l
Note how the occurrence of ⊥ in Y oc
and Z oc
for l
note that in this example, the upper bound k is 2.

1 effectively trivializes the corresponding implications. Furthermore,

The following example illustrates a situation where all four types of implications play a non-trivial role.
Example 5. The concept of a public school has a meaning in the UK which is different from the one in the USA. In particular,
what is called a public school in the UK is called a private school in the USA, and what is called a public school in the USA
is called a state school in the UK. Now assume that sources are supposed to use the UK terminology, but that occasionally,
some source makes the mistake of using the term public school in its USA meaning. The possible understandings of the
atom pu (public school) in terms of the remaining atoms pr (private school) and st (state school) can then be encoded in
possibilistic logic as follows:

M=

pu @si → pu ∨ st , 1 , pu → pu @si ∨ pr @si , 1 , ¬ pu @si → ¬ pu ∨ ¬st , 1 , ¬ pu → ¬ pu @si ∨ ¬ pr @si , 1 ,
pu @si → pu , λ0 , pu → pu @si , λ0

where 0 < λ0 < 1. In this case, we get:
0
W pu
= { pu },

X 0pu = { pu },

0
Y pu
= { pu },

Z 0pu = { pu }

1
W pu
= { pu , st },

X 1pu = { pu , pr },

1
Y pu
= { pu , st },

Z 1pu = { pu , pr }

Throughout the paper, certainty degrees which are attached to formulas will be interpreted in a variety of different ways
(including necessities, priorities, and penalties). In each case, the knowledge base will syntactically be expressed in the same
way, however. In particular, given the sets W al , X al , Y al and Z al , for each source si , we will consider the following weighted
knowledge base:

a@s i →

w w ∈ W al , λ(Wl,si ,a)

a ∈ A , l ∈ { 0, . . . , k }

∪

a→

x@si x ∈ X al , λ(Xl,si ,a)

a ∈ A , l ∈ { 0, . . . , k }

∪

¬a@si →

∪

¬a →

M si =

¬ y y ∈ Y al , λ(Yl,si ,a)
¬x@si z ∈ Z al , λ(Zl,si ,a)

a ∈ A , l ∈ { 0, . . . , k }
a ∈ A , l ∈ { 0, . . . , k }

(16)

where the values λ×
(l,si ,a) (with × ∈ { W , X , Y , Z }) are certainty values (or priorities) taken from a totally or partially ordered
set (Λ, ) such that λ×
λ×
l2 , i.e. the more the constraints on the meaning of a certain atom a are
(l1 ,si ,a)
(l2 ,si ,a) when l1
relaxed, the more certain that they are correct. Note that the full generality of (16) is not always needed, and sometimes
even a single sequence of certainty values λl would be enough. Among others this depends on how the certainty weights
are interpreted (symbolic weights, penalties, priorities, etc.). In Section 5 we will discuss how different interpretations of
these certainty values lead to merging operators with a different behavior.
In the following, we also consider the following alternative to the set M si :

a@s i →

M si =

∧ ¬a →

w w ∈ W al

∧ a→

¬x@si z ∈ Z al

, λ(l,si ,a)

x@si x ∈ X al

∧ ¬a@si →

a ∈ A , l ∈ { 0, . . . , k }

¬ y y ∈ Y al
(17)

l2 implies λ(l1 ,si ,a) λ(l2 ,si ,a) . Note that in possibilistic logic, when λ(Wl,s ,a) =
i
=
=
= λ(l,si ,a) , we have that M si is equivalent to M si , while this is not the case in, for instance, penalty
logic [27,28] due to the additive interpretation of the weights.
Clearly, the combined use of the four types of implications (12)–(15) results in a powerful mechanism which encompasses a wide variety of inconsistency scenarios. To bridge the gap between this general, but abstract mechanism and
practical applications, we discuss in Section 4 four prototypical scenarios in which our framework could be applied. We
then also introduce a graph representation, which allows to describe in a compact and intuitive way, how an atom may be
understood ﬂexibly, at a given level of tolerance.
with the corresponding assumption that l1

λ(Xl,si ,a)

λ(Yl,si ,a)

λ(Zl,si ,a)

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1823

3.3. Semantic encoding
At the semantic level, a ﬂexible understanding of statements can be obtained by using suitable operators that manipulate
sets of atoms (i.e. interpretations). Let R be a reﬂexive relation in A, and let I ⊆ A be a set of atoms. Then we deﬁne the
expansion I R and contraction [ I ] R of I w.r.t. R as the following sets:

I

R

= a ∈ A ∃b ∈ A . (a, b) ∈ R ∧ b ∈ I

[ I ] R = co coI

R

(18)

= a ∈ A ∀b ∈ A . (a, b) ∈ R ⇒ b ∈ I

(19)

where co X is understood as the set complement of X w.r.t. A, i.e. co X = A \ X . Since R was assumed to be reﬂexive, we
clearly have [ I ] R ⊆ I ⊆ I R . Also note that when R is an equivalence relation, expansion and contraction correspond to the
notion of upper and lower approximation from rough set theory [29]. In that case, I R is the union of the equivalence
classes that overlap with I , whereas [ I ] R is the union of the equivalence classes that are included in I .
The intuition behind (18)–(19) is best seen when we interpret R as modeling some form of similarity. Then I R contains those atoms that are similar to at least one atom from I . Hence J ⊆ I R can be interpreted as asserting that J is
approximately included in I , in the sense that every atom in J is similar to an atom in I . The set [ I ] R , on the other hand,
contains those atoms that are only similar to atoms from I . This means that [ I ] R ⊆ J can be interpreted as asserting that
co J is approximately included in coI , in the sense that every atom outside J is similar to an atom outside I . Let us now
consider the following relation:

σ( R 1 , R 2 ) ( I , J ) iff [ I ] R 1 ⊆ J ⊆ I

(20)

R2

Then σ( R 1 , R 2 ) ( I , J ) expresses some form of similarity between interpretations I and J . Indeed, since J ⊆ I R 2 we know that
every atom interpreted as true by J is similar (w.r.t. R 2 ) to an atom interpreted as true by I , and since [ I ] R 1 ⊆ J we know
that every atom interpreted as false by J is similar (w.r.t. R 1 ) to an atom interpreted as false by I .
In the particular case where R 1 and R 2 are both equal to the identity relation (i.e. (a, b) ∈ R 1 iff (a, b) ∈ R 2 iff a = b),
then also σ( R 1 , R 2 ) degenerates to the identity relation (i.e. σ( R 1 , R 2 ) ( I , J ) iff I = J ). In general, when R 1 and R 2 capture some
form of similarity between atoms, σ( R 1 , R 2 ) captures a notion of similarity between interpretations. Note however that in
general σ( R 1 , R 2 ) is not symmetric, not even when R 1 and R 2 are symmetric and/or R 1 = R 2 .
Example 6. Let pu and pr correspond to public and private schools, as before, and let un refer to a university. Then we
may consider that public and private schools are similar to each other, but neither is similar to a university, i.e. we let
R = {( pu , pu ), ( pr , pr ), (un, un), ( pu , pr ), ( pr , pu )}, I = { pu , un} and J = {un}. Then we have

[ I ] R = {un},

I

R

= { pu , pr , un}

[ J ] R = {un},

J

R

= {un}

In particular we ﬁnd that [ I ] R ⊆ J ⊆ I

R,

but not [ J ] R ⊆ I ⊆ J

R.

Thus

σ ( R , R )( I , J ) holds but not σ ( R , R )( J , I ).

Thus the relatedness between interpretations I and J may be parametrized by four reﬂexive relations R 1 , R 2 , R 3 , R 4
between atoms, by considering that I is similar to J when σ( R 1 , R 2 ) ( I , J ) and σ( R 3 , R 4 ) ( J , I ) both hold. From a practical point
of view, however, it is not immediately clear why we need four different relations, and how they should be deﬁned to
model a particular scenario. This situation is reminiscent of the four types of implications (12)–(15) we have considered at
the syntactic level. The intuitive correspondence between the aforementioned relations and the families W al , X al , Y al and Z al
can be made explicit by interpreting the latter families as relations W l , X l , Y l and Z l , where W l = {(a, b) ∈ A 2 | b ∈ W al },
and analogously for the other relations. The exact correspondence is revealed by the following proposition.
Proposition 1. Let W al , X al , Y al , and Z al for a ∈ A and l ∈ {0, . . . , k} be deﬁned as before. Then we have the following characterizations:

⇔

I ∪ J @s i |

a@s i →

a∈ J \ I

Wl

a∈ I\ J

Xl

⇔

I ∪ J @s i |

a→

a ∈ [ I ]Y l \ J

⇔

I ∪ J @s i |

¬a@si →

a ∈ [ J ]Zl \ I

⇔

I ∪ J @s i |

¬a →

w w ∈ W al
x@si x ∈ X al

¬ y y ∈ Y al
¬ z@si z ∈ Z al

(21)
(22)
(23)
(24)

Intuitively, e.g. (21) teaches us that I ∪ J @si | (a@si → { w | w ∈ W al }) means that whenever a ∈ J for an atom a, we
also have a ∈ I W l . In other words, the fact that I ∪ J @si | (a@si → { w | w ∈ W al }) for all a ∈ A is equivalent to stating
that J ⊆ I W l . Thus, Proposition 1 essentially reveals that the relations R 1 , R 2 , R 3 and R 4 correspond to Y l , W l , Z l and X l

1824

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

Fig. 1. Four prototypical types of weakening the meaning of an atom.

respectively. This observation also makes clear why we need, in general, the ﬂexibility of parametrizing similarity between
interpretations by four different relations between atoms. In practice, on the other hand, these four relations do not have to
be speciﬁed explicitly most of the time, as we will explain in the following section.
4. Prototypical use cases
The use of four different families of nested sets, W al , X al , Y al and Z al , results in an expressive method to encode how
knowledge bases may be weakened. This generality is needed, as the fact that an atom may be understood in a ﬂexible way
may mean different things in different contexts. For illustrative purposes, let us write ext(a) to denote the set of situations
in which property a holds (e.g. we may think of ext(a) as some region of a conceptual space in the sense of Gärdenfors
[30]). There seem to be four prototypical scenarios that are often encountered in practice:
liberalization In certain contexts, and depending on the view one takes, we may be more liberal regarding the exact
meaning of a certain property. As a result, for borderline situations, a given property may be considered to be
satisﬁed according to some sources, but not satisﬁed according to others. In this scenario, ﬂexibility means that
we need to admit that the sources may have a more liberal understanding of the meaning of a property: ext(a) ⊆
@s i
). This situation is illustrated in Fig. 1(a).
i ext (a
restriction The opposite situation also occurs: sources may hold a stricter view on the meaning of certain properties, for
instance restricting the situations in which a property is considered to hold to the most typical situations. Being
ﬂexible then means that we may need to exclude certain borderline cases of the property: ext(a) ⊇ i ext(a@si ).
This situation is illustrated in Fig. 1(b); note, however, that the understandings by different sources do not necessarily need to overlap.
continuity The meaning of a property may depend on some threshold value in a continuous domain, which is to some
extent arbitrary. Statements provided by different sources may then be based on slightly different threshold values.
This means that there are a number of situations in which the property would hold according to all sources, in
addition to situations in which it is the particular deﬁnition adopted by a source that determines whether or not
the property is considered to hold: ext(a) ∩ i ext(a@si ) = ∅. Furthermore, due to the continuity there is potentially
an inﬁnite number of reasonable delineations of those situations in which the property is considered to hold. Due
to the ﬁnite number of atoms that are considered in the language,3 the exact meaning of a property, according to
one source, is typically not expressible. Fig. 1(c) depicts this situation.
ambiguity The meaning of a property may be ambiguous. This means that some sources may interpret an atom in a
completely different way than other sources. In such a case, it may happen that there is not a single situation in
which the property is considered to hold according to two different sources. Intuitively, the meaning of an atom

3

Note that even moving from a ﬁnite number of atoms to a countably inﬁnite number of atoms would not change this situation.

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1825

Fig. 2. Similarity graph for atoms related to marriage.

Table 1
Deﬁning the sets W al , X al , Y al and Z al in terms of the sets of related atoms S la .
W al

X al

Y al

Z al

Liberalization

S la

{a}

{a}
{a, }

{a}

Restriction
Continuity

S la

S la

{a, ⊥}

Ambiguity

S la

{b | a ∈ S lb }

S la

{a, ⊥}
{a}
{a, ⊥}
{b | a ∈ S lb }

S la

is then translated, rather than stretched: ext(a) = ext(a@si ) or ext(a) ∩ ext(a@si ) = ∅. This situation is depicted in
Fig. 1(d).
Note that from a formal point of view, in continuous domains, liberalization and restriction may be seen as special cases of
continuity. However, we reserve the term continuity for scenarios in which it is not known a priori whether sources will
assume a more liberal, more restrictive, or overlapping meaning. Moreover, when considering liberalization and restriction
below, we will focus on discrete domains, and in particular assume that the exact meaning of a@si can be expressed as a
disjunction, resp. conjunction of atoms from A.
In each of these four scenarios, it suﬃces to specify, for each given atom a and tolerance level l, a single set of atoms S la .
This set S la may be understood as the set of atoms that are similar, or related to a at the given tolerance level, although the
precise interpretation of this set will differ in each of the scenarios. In practical applications, the sets S la may be conveniently
and compactly described using a weighted, directed graph. As such graphs encode some notion of similarity, we will refer to
them as similarity graphs. Figs. 2, 3 and 4 depict examples of such graphs. The underlying intuition is that whenever there
is a path from node a to a node b, if some source claims that a holds, it is somewhat plausible that b is the case instead.
The level of plausibility depends on the weights on the edges, which in turn correspond to the tolerance levels. Hence, like
the tolerance levels, the weights may be given a number of different interpretations (qualitative as well as quantitative), as
will become clear below. In many situations, these weights will be given an ordinal interpretation, in which case it suﬃces
to rank the edges according to how likely the corresponding transition is.
Given a similarity graph, S la can be deﬁned as

S al = b b ∈ A , dist(a, b)

l

(25)

where dist(a, b) is the sum of the weights on the shortest path between a and b in the similarity graph under consideration. Intuitively, the graph from Fig. 2 speciﬁes, among others, that married can progressively be weakened to
married ∨ civilUnioned and then married ∨ civilUnioned ∨ widowed ∨ divorced ∨ cohabitating. When the atoms in a similarity graph express jointly exhaustive and pairwise disjoint (JEPD) properties, i.e. when in every possible world exactly one of
these atoms is true, symmetric and uniformly weighted similarity graphs essentially correspond to conceptual neighborhood
graphs in the sense of Freksa [31,32].
It is important to note that similarity graphs are nothing more than a convenient vehicle to specify the sets S la in some
applications. Sometimes, it may not be appropriate to use such a graph, and it makes more sense to specify the sets S la
l

directly. For instance, the use of a similarity graph to specify the sets S la implies some form of transitivity, i.e. b ∈ S a1 and
c∈

l
S b2

implies that c ∈

l +l
S a1 2 .

Depending on the scenario, the exact nature of the sets S la is different, resulting in different deﬁnitions of the sets W al ,
X al , Y al and Z al . Below we discuss in more detail how each given scenario dictates the deﬁnition of W al , X al , Y al and Z al . The
results that are going to be established are summarized in Table 1.

1826

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

4.1. Liberalization
The atoms that are connected to married in the graph from Fig. 2 represent situations that can be considered as special
cases of being married, when assuming a liberal usage of this term. This means that non-standard understandings of married
may only enlarge its intended meaning by designating extra atoms to be special cases of married. Such a reading of a
similarity graph may be formalized as follows:

a@s i ≡

S

(26)

{a}⊆ S ⊆ S la

which directly translates the intuition that the understanding of a by source si can be expressed as the disjunction of a
particular set S of atoms, i.e. a@si ≡
S. We do not know which atoms are in S, but we assume that all atoms in S are
similar to a (at a given tolerance level l). This reading can be equivalently expressed using the following implications:

a@s i →

S al ,

a → a @s i

(27)

Lemma 1. The formula (26) is satisﬁed for all a ∈ A iff the implications in (27) are satisﬁed for all a ∈ A.
We may thus deﬁne:

W al = S al ,

X al = Y al = {a},

Z al = {a, ⊥}

where we used the fact that a → a@si is equivalent to ¬a@si → ¬a. Alternatively, we may deﬁne X al = {a, } and Y al = {a},
or X al = {a} and Y al = {a, ⊥}. Note in particular that negative literals of the form ¬a@si are always understood as ¬a. This
corresponds to the observation that married in its standard understanding is truly atomic, i.e. there are no situations that
are normally considered as married but may not be considered as such by certain sources. Conversely, when ¬a holds, only
trivial conclusions can be expressed using the atoms in A @si , unless W al = {a}, in which case a@si ≡ a.
Example 7. When a source claims that somebody is married, we may consider the possibility that he or she is actually in a
civil union, whereas the standard understanding of marriage only permits the strict meaning (for l = 1):
1
W married
= {married, civilUnion}
1
X married
= {married}

When ¬married is known to hold, no conclusions can be expressed using the atoms in A @si because source si may use the
term married in a more liberal sense, such that married@si holds. On the other hand, when ¬married@si holds, we know that
also ¬married holds. This may be expressed as:
1
Y married
= {married}
1
Z married
= {married, ⊥}

4.2. Restriction
The idea of restriction is dual to the idea of liberalization. Rather than admitting more borderline cases, here we need to
consider the possibility of excluding borderline cases:

a@s i ≡

S

(28)

{a}⊆ S ⊆ S la

which expresses the intuition that some source may only consider a to hold when some additional properties are satisﬁed.
Although we do not know which set S of properties needs to be satisﬁed for a@si to hold, but we make the assumption
that all these properties are similar to a. Although the sets S la may, in principle, again be speciﬁed as a graph, such a
graph would look less natural. Indeed, rather than specifying similar atoms, the set S la here contains properties describing
typicality. Again, implications of the form (12)–(15) may be used to describe the state of affairs:

¬a@si →

¬s s ∈ S al ,

¬a → ¬a@si

Lemma 2. The formula (28) is satisﬁed for all a ∈ A iff the implications in (29) are satisﬁed for all a ∈ A.

(29)

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1827

Fig. 3. Similarity graph for atoms related to the weather.

We may thus deﬁne:

W al = {a},

X al = {a,

Y al = S al ,

},

Z al = {a}

where we used the fact that ¬a → ¬a@si is equivalent to a@si → a. Alternatively, we may deﬁne W al = {a, } and Z al = {a},
or W al = {a} and Z al = {a, ⊥}. Note in particular that positive literals of the form a@si are always understood as a.
Example 8. The term polyhedron in mathematics is sometimes deﬁned as the ﬁnite union of convex polyhedra, where a
convex polyhedron is the intersection of a ﬁnite number of half-spaces in some particular dimension. Different deﬁnitions
are used, however, where the term polyhedron is sometimes used only for convex polyhedra, for bounded polyhedra, or for
three-dimensional polyhedra. Thus we have, for instance:
1
S polyhedron
= {polyhedron, convex, bounded, threeDimensional}

When a source claims polyhedron@si we may not know which particular deﬁnition was assumed, but in each case we may
conclude polyhedron, as all possible deﬁnitions are more speciﬁc. If the source claims ¬polyhedron@si , however, this may
not be suﬃcient to conclude ¬polyhedron as it may, for instance, be the case that the source is describing an unbounded
polyhedron. Hence, all that may be concluded from ¬polyhedron@si is ¬polyhedron ∨¬convex ∨¬bounded ∨¬threeDimensional.
Of course, it may be the case that both more liberal and more restrictive deﬁnitions of some property exist, depending
on the considered source. As such hybrid situations can be treated entirely analogously as the scenarios of liberalization and
restriction, we omit the details.
4.3. Continuity
The notion of similarity from Fig. 3 relates to the fact that the atoms involved are the result of a partially arbitrary
discretization of a continuous domain. There is no well-deﬁned, crisp boundary between situations that should be described
as overcast and situations that should be described as partiallyCloudy. As a result, what is called overcast by one source may
be called partiallyCloudy by another source. Due to their particular nature, such similarity graphs are symmetric. Formally
we may describe the underlying intuition as:

a@s i →

W al ,

a→

W al

@s i

While we may not be able to exactly express the meaning of a@si using the atoms in A, we know at least that when a source
asserts a@si , an atom similar to a will be the case. Conversely, when a holds, we may assume that b@si holds for some atom
b similar to a. Note that the information described in a similarity graph, under this reading, is completely captured by
implications of the form (12)–(13), hence we may take the implications (14)–(15) to be trivial:

W al = X al = S al ,

Y al = Z al = {a, ⊥}

In particular, this means that unless a → a@si or a@si → a is assumed, no conclusions can be drawn from ¬a or ¬a@si , using
respectively atoms from A @si and atoms from A.
Example 9. The use of the atom partiallyCloudy may arise in conﬂict because the exact boundary between overcast situations
and partiallyCloudy situations may be slightly different according to different sources, as does the exact boundary between
openSky situations and partiallyCloudy situations. This situation is symmetric in that statements we believe to hold may
assume a boundary which is more liberal or more restrictive than the boundary assumed by a given source, i.e.:
1
1
W partiallyCloudy
= X partiallyCloudy
= {openSky, overcast, partiallyCloudy}

Moreover, from ¬partiallyCloudy@si no conclusions can be formulated in terms of the atoms in A. The reason is that when
¬partiallyCloudy@si is claimed by a source, it may assume a more restrictive deﬁnition of the atom partiallyCloudy, and it
may still be the case that partiallyCloudy holds in our standard understanding of this term. Thus we obtain:
1
1
Y partiallyCloudy
= Z partiallyCloudy
= {partiallyCloudy, ⊥}

1828

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

Fig. 4. Similarity graph for atoms related to school types.

4.4. Ambiguity
Fig. 4 illustrates a third possible reading of similarity graphs. Edges in this graph correspond to atom-pairs that are
sometimes confused. Whereas the readings discussed in Sections 4.1 and 4.3 relate to the issue of vagueness, the similarity
graph from Fig. 4 is centered around the ambiguity of the term publicSchool. While there is no straightforward possibility
to be ﬂexible about the meaning of a public school in either the UK or the USA meaning, confusion is caused by the fact
that we do not know with absolute certainty that all sources conform to the requirement to adopt the UK interpretation. A
similarity graph should then be understood as follows:

a@s i ≡ b ,
b∈ W al

a ≡ b @s i

(30)

a∈ W bl

The formula on the left expresses that when a source asserts a@si , this either means a or some atom with which a may
have been confused. Conversely, the formula on the right expresses that the standard understanding of a corresponds to
a@si or to an atom b@si such that sources may confuse b with a. It is not hard to see that (30) is equivalent to the following
implications:

a@s i →

¬a@si →

b b ∈ W al ,

¬b b ∈ W al ,

a→

b@si a ∈ W bl

¬a →

¬b@si a ∈ W bl

We obtain:

X al = Z al = b a ∈ S lb ,

Y al = W al = S al

Example 10. While we can be conﬁdent that statements we believe to be true are based on a usage of the term publicSchool
in its UK meaning, we need to consider the possibility that some source has assumed the USA meaning. Moreover, this
confusion remains regardless of whether the atom publicSchool is used in a positive or in a negative literal:
1
1
W publicSchool
= Y publicSchool
= {publicSchool, stateSchool}
1
1
X publicSchool
= Z publicSchool
= {publicSchool, privateSchool}

5. Merging operators and interpretation of the weights
@s

@s

The methodology that was outlined above leaves us with a list of propositional knowledge bases K 1 1 , . . . , K n n , a set
of integrity constraints C , and a list of weighted knowledge bases M s1 , . . . , M sn , deﬁned by (16), that express ﬂexible constraints on how atoms may be understood by the different sources. All these knowledge bases are built from different
@s
sets of atoms: A @si in the case of K i i , A in the case of C , and A @si ∪ A in the case of M si . Given this point of departure, we are now interested in ﬁnding a single, consistent knowledge base K which encodes the combined beliefs of
sources s1 , . . . , sn to the best extent possible, such that all models of K satisfy the integrity constraints, i.e. ❏ K ❑ ⊆ ❏C ❑. We
write K = ( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) to denote the result of applying a speciﬁc merging operator . When the sets
M s1 , . . . , M sn and C are clear from the context, we will also write this as ( K 1 , . . . , K n ). The exact behavior of the operator
primarily depends on the following two factors:
1. The weights λ(l,si ,a) in the weighted knowledge bases M si may be interpreted as necessities, priorities, or as penalties,
among others. They may be totally ordered or partially ordered, and may either depend on all of l, si and a, only on l
and a, only on l and si , or only on l. In each case, the nature of the knowledge bases M si changes, and so should the
result K .
2. There is a trade-off between having a more informative result and having a more cautious result. By appropriately
tuning this trade-off, the result may be conﬁgured to match the needs of a particular application. Another solution is

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1829

to combine both informative and cautious results, by representing the result as a weighted knowledge base, giving the
most cautious results the highest certainty or priority.
In this section, we analyze how exactly these two factors may inﬂuence the result K . First, we illustrate some of the
possibilities in the following example.
Example 11. Consider again the weather example, but assume that the language contains 5 atoms: os, pc1 , pc2 , pc3 and
oc, where pc1 intuitively corresponds to just a sky which is mostly open with the exception of a few clouds, pc2 to about
half-open and half-cloudy, and pc3 to a sky which is mostly cloudy. Now consider the following three knowledge bases:

K 1 = {os},

K 2 = {os},

K 3 = {oc}

together with integrity constraints expressing that os, pc1 , pc2 , pc3 and oc are JEPD properties. Then one may wonder what
would intuitively be the most desirable result, assuming as before that all sources are well-reputed. We may consider that
since the sources make claims which are completely opposite, we cannot draw any reliable conclusions and should therefore
deﬁne the result as

{os ∨ pc1 ∨ pc2 ∨ pc3 ∨ oc}
A different point of view is that the majority of the sources agrees on os and this should therefore be the result:

{os}
although we may also prefer the following, more cautious alternatives:

{os ∨ pc1 },

{os ∨ pc1 ∨ pc2 }

Another point of view is to consider that none of the sources would be completely wrong, and that the result should
therefore be in (or close to) the middle of os and oc, e.g. one of the following alternatives:

{pc2 },

{pc1 ∨ pc2 ∨ pc3 }

Finally, because os is asserted by two out of three sources, we may also consider that the result should be between os and
oc, but closer to os, e.g.:

{pc1 },

{os ∨ pc1 ∨ pc2 },

{os ∨ pc1 ∨ pc2 ∨ pc3 }

The type of ﬂexibility illustrated by the previous example cannot be achieved using standard approaches, which are based
on the Hamming distance. Indeed, the only conclusions that can reasonably be motivated in terms of the Hamming distance
are {os ∨ pc1 ∨ pc2 ∨ pc3 ∨ oc}, {os}, and {os ∨ oc}. In this section, we study a number of different merging operators, based
on the sets M si and M si introduced in (16) and (17). Each of these merging operators naturally results from interpreting
the weights in M si or M si in a particular way. Moreover, in each case, we characterize the behavior of the merging operator
at the semantic level, establishing close links with the existing frameworks for merging propositional knowledge bases that
were recalled in Section 2. In some cases, our merging operators are a special case of distance-based or morphological
merging operators, albeit w.r.t. a completely novel type of distance which is based on the idea of interpreting atoms in a
ﬂexible way. In other cases, merging operators can be described in natural extensions of distance-based or conﬂict-based
merging operators. It is important to note here, however, that the aim of our paper is not to introduce a new family
of merging operators per se, but rather to advocate a different way of measuring the similarity (or distance) between
interpretations.
In the discussion that follows, we speciﬁcally consider the task of merging the beliefs held by different sources, rather
than merging incompatible goals or preferences [33,34]. As such, the intuition behind each merging operator will be to
ﬁnd the interpretations that are most likely to be models of the real world, given the assertions of the different sources
(rather than looking for a trade-off between conﬂicting objectives). By interpreting this notion of likelihood in different ways
(ranging from purely qualitative to purely quantitative), and by assuming different levels of caution, it will become clear
that a variety of behaviors can be obtained in a natural way, including those that are illustrated in Example 11.
5.1. Basic structure of the merging operators
Before going into the details of particular merging operators, we discuss the underlying idea which is common to each
of the approaches that are introduced below. For the ease of presentation, we deﬁne the weighted knowledge bases P and
P as follows:

1830

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

P=

@si

M si ∪
i

(α , 1) α ∈ K i

∪ (c , 1) c ∈ C

(31)

∪ (c , 1) c ∈ C

(32)

i

P =

@s i

M si ∪
i

(α , 1) α ∈ K i
i

@s

where we reinterpret the propositional knowledge bases K i i and C as weighted knowledge bases in which all weights
are equal to 1, and M si and M si are deﬁned as in (16) and (17). Let us furthermore write P ∗ and P ∗ for the sets of
(unweighted) formulas that appear in P and in P respectively, i.e. P ∗ = {α | ∃λ . (α , λ) ∈ P } and similar for P ∗ . Throughout
this section, we implicitly assume that λ(l,si ,a) < 1 for all l ∈ {0, . . . , k}, i ∈ {1, . . . , n} and a ∈ A. The idea is that the formulas
@s
in K i i and C are unconditionally true, hence they receive a maximal certainty level, while the formulas in M si and M si are
more or less plausible assumptions that may be violated, and thus receive a certainty level that is strictly smaller than 1.
Moreover, unless otherwise stated, we assume that λ(l,si ,a) = λ(Wl,s ,a) = λ(Xl,s ,a) = λ(Yl,s ,a) = λ(Zl,s ,a) = λl for every source si and
i
i
i
i
atom a, which implies among others that the weights are totally ordered, and in particular that we may assume that the
weights λ(l,si ,a) = λl correspond to numbers in [0, 1[. As a notational convenience, let us furthermore introduce the following
abbreviations:

α(Wl,si ,a) = a@si →
α(Xl,si ,a) = a →

w w ∈ W al
x@si x ∈ X al

α(Yl,si ,a) = ¬a@si →
α(Zl,si ,a) = ¬a →

¬ y y ∈ Y al
¬ z@si z ∈ Z al

α(l,si ,a) = α(Wl,si ,a) ∧ α(Xl,si ,a) ∧ α(Yl,si ,a) ∧ α(Zl,si ,a)
i.e.

α(×l,si ,a) is the formula which appears with weight λ×
(l,si ,a) in M si , and α(l,si ,a) is the formula which appears with weight

λ(l,si ,a) in M si .

that we will consider proceeds by selecting particular consistent subsets of P ∗ or P ∗ ,
Each of the merging operators
@s
which contain at least all the formulas with weight 1, i.e. the formulas from C and K i i . Let Pref ( P ) and Pref ( P ) be the
subsets P ∗ and P ∗ that are selected according to some criterion. A ﬁrst idea might then be to deﬁne the corresponding
and
as follows:
merging operators

q

②
(K 1, . . . , Kn ) =
q
②
(K 1, . . . , Kn ) =

Pref ( P )
Pref P

where the sets of formulas in Pref ( P ) and Pref ( P ) are treated as conjunctions of formulas.
One possible drawback of this method is that the results ( K 1 , . . . , K n ) and
( K 1 , . . . , K n ) still refer to atoms from A @si .
Recall that these atoms were introduced mainly for technical reasons, and their exact meaning is not known. Thus it seems
reasonable to require that no occurrences of these atoms remain after merging (although they may provide insight to the
user regarding the source of conﬂicts). To this end, we employ the notion of variable forgetting [35,24], deﬁned for a set of
propositional formulas Φ , an atom x and a set of atoms X as:

forgetVar(Φ, ∅) = ∅
forgetVar Φ, {x} = Φ[x :=

] ∨ Φ[x := ⊥]

forgetVar Φ, {x} ∪ X = forgetVar forgetVar(Φ, x), X
where Φ[x := φ] for an atom x and a formula φ denotes the set of formulas that is obtained from Φ by replacing every
occurrence of x by φ ; sets of formulas are interpreted as conjunctions. The propositional knowledge base forgetVar(Φ, X ) can
be seen as a projection of Φ which does not contain any occurrence of atoms from X and moreover, if ψ does not contain
occurrences of atoms from X , we have Φ | ψ iff forgetVar(Φ, X ) | φ . Thus, to merge the knowledge bases K 1 , . . . , K n we
may take ( K 1 , . . . , K n ) or
( K 1 , . . . , K n ) and forget the variables in A @s1 ∪ · · · ∪ A @sn :
f (K 1, . . . , Kn )

= forgetVar

( K 1 , . . . , K n ), A @s1 ∪ · · · ∪ A @sn

f (K 1, . . . , Kn )

= forgetVar

( K 1 , . . . , K n ), A @s1 ∪ · · · ∪ A @sn

While variable forgetting is computationally expensive in general, in certain cases eﬃcient syntactic procedures can be
obtained. In Appendix B, an example of such a procedure is provided.
In Sections 5.2–5.6 merging operators are introduced that result from interpreting the certainty weights in different ways.
In Section 5.2, certainty weights are interpreted in a purely ordinal way, as possibilistic certainty weights. In Sections 5.3

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1831

Fig. 5. Similarity graph for atoms related to Toulouse.

and 5.4 this approach is reﬁned by using the concept of preferred subtheories, respectively deﬁned w.r.t. subset-inclusion
and w.r.t. cardinalities. Subsequently, in Section 5.5 a purely qualitative approach is presented, which uses symbolic certainty
weights that are taken from an arbitrary partially ordered set. Finally, Section 5.6 discusses a quantitative approach with
a probabilistic ﬂavor based on penalty logic. Each of these ﬁve sections ﬁrst introduces merging operators at the syntactic
level, and subsequently provides a semantic characterization which clariﬁes their relationship to existing approaches for
information merging. Moreover, the ﬁve sections can be read independently from each other.
5.2. Possibilistic certainty weights
5.2.1. Merging operators
In this section, we interpret the weighted knowledge bases M si as possibilistic knowledge bases. In this case, the weights
reﬂect lower bounds on a necessity measure. Following the standard treatment of inconsistencies in the possibilistic setting,4
we obtain the following merging operators
poss
poss

( K 1 , . . . , K n ) = forgetVar P inc( P ) , A @s1 ∪ · · · ∪ A @sn
( K 1 , . . . , K n ) = forgetVar P inc( P ) , A

@s1

∪ ··· ∪ A

@sn

(33)
(34)

where P inc( P ) is the set of formulas from P whose weight is strictly above the inconsistency level of P . In other
words, we take Pref ( P ) = { P inc( P ) } and Pref ( P ) = { P inc( P ) }. As P inc( P ) is equivalent to P inc( P ) , we immediately ﬁnd that
poss

(K 1, . . . , Kn ) ≡

poss

( K 1 , . . . , K n ), hence we will only consider (33) henceforth.

Example 12. Consider two sources, where one source claims that Peter is married, lives in Toulouse, and works in Montauban (a city about 50 km from Toulouse). A second source also claims that Peter is married, and that he works in
Saint-Alban. However, there are at least four places called Saint-Alban5 : in the regions Ain, Côtes-d’Armor, Haute-Garonne
(in France), and in Québec (in Canada). We assume that the source does not want to commit itself to one of the four places.
We may then encode the two knowledge bases as follows:

K 1 = mar( p ), li( p , t ), wo( p , m)
K 2 = mar( p ), wo( p , saa ) ∨ wo( p , sacda ) ∨ wo( p , sahg ) ∨ wo( p , saq )
where e.g. li( p , t ) means that Peter lives in Toulouse and wo( p , m) means that Peter works in Montauban. The integrity
constraints C specify the JEPD nature of the propositions related to marriage from Fig. 2, the fact that a person can only
work in one place, and can only live in one place (e.g. li( p , t ) → ¬li( p , m)), and that Toulouse and Saint-Alban (HauteGaronne) are both contained in the Urban Community of Greater Toulouse (e.g. li( p , t ) → li( p , gt )). The families W al , X al ,
Y al and Z al are deﬁned according to the similarity graph from Fig. 2 for the atoms a related to marriage, and according to
the similarity graph from Fig. 5 for the remaining atoms. In both cases, the similarity graphs are interpreted in terms of
liberalization, and they are understood in a transitive way.
To obtain the result of merging K 1 and K 2 using poss , we may use Proposition 10:
(1 )

K 1 = mar( p ) ∨ civ( p ), li( p , t ) ∨ li( p , gt ), wo( p , m)
(2 )

K 1 = mar( p ) ∨ civ( p ) ∨ wid( p ) ∨ div( p ) ∨ coh( p ), li( p , t ) ∨ wo( p , t ) ∨ li( p , gt ), wo( p , m) ∨ li( p , m)
4
5

A brief introduction to possibilistic logic is provided in Appendix A.
http://en.wikipedia.org/wiki/Saint-Alban, accessed May 31, 2010.

1832

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

(1 )

K 2 = mar( p ) ∨ civ( p ), wo( p , saa ) ∨ wo( p , sacda ) ∨ wo( p , sahg ) ∨ wo( p , gt ) ∨ wo( p , saq )
(2 )

K 2 = mar( p ) ∨ civ( p ) ∨ wid( p ) ∨ div( p ) ∨ coh( p ), wo( p , saa ) ∨ li( p , saa ) ∨ wo( p , sacda ) ∨ li( p , sacda )

∨ wo( p , sahg ) ∨ li( p , sahg ) ∨ wo( p , gt ) ∨ wo( p , saq ) ∨ li( p , saq )
( j)

where we write K i

Pλ j

as an abbreviation of K i

(1)

(1)

. Clearly K 1 ∪ K 2 ∪ C is inconsistent, since Peter cannot, according to C ,
(2)

(2)

work in two disjoint places. On the other hand, K 1 ∪ K 2 ∪ C is consistent, hence
poss

poss

(2)

(2)

( K 1 , K 2 ) = K 1 ∪ K 2 ∪ C , where

( K 1 , K 2 ) ≡ C ∪ mar( p ) ∨ civ( p ) ∨ wid( p ) ∨ div( p ) ∨ coh( p ), li( p , sahg ) ∧ wo( p , m) ∨ wo( p , t ) ∧ li( p , m)

This solution reveals two possible hypotheses regarding the origin of the conﬂict between K 1 and K 2 . The ﬁrst hypothesis is
that the claim of the ﬁrst source that Peter lives in Toulouse, should be interpreted as Peter living in Greater Toulouse, and
that the second source has confused the fact that Peter lives in Saint-Alban with the statement that Peter works in SaintAlban; this leads to li( p , sahg ) ∧ wo( p , m). The second hypothesis is that the ﬁrst source has swapped the places where Peter
lives and works, i.e. in fact Peter works in Toulouse and lives in Montauban. In addition, the second source has confused
Saint-Alban with Greater Toulouse. The second hypothesis leads to wo( p , t ) ∧ li( p , m).
Note that after merging, in the example, ma( p ) is also understood in a liberal sense, although this is not necessary for
solving the inconsistency. This can be seen as a shortcoming of the possibilistic approach, which is overly cautious here.
Intuitively, this issue can be addressed by ﬁrst determining which atoms participate in the conﬂict, and only weakening
these atoms. Several of the reﬁned merging operators that are proposed below are based on this intuition.
5.2.2. Semantic characterization
Semantically, poss can be seen as a particular case of morpho-logical merging (cf. Section 2.3), using a structuring
element that is deﬁned in terms of the similarity relation σ deﬁned in (20). In particular, we deﬁne the structuring element
B ( W l , X l ,Y l , Z l ) as

B ( W l , X l ,Y l , Z l ) ( I ) = J ∈ 2 A

σ(Y l ,W l ) ( I , J ) ∧ σ( Z l , X l ) ( J , I )

Then we have the following characterization.
Proposition 2. Assume that λ(l,si ,a) = λl for every source si , atom a and l ∈ {0, . . . , k}, and assume λk < 1. Moreover, assume that P λk
is consistent, and let r be the smallest value from {0, . . . , k} such that P λr is consistent, i.e. inc( P ) = λr −1 < λk . It holds that
poss

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) ≡

morph

( K 1 , . . . , K n ; C ; B ( W r , X r ,Y r , Z r ) )

Note however that this linkage can only be established once the structuring element of the morpho-logical approach
is no longer deﬁned by means of the Hamming distance, as usual, but in terms of the similarity relation underlying our
approach.
5.3. Inclusion-based preferred subtheories
5.3.1. Merging operators
As illustrated by Example 12, the standard possibilistic approach leads to results that may be deemed too cautious. In
particular, the conﬂict about where Peter lives and works should, intuitively, not inﬂuence our belief that Peter is married,
a belief which is shared by both sources. This behavior is due to the drowning effect in possibilistic logic, i.e. the fact that
every statement whose certainty is not greater than the inconsistency level is ignored, regardless of whether it participates
in any conﬂict. In Example 12, for instance, mar( p ) has a lower priority than mar( p ) ∨ civ( p ) ∨ wid( p ) ∨ div( p ) ∨ coh( p );
the former expression is below the inconsistency whereas the latter is above, which is why the disjunction is entailed by
the result but mar( p ) is not. A reﬁned merging strategy, in which this drowning effect no longer occurs, can be obtained by
resorting to methods based on maximal consistent subsets. A standard approach was proposed by Brewka [36] within the
context of default reasoning. The idea is to use priorities attached to formulas to designate particular maximal consistent
subsets of formulas as preferred. In particular, let K be a prioritized knowledge base, where all priorities are taken from
the set {λ0 , . . . , λk } with λ0 < · · · < λk . As for possibilistic knowledge bases, we write K λ to denote the set of formulas in
K whose priority is at least λ. A consistent set of formulas F = F k ∪ · · · ∪ F 0 is then called an (inclusion-based) preferred
subtheory of K , with F l a subset of the formulas that appear in K with priority λl (l ∈ {0, . . . , k}), iff F k ∪ · · · ∪ F l is a maximal
consistent subset of K λl for all l ∈ {0, . . . , k}, i.e. there is no consistent subset F of K λl such that F ⊃ F k ∪ · · · ∪ F l . This
boils down to selecting as many formulas with priority λk as possible (without getting inconsistency), subsequently adding
as many formulas with priority λk−1 as possible, etc. Given a prioritized knowledge base K , we write Pref ⊆ ( K ) for the set
of all inclusion-based preferred subtheories of K . A standard approach to deal with inconsistency in prioritized knowledge
bases is to only consider formulas that are entailed by every preferred subtheory. Considering again the weighted knowledge
base P deﬁned in (31), this leads to the following merging operator:

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

prior⊆

1833

Pref ⊆ ( P ), A @s1 ∪ · · · ∪ A @sn

( K 1 , . . . , K n ) = forgetVar

By considering P instead of P , the merging operator prior⊆ is obtained. Note that when we interpret P as a possibilistic
knowledge base, whenever I is a model of a preferred subtheory of P we also have that I ∈ ❏ P inc( P ) ❑. This means in
particular that
prior⊆
prior⊆

(K 1, . . . , Kn ) |

poss

(K 1, . . . , Kn )

(35)

(K 1, . . . , Kn ) |

poss

(K 1, . . . , Kn )

(36)

i.e. the use of preferred subtheories is indeed a reﬁnement of the possibilistic treatment of inconsistency.
Example 13. Let us go back to the scenario of Example 12, and let us consider the weighted knowledge base P , which
is not equivalent to considering P , in contrast to the possibilistic setting. In general, using P may lead to more cautious
results, whereas using P may lead to more informative results. Since P λ2 is consistent, we know that every preferred
subtheory should in each case contain all formulas that appear in P with weight at least λ2 . There are two different
maximal consistent subsets of P λ1 , leading to the following two preferred subtheories

B1 = P ∗ \
B2 = P

∗

α(0,s1 ,li( p,t )) , α(0,s2 ,wo( p,sahg )) , α(1,s2 ,wo( p,sahg ))

\ α(0,s1 ,wo( p ,m)) , α(1,s1 ,wo( p ,m)) , α(0,s1 ,li( p ,t )) , α(1,s1 ,li( p ,t )) , α(0,s2 ,wo( p ,sahg ))

We ﬁnd
B

K 1 1 = mar( p ), li( p , t ) ∨ li( p , gt ), wo( p , m)
B

K 1 2 = mar( p ), li( p , t ) ∨ li( p , gt ) ∨ wo( p , t ), wo( p , m) ∨ li( p , m)
B

K 2 1 = mar( p ), wo( p , saa ) ∨ wo( p , sacda ) ∨ wo( p , sahg ) ∨ wo( p , gt ) ∨ li( p , sahg ) ∨ wo( p , saq )
B

K 2 2 = mar( p ), wo( p , saa ) ∨ wo( p , sacda ) ∨ wo( p , sahg ) ∨ wo( p , gt ) ∨ wo( p , saq )
which, using Proposition 10 leads to
prior⊆

(K1, K2) ≡ C ∪

B

B

B

B

K1 1 ∧ K2 1 ∨ K1 2 ∧ K2 2

≡ C ∪ mar( p ), li( p , sahg ) ∧ wo( p , m) ∨ wo( p , t ) ∧ li( p , m)
Hence, as desired, the belief that Peter is married is kept, while obtaining the same conclusions about where Peter might
live or work as in the possibilistic setting (i.e. Example 12).
5.3.2. Semantic characterization
Semantically, the merging operators prior⊆ and prior⊆ are similar in spirit to the idea of conﬂict based merging. In
the following, we reveal the exact relationship between both approaches. At a given tolerance level l we may deﬁne the
following partial conﬂict sets for all interpretations I and J

diff aW l ( I , J ) = J \ I
diff bX l ( I ,
diff cY l ( I ,
diff dZ l ( I ,

J) = I \ J

(37)

Wl
Xl

(38)

J ) = [ I ]Y l \ J

(39)

J ) = [ J ]Zl \ I

(40)

Note in particular that for l = 0, we recover the standard notion of conﬂict set, as deﬁned in (6), i.e. diff ( I , J ) =
diff bX l ( I ,

J ) ∪ diff cY l ( I ,

J ) ∪ diff dZ l ( I ,

J ). An atom p will be contained in

diff aW l ( I ,

diff aW l ( I ,

J)∪

J ) if it is in J and it is not similar to an atom

in I (where similarity is deﬁned w.r.t. the relation W l ). The intuition behind the other partial conﬂict sets is analogous. The
higher the value of l, the more pairs of atoms are considered similar, and the fewer atoms remain in the partial conﬂict
sets, i.e. for l > 0 we have

diff aW l ( I , J ) ⊆ diff aW l−1 ( I , J ),
diff cY l ( I , J ) ⊆ diff cY l−1 ( I , J ),

diff bX l ( I , J ) ⊆ diff bX l−1 ( I , J )
diff dZ l ( I , J ) ⊆ diff dZ l−1 ( I , J )

Intuitively, for larger values of l, the sets (37)–(40) only contain the most important conﬂicts between I and J . This idea of
E
that was deﬁned in Section 2.2.
priorities among conﬂicts will allow us to reﬁne the relation conﬂ
To gather all the information about the conﬂict at a given tolerance level l, two alternatives present themselves:

1834

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

diff 1Sl ( I , J ) = diff aW l ( I , J ), diff bX l ( I , J ), diff cY l ( I , J ), diff dZ l ( I , J )
diff 2Sl ( I , J ) = diff aW l ( I , J ) ∪ diff bX l ( I , J ) ∪ diff cY l ( I , J ) ∪ diff dZ l ( I , J )
where we write Sl as a shorthand for ( W l , X l , Y l , Z l ). Note that diff 1Sl is more informative, while diff 2Sl stays closer to

the standard approach of conﬂict-based merging. In particular, for l = 0, diff 2Sl ( I , J ) = diff ( I , J ) with diff ( I , J ) deﬁned as
in (6). In contrast to standard conﬂict-based merging, where the conﬂict between two interpretations is characterized using
one conﬂict set, here we have a different conﬂict set for each tolerance level. All available information about the conﬂict
between two interpretations is thus described as a vector of conﬂict sets:

diff 1S ( I , J ) = diff 1Sk ( I , J ), . . . , diff 1S0 ( I , J )
and similar for diff 2S . We use S as a shorthand for (Sk , . . . , S0 ). Analogously as for the standard conﬂict-based merging
scheme, we may then deﬁne the conﬂict between an interpretation I and a knowledge base K . We consider the following
two variants:

diff 1S ( I , K ) = min diff 1S ( I , J )

J ∈ ❏ K ❑ , lex ⊆4 , . . . , ⊆4

diff 2S ( I , K ) = min diff 2S ( I , J )

J ∈ ❏ K ❑ , lex(⊆, . . . , ⊆)

where ⊆ = par(⊆, ⊆, ⊆, ⊆). Note the use of the lexicographic ordering here. Indeed, when comparing conﬂicts, it makes
sense to ﬁrst look at the most important conﬂicts, i.e. those that only disappear when considering high values for the
tolerance level l.
Accordingly, we may deﬁne conﬂict vectors between an interpretation I and a list of knowledge bases K = ( K 1 , . . . , K n ):
4

diff 1S ( I , K) =

c 1 , . . . , cn

∀i . c i ∈ diff 1S ( I , K i )

and similar for diff 2S . The conﬂict vectors in diff 1S ( I , K) and diff 2S ( I , K) are now vectors of vectors of conﬂict representations
between interpretations. To compare such conﬂict vectors, ﬁrst let us deﬁne l1 and l2 as

ck1 , . . . , c01 , . . . , ckn , . . . , cn0

1
l

cl1 , . . . , cln , c1l , . . . , cnl

iff

ck1 , . . . , c 10 , . . . , cnk , . . . , cn0
cl1 , . . . , cnl , c 1l , . . . , cnl

iff

k

c1k , . . . , c10 , . . . , c n , . . . , cn0

∈ par ⊆4 , . . . , ⊆4
2
l

c 1k , . . . , c 10 , . . . , cnk , . . . , cn0

∈ par(⊆, . . . , ⊆)

where the cli in the ﬁrst inequality are 4-tuples of sets of atoms, while the cli in the second inequality are simply sets of
atoms. Thus, l1 and l2 compare conﬂict vectors, by considering the conﬂicts between interpretations and knowledge bases
at a given tolerance level l. We now have the following characterization.
Proposition 3. Let P and P be deﬁned as before and assume that λ(l,si ,a) = λl for every source si and atom a, with λk < 1. It holds
that

with

prior⊆

(K 1, . . . , Kn ) ≡

conﬂ2

K 1 , . . . , K n ; C ; diff 1S , lex

1
k,...,

1
0

(41)

prior⊆

(K 1, . . . , Kn ) ≡

conﬂ2

K 1 , . . . , K n ; C ; diff 2S , lex

2
k,...,

2
0

(42)

conﬂ2

deﬁned as in (8).

This means that semantically, the use of preferred subsets in prior⊆ and prior⊆ is very close to the standard approach
of conﬂict-based merging. The key difference is that the conﬂict between two interpretations is not represented as a single
set, but as a vector of (vectors of) sets, discriminating between conﬂicts that are more important than others, in the sense
that they are more diﬃcult to explain in terms of ﬂexible usage of atoms.
5.4. Cardinality-based preferred subtheories
5.4.1. Merging operators
A further reﬁnement can be obtained by looking only at consistent subsets of maximal cardinality. A consistent set of
formulas F = F k ∪ · · · ∪ F 0 is a cardinality-based preferred subtheory of K , with F l a subset of the formulas that appear
in K with priority λl (l ∈ {0, . . . , k}), iff for every consistent set of formulas F = F k ∪ · · · ∪ F 0 , with F l a subset of the
formulas that appear in K with priority λl , it holds that either ∀l . | F l | = | F l | or there exists some l∗ ∈ {0, . . . , k} such that
| F l | = | F l | for all l > l∗ and | F l∗ | > | F l∗ |. We write Pref card ( K ) for the set of all cardinality-based preferred subtheories of K .
The corresponding merging operators are given by

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

priorcard

Pref card ( P ), A @s1 ∪ · · · ∪ A @sn

( K 1 , . . . , K n ) = forgetVar

Again we consider the variant using P instead of P , which we write as
i.e.
priorcard

prior⊆

(K 1, . . . , Kn ) |
poss

1835

priorcard

(43)
. Clearly,

prior card

is a reﬁnement of

prior⊆

,

(K 1, . . . , Kn )

prior⊆

The choice between
,
and priorcard depends, in practice, on how the weights can be interpreted. Evidently,
the weaker the merging operator, the weaker the assumptions that need to be made on the meaning of the weights. In
particular, in the possibilistic approach, weights may be interpreted in a purely qualitative way: formulas with a higher
weight are assumed to be more plausible than formulas with a lower weight. The intuitive interpretation of the priorities,
in the case of prior⊆ is in terms of order-of-magnitudes of probabilities [37,38], which can be formally studied using
Spohn’s ordinal conditional functions [39], or equivalently using quantitative possibility theory [40]. The fact that a formula
φ has a higher priority than a formula ψ then intuitively means that the probability that φ is true is an order-of-magnitude
higher than the probability that ψ is true. Under this interpretation, the preferred subtheories may be seen as those subsets
of formulas that most likely correspond to the formulas that are correct. The restriction to consistent subsets of maximal
cardinality, when using priorcard , is motivated from the additional assumption that formulas with the same priority have
approximately the same probability of being true, and an assumption of independence, i.e. the probability that one formula
is true, is independent from the probability that other formulas with the same priority are true.
Example 14. Continuing on Example 13, we ﬁnd that the inclusion-preferred subtheory B 2 is not cardinality preferred, since
B 1 satisﬁes strictly more formulas from P with weight λ1 . Thus we obtain
card⊆

( K 1 , K 2 ) ≡ C ∪ K 1B 1 ∧ K 2B 1 ≡ C ∪ mar( p ), li( p , sahg ), wo( p , m)

which is more informative, and less cautious, than the result we found in Example 13.
5.4.2. Semantic characterization
Next, we establish a semantic counterpart to priorcard and priorcard , and show the connection between these operators
and the distance based framework. First, note that two “distances” can naturally be deﬁned between two interpretations, at
each tolerance level l:

dl1 ( I , J ) = J \ I
dl2 ( I ,

J) =

J\ I

Wl
Wl

+ I\ J

Xl

∪ I\ J

Xl

+ [ I ]Y l \ J + [ J ] Z l \ I
∪ [ I ]Y l \ J ∪ [ J ] Z l \ I

The notion of distance that is considered here is very weak, obeying neither the triangle inequality nor symmetry. Nonetheless, we will refer to dl1 and dl2 as distances, to preserve the terminology of the distance-based framework. Clearly, in the
speciﬁc case where l = 0, we have

dl2 ( I , J ) =

1
2

dl1 ( I , J ) = dHam ( I , J )

while for l > 0, we ﬁnd

dl1 ( I , J )

dl1−1 ( I , J ),

dl2 ( I , J )

dl2−1 ( I , J )

That is, for larger values of l, the distances dl1 and dl2 only consider the most important conﬂicts. The distance between two
interpretations, independent of a given tolerance level l, can be represented as a (k + 1)-dimensional vector:

d1 ( I , J ) = dk1 ( I , J ), . . . , d10 ( I , J )

(44)

The distance between an interpretation and a knowledge base K is then given by

d1 ( I , K ) = min d1 ( I , J )

J ∈ ❏ K ❑ , lex( , . . . ,

)

In particular, note that to compare distance vectors, we ﬁrst look at the distances for higher tolerance levels l, as these
reveal the most important discrepancies between interpretations, using the distances at lower tolerance levels only as a
further reﬁnement. The distance between an interpretation and a list of knowledge bases K = ( K 1 , . . . , K n ) is then given by

d1 ( I , K) = d1 ( I , K 1 ) + · · · + d1 ( I , K n )
The only difference with standard distance-based merging, when considering f =
in (4), is that the additions in the
right-hand side are vector additions instead of additions between real numbers. It turns out that d1 ( I , K) and d2 ( I , K)
correspond to the notion of distance underlying priorcard and priorcard . First, note that the closeness of interpretations to K
can be compared by

1836

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1

I

I

d1 ( I , K), d1 I , K

iff

∈ lex( , . . . , )

In entirely the same way, we arrive at the relation
sition.

2

(45)

by considering dl2 instead of dl1 . We then have the following propo-

Proposition 4. Let P and P be deﬁned as before and assume that λ(l,si ,a) = λl for every source si and atom a, with λk < 1. It holds
that
priorcard
priorcard

(K 1, . . . , Kn ) ≡

dist

(K 1, . . . , Kn ; C ;

1)

(46)

(K 1, . . . , Kn ) ≡

dist

(K 1, . . . , Kn ; C ;

2)

(47)

Note that for k = 0, we obtain a syntactic encoding of distance-based merging with the Hamming distance, for the special
(but common) case where the sum is used as aggregation operator; existing syntactic encodings of Hamming-distance based
merging can be found in [5] and [41].
5.5. Partially ordered weights
5.5.1. Merging operators
The results in Propositions 2, 3 and 4 are based on the assumption that λ×
(l,si ,a) = λ(l,si ,a) = λl for every × ∈ { W , X , Y , Z },
×
source si and atom a. Due to the assumption that l1 l2 implies λ(l ,s ,a) λ×
λ(l2 ,si ,a) , this means in
(l2 ,si ,a) and λ(l1 ,si ,a)
1 i
particular that all priorities are totally ordered. In practice, however, this requirement is often too strong. Consider again
the weather forecast scenario from Example 11. It may be the case that some source s1 is known to be more reliable than
another source s2 , in the sense that predictions from s1 are likely to be closer to the truth than predictions from s2 . In such
a situation, however, it is often not possible to exactly quantify the difference in reliability between s1 and s2 . For example,
we may be in a case where we can reasonably assume that λ(1,s1 ,os) > λ(1,s2 ,os) , without having suﬃcient information to
decide whether λ(1,s1 ,os) > λ(2,s2 ,os) , λ(1,s1 ,os) = λ(2,s2 ,os) or λ(1,s1 ,os) < λ(2,s2 ,os) . Moreover, even the assumption that, for a
ﬁxed source s, λ(1,s,os) = λ(1,s,oc) may be considered too strict. It may, for instance, be the case that os is less likely to be
accurate than oc (e.g. because the sources prefer an optimistic attitude when available evidence is inconclusive). In general,
we will typically not be able to exactly quantify the “amount of stretching” that is needed to go from one atom to another,
and as a consequence, insisting that λ(l,s,a) = λ(l,s,b) for a = b may be considered too imprudent.
We now show how the merging operators that have been proposed may be adapted to cope with partially ordered
certainty weights. The possibilistic merging operators may still be used, provided that we extend the notion of λ-cut to
partially ordered certainty weights. Several such extensions have been proposed in [42], based on the idea of selecting
particular maximal consistent subsets. In particular if K 1 and K 2 are two subsets of a possibilistic knowledge base K with
partially ordered weights, we consider the following relation:

K1

K2

iff

∀(α1 , λ1 ) ∈
/ K 1 . ∃(α2 , λ2 ) ∈
/ K 2 . λ1

λ2

Let Pref ( K ) be the corresponding preferred subtheories, i.e.

Pref ( K ) = B ∗ B ∈ min Cons( K ),
where we write Cons( K ) for the consistent subsets of K , and B ∗ = {α | (α , λ) ∈ K }. We may then consider the following
merging operator:
poss

( K 1 , . . . , K n ) = forgetVar

and the variant
poss

and

poss

Pref ( P ), A @s1 ∪ · · · ∪ A @sn

which is based on P . It is easy to see that when λ×
(l,si ,a) = λ(l,si ,a) = λl , we have indeed that both
coincide with (33).
poss

Example 15. When going back to the weather forecast of Example 11, we may notice that poss , prior⊆ and priorcard
all lead to the same conclusion, viz. {pc2 }. While the use of inclusion- and cardinality-based preferred subtheories reﬁnes
the possibilistic approach, by avoiding the drowning effect, these strategies are essentially based on the same intuition of
priority. This intuition dictates that pc2 is a more plausible conclusion than e.g. pc1 as pc1 is intuitively further away from
oc than pc2 is away from either os or oc. In other words, what is maximized is the minimal similarity between models of
the resulting base and any given knowledge base K 1 . In practice, however, we may be less conﬁdent in where exactly is the
middle between os and oc. Assume that the weights are ordered as follows:

λ(l,s,a)

λ(l ,s ,a ) iff

a =a ∧l

l ∨ l

with θ = 1. Then Pref ( P ) contains three subsets:

l −θ

(48)

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1837

B pc1 = P ∗ \ {α(0,s1 ,os) , α(0,s2 ,os) , α(0,s3 ,oc) , α(1,s3 ,oc) , α(2,s3 ,oc) }
B pc2 = P ∗ \ {α(0,s1 ,os) , α(1,s1 ,os) , α(0,s2 ,os) , α(1,s2 ,os) , α(0,s3 ,oc) , α(1,s3 ,oc) }
B pc3 = P ∗ \ {α(0,s1 ,os) , α(1,s1 ,os) , α(2,s1 ,os) , α(0,s2 ,os) , α(1,s2 ,os) , α(2,s2 ,os) , α(0,s3 ,oc) }
which leads to
poss

( K 1 , K 2 , K 3 ) ≡ C ∪ {pc1 ∨ pc2 ∨ pc3 }

In the same way, choosing θ = 2, we obtain the trivial conclusion
poss

( K 1 , K 2 , K 3 ) ≡ C ∪ {os ∨ pc1 ∨ pc2 ∨ pc3 ∨ oc}

Finally, note that by encoding information about the relative reliability of different sources, results may be obtained that are
not centered around pc2 .
The idea of inclusion-based preferred subtheories of a prioritized knowledge base, in the case where priorities are partially ordered, was already proposed in [36]. The basic idea is to consider all possible linearizations κ . Speciﬁcally, let a
linearization κ be any mapping from Λ to [0, 1] satisfying λ1 λ2 ⇒ κ (λ1 ) κ (λ2 ), and let us write Lin(Λ, ) to denote
the set of all such linearizations. Given a linearization κ , the linearized version κ ( K ) of the prioritized knowledge base K is
obtained by replacing all weights λ by their value in [0, 1]:

κ ( K ) = α , κ (λ)

(α , λ) ∈ K

The preferred subtheories of K are then simply the maximal consistent subsets that are preferred for
linearization κ . Thus we arrive at the following merging operators:
prior⊆

( K 1 , . . . , K n ) = forgetVar

Pref ⊆

κ ( K ), for some

κ ( P ) , A @s1 ∪ · · · ∪ A @sn

κ
priorcard

( K 1 , . . . , K n ) = forgetVar

Pref card

κ ( P ) , A @s1 ∪ · · · ∪ A @sn

κ

as well as the variants prior⊆ and priorcard which are based on P . It is trivial to see that when λ×
(l,si ,a) = λ(l,si ,a) = λl , these
deﬁnitions coincide with (35), (43), (35) and (43) respectively.
5.5.2. Semantic characterization
Thus, in the special case where all weights are totally ordered, the deﬁnitions of the operators we have presented here to
cope with partially ordered weights coincide with those that were presented in the previous sections. At the other extreme,
when all weights λ(li ,si ,ai ) and λ(l j ,s j ,a j ) are incomparable unless si = s j and ai = a j , it turns out that prior⊆ , priorcard and
poss

, as well as

prior ⊆

,

priorcard

and

poss

coincide.
×

Proposition 5. Let P and P be deﬁned as before and assume that λ(l i,s ,a )
i

i

i

×

λ(l jj,s j ,a j ) iff li

l j , si = s j , ai = a j and ×i = × j . It holds

that
conﬂ2

K 1 , . . . , K n ; C ; diff 1S , par

Furthermore, when λ(li ,si ,ai )
conﬂ2

λ(l j ,s j ,a j ) iff li

K 1 , . . . , K n ; C ; diff 2S , par

1
k,...,

1
0

≡

poss

≡

prior⊆

≡

priorcard

(K 1, . . . , Kn )
(K 1, . . . , Kn )
(K 1, . . . , Kn )

(49)
(50)
(51)

l j , si = s j and ai = a j , it holds that
2
k,...,

2
0

≡

poss

≡

prior⊆

≡

priorcard

(K 1, . . . , Kn )
(K 1, . . . , Kn )
(K 1, . . . , Kn )

(52)
(53)
(54)

The fact that merging operators correspond to the Pareto extensions par( k1 , . . . , 10 ) and par( k2 , . . . , 20 ), rather than
the lexicographic extensions as in Proposition 3 conﬁrms our intuition that by being more cautious in deﬁning the ordering
on Λ, a more tolerant merging operator is obtained which provides more cautious results. In practice, useful merging
operators may be somewhere in between, adopting a balance between cautiousness and informativity.

1838

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

As a general remark, let us also mention that when the weights are maximally incomparable, as in Proposition 5, the
different approaches recover the intersection of the maximal consistent subsets. In this way, these approaches implement
the idea that was suggested after Example 12, to apply the possibilistic approach, but only looking at those formulas that
are not involved in any conﬂict.
5.6. Penalties
5.6.1. Merging operators
The treatment of partially ordered weights in the previous section deals with situations where less information is available than is required by the merging operators from Sections 5.2–5.4. In this section, we show how penalty logic, conversely,
allows to handle the case where precise information is available about the certainty of the statements in M si or M si . To this

end, we assume that the weights λ×
λ×
(l,s,a) and λ(l,s,a) are non-negative integers, i.e. 0
(l,s,a) , λ(l,s,a) < +∞. The weighted
6
knowledge bases M si and M si then correspond to theories in penalty logic [27,28]. In this case, rather than encoding an
ordering, the weights have a numerical interpretation with a probabilistic ﬂavor.
Let us deﬁne the following penalty logic bases:

Q =

@s i

( M si )λ ∪
i

(α , +∞) α ∈ K i

∪ (c , +∞) c ∈ C

(55)

i

Q =

@s i

M si λ ∪
i

(α , +∞) α ∈ K i

∪ (c , +∞) c ∈ C

(56)

i

and let us write Q ∗ and Q ∗ for the corresponding classical knowledge bases that are obtained by ignoring the penalties.
In [28], some relationships between penalty logic and Dempster–Shafer theory [43,44] are revealed, essentially suggesting
to interpret a penalty logic formula (α , p ) as an upper bound on the probability that α is violated: P (¬α ) e − p . When
the correctness of formulas appearing in a penalty logic base K is independent of the correctness of other formulas, the
probability that a set of formulas (αi , p i ) from K are violated in the true world is then upper bounded by i e − p i = e − i p i .
This independence assumption is, however, quite strong, and presupposes that no formula is entailed by a subset of the
remaining formulas. In particular, the independence assumption cannot be made in the case of Q and Q , since we already
have α(×l−1,s,a) → α(×l,s,a) for any source s, atom a, × ∈ { W , X , Y , Z }, and l > 0. However, in the case of Q , it seems natural
to interpret λ(l,s,a) by

P (¬α(l,s,a) | ¬α(l−1,s,a) )

e −λ(l,s,a)

for l > 0, and

P (¬α(0,s,a) )

e −λ(0,s,a)

This only presupposes that the amount of tolerance required for a given atom when considering a given source, is independent of the amount of tolerance required for other atoms and other sources. A similar interpretation could be given to the
, α(Xl,s,a) ,
penalties in Q , although this requires additional information about the probability that the four implications α(W
l,s,a)

α(Yl,s,a) and α(Zl,s,a) are correct, and about the dependencies between these probabilities.
Given a penalty logic base K , we may consider the following relation between subsets B 1 and B 2 of formulas that appear
in a weighted knowledge base K :

B1

p

B2

iff pen K ( B 1 )

pen K ( B 2 )

where pen K ( B i ) = (α , p )∈ K ,α ∈/ B i p (see Appendix A for further details). We write Pref pen ( K ) to denote the subsets of formulas that are minimal w.r.t. p . Merging operators corresponding to Q and Q can then be deﬁned as follows:
pen
pen

( K 1 , . . . , K n ) = forgetVar

Pref pen ( Q ), A @s1 ∪ · · · ∪ A @sn

(57)

( K 1 , . . . , K n ) = forgetVar

Pref pen Q , A @s1 ∪ · · · ∪ A @sn

(58)

Now assume, as in Sections 5.2–5.4 that λ(l,si ,a) is independent of the source si and the atom a, and depends only on
the tolerance level l, i.e. λ(l,si ,a) = λl . From an application point of view, a natural choice seems to assume that λl is equal
to a constant. As the exact value of this constant does not affect the result of the merging operators (57) and (58), we may
assume this constant to be 1. It is not hard to see that in this case, the elements of Pref pen ( Q ) and Pref pen ( Q ) simply are
the consistent subsets with maximal cardinality of Q and Q . Note that in this case, we have the following interpretation

6

A brief introduction to penalty logic is provided in Appendix A.

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

P (¬α(l,si ,a) ) = P (¬α(l,si ,a) | ¬α(l−1,si ,a) ) · · · · · P (¬α(0,si ,a) )

1839

γ l +1

for some constant γ .
As a second important interpretation of the penalties λl , we consider the case where tolerance levels correspond to
different order-of-magnitudes of probabilities. In this view, the probability P (¬α(l,si ,a) | ¬α(l−1,si ,a) ) should be decreasing
l

in l. In particular, we assume that this conditional probability is upper bounded by r for some ∈ ]0, 1[ and a suﬃciently
large r. The corresponding penalty is of the form λl = γ · rl , where in fact γ = − log( ). Note that the exact value of γ > 0
does not inﬂuence the result, hence we may assume γ = 1. Recall that the motivation of using cardinality-based preferred
subtheories was also in terms of order-of-magnitudes of probabilities. Accordingly, we have the following proposition.
l
Proposition 6. Let Q and Q be deﬁned as before, and assume that λ×
(l,s,a) = λ(l,s,a) = λl = r for some r ∈ N. If r is suﬃciently large,
it holds that

pen
pen

(K 1, . . . , Kn ) ≡
(K 1, . . . , Kn ) ≡

priorcard

(K 1, . . . , Kn )

priorcard

(59)

(K 1, . . . , Kn )

(60)

In addition to constant weights (λl = 1) and exponential weights (λl = rl ), we may also consider weights that are proportional to l + 1, i.e. choose λl = l + 1. Note that l + 1 is considered rather than l to ensure a non-zero weight for the case
where l = 0.
Example 16. Consider again the weather scenario from Example 11. Recall that the approaches from Sections 5.2–5.4 all
yield the same result pc2 , while the more cautious alternative pc1 ∨ pc2 ∨ pc3 and the trivial conclusion os ∨ pc1 ∨ pc2 ∨ pc3 ∨ oc
can be obtained using the partially ordered weights from Section 5.5. Example 11 suggests a number of other possibilities,
which, as we will see, can be obtained by interpreting the penalties as weights.
There are ﬁve maximal consistent subsets of Q :

B os = Q ∗ \ {α(0,s3 ,oc) , α(1,s3 ,oc) , α(2,s3 ,oc) , α(3,s3 ,oc) }
B pc1 = Q ∗ \ {α(0,s1 ,os) , α(0,s2 ,os) , α(0,s3 ,oc) , α(1,s3 ,oc) , α(2,s3 ,oc) }
B pc2 = Q ∗ \ {α(0,s1 ,os) , α(1,s1 ,os) , α(0,s2 ,os) , α(1,s2 ,os) , α(0,s3 ,oc) , α(1,s3 ,oc) }
B pc3 = Q ∗ \ {α(0,s1 ,os) , α(1,s1 ,os) , α(2,s1 ,os) , α(0,s2 ,os) , α(1,s2 ,os) , α(2,s2 ,os) , α(0,s3 ,oc) }
B oc = Q ∗ \ {α(0,s1 ,os) , α(1,s1 ,os) , α(2,s1 ,os) , α(3,s1 ,os) , α(0,s2 ,os) , α(1,s2 ,os) , α(2,s2 ,os) , α(3,s2 ,os) }
When the penalties in M si are constant, the following penalties are obtained (for λl = 1):

pen Q ( B os ) = 4,

pen Q ( B pc1 ) = 5,

pen Q ( B pc2 ) = 6,

pen Q ( B pc3 ) = 7,

pen Q ( B oc ) = 8

Hence only B os is minimal which leads to
pen

( K 1 , K 2 , K 3 ) ≡ C ∪ {os}

When considering exponential penalties of the form λl = rl , with a suﬃciently large r, we obtain using (58)
pen

( K 1 , K 2 , K 3 ) ≡ C ∪ {pc2 }

Finally, when considering penalties of the form λl = l + 1, we ﬁnd

pen Q ( B os ) = 10,

pen Q ( B pc1 ) = 8,

pen Q ( B pc2 ) = 9,

pen Q ( B pc3 ) = 13,

pen Q ( B oc ) = 20

leading to
pen

( K 1 , K 2 , K 3 ) ≡ C ∪ {pc1 }

Clearly, by using constant penalties of the form λl = 1, the result reﬂects the opinion of the majority. By choosing exponential penalties of the form λl = rl , for a suﬃciently large r, we obtain an opinion in the middle, as for the operators that
were discussed in Sections 5.2–5.4. Finally, using linearly increasing penalties of the form λl = r + 1, the result intuitively reﬂects the center-of-gravity of the opinions held by the sources. This latter behavior is related to least squares approximation,
which can be made explicit as follows. For each atom a, let us write l∗(a,s) to denote the largest value of l for which α(l,s,a) is
violated in the real world, where l∗(a,s) = −1 if even α(0,s,a) is satisﬁed. Then l∗(a,s) + 1 measures how abnormal the situation
w.r.t. atom a and source s is. If we make the assumption that the abnormality l∗(a,s) + 1 is, in fact, the discrete approximation
of a continuous parameter which follows a normal distribution, and the degree of abnormality of each atom a and source

1840

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855
l

2
s is independent, the most likely situation is recovered by pen when the penalties are such that
i =0 λi = γ · (l + 1) for
some constant γ . This is due to the well-known correspondence between least squares and maximum likelihood estimation
l
1
of normally distributed variables. By taking λl = l + 1, we ﬁnd
i =0 λi = 2 · (l + 1) · (l + 2), which may indeed serve as an

approximation of γ · (l + 1)2 for γ = 12 .
Depending on how the penalties are chosen, we either ﬁnd a solution in the middle between the beliefs of the different
sources, a solution expressing the majority opinion, or the centre-of-gravity of the beliefs of the sources. Formally, however,
it is not hard to show that for every non-trivial choice of penalties (i.e. 0 < λ(l,s,a) < +∞ for all l, s and a), pen and pen
are majority operators in the sense of [17]:

∀ K 1 , K 2 . ∃n ∈ N .

pen

(K1, . . . , K1, K2) | K1
n

and similar for pen . On the other hand, we do have the intuition that the more λl increases with l, the less the notion
of majority plays a role. Interestingly, such weak majority operators are also studied in [45], considering a distance-based
merging operator which is based on the square of the Hamming distance.
Finally, note that, due to their quantitative nature, a more cautious variant of pen and pen can straightforwardly be
deﬁned, by being more tolerant in the deﬁnition of Pref pen ( Q ) and Pref pen ( Q ). For instance, we may deﬁne

B 1 ∈ Pref pen ( Q )
for some constant

γ

iff pen Q ( B 1 )

γ · min pen Q ( B )
B

1. Thus, in the scenario of Example 11, results such as os ∨ pc1 , or os ∨ pc1 ∨ pc2 may be obtained.

5.6.2. Semantic characterization
Semantically, pen and pen can be described in the distance-based framework, provided that the penalties do not
×
depend on the underlying source,7 i.e. for each × ∈ { W , X , Y , Z }, tolerance level l, source s and atom a, λ×
(l,s,a) = λ(l,a) for
×
some λ(l,a) ∈ [0, +∞], and λ(l,s,a) = λ(l,a) for some λ(l,a) ∈ [0, +∞]. Then, we may consider the following distances:

d1Q ( I , J ) =

λ(Wl,a) l ∈ {0, . . . , k}, a ∈ J \ I
+

d2Q ( I , J ) =

Wl

+

λ(Xl,a) l ∈ {0, . . . , k}, a ∈ I \ J

λ(Yl,a) l ∈ {0, . . . , k}, a ∈ [ I ]Y l \ J +
λ(l,a) l ∈ {0, . . . , k}, a ∈ J \ I

Wl

∪I\ J

Xl

λ(Zl,a) l ∈ {0, . . . , k}, a ∈ [ J ] Z l \ I
Xl

∪ [ I ]Y l \ J ∪ [ J ] Z l \ I

Accordingly, we may deﬁne the distance between an interpretation and a knowledge base, or an interpretation and a list of
knowledge bases, exactly as in the distance-based framework. This leads to the following proposition.
×
Proposition 7. Let Q and Q be deﬁned as before and assume that λ×
(l,si ,a) = λ(l,a) < +∞ and λ(l,si ,a) = λl,a < +∞ for every source
si and atom a. It holds that
pen
pen

(K 1, . . . , Kn ) ≡

dist

(K 1, . . . , Kn ; C ;

(K 1, . . . , Kn ) ≡

dist

(K 1, . . . , Kn ; C ;

d1Q

)

(61)

d2Q

)

(62)

When comparing Proposition 7 with Proposition 4, it becomes obvious that pen and priorcard present two different
solutions to deal with the fact that the distance between interpretations is most naturally represented as a vector in our
setting, which was deﬁned in (44). While pen uses a quantitative approach, which aggregates such vectors to scalar values,
before proceeding in the standard distance-based framework (albeit with a non-standard distance), priorcard follows a more
qualitative approach, in which a lexicographic extension of the distance-based framework is rather used.
6. Computational complexity
In this section, we investigate the computational complexity of our merging operators. First note that the proposed
merging operators have syntactically been deﬁned in existing logical formalisms. Hence existing algorithms for reasoning in
e.g. possibilistic logic or penalty logic can readily be reused to implement the merging operators. Moreover, the membership
results for these existing frameworks immediately carry over to the present setting.
Typically, propositional merging tasks are at the lower levels of the polynomial hierarchy. Recall that the complexity
classes, kP , ΣkP and ΠkP , which constitute the polynomial hierarchy, are deﬁned as follows (i ∈ N) [46]:
7
The general case can be treated by a straightforward generalization of the distance-based framework in which a difference distance is used for each
source; we omit the details.

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1841

Table 2
Complexity of entailment checking for different merging operators.
linear weights

Θ2P -complete

partially ordered weights

Π2P -complete

linear weights

Π2P -complete

partially ordered weights

Π2P -complete

priorcard

linear weights

P
2 -complete
Π2P -complete

pen

exponentially bounded penalties

poss

prior⊆

partially ordered weights

polynomially bounded penalties

P
0

= Σ0P = Π0P = P,
P

P
i +1

P

= PΣ i ,

P

ΣiP+1 = NPΣi ,

P
2 -complete

Θ2P -complete

ΠiP+1 = co ΣiP+1

P

where NPΣi (resp. PΣi ) is the class of problems that can be solved in polynomial time on a non-deterministic machine
(resp. deterministic machine) with a ΣiP oracle, i.e. assuming a procedure that can solve ΣiP problems in constant time.
All problems in the polynomial hierarchy are solvable with a polynomial amount of space and an exponential amount of
time, e.g. using systematic search based on branch-and-bound. In particular, so far, no complexity class of the polynomial
hierarchy is known to be strictly harder than P . Knowing which class of the polynomial hierarchy a given problem belongs
to, however, is still important, as it serves as an indication of how hard we can expect the problem to be in practice. In
the following, we also refer to the class ΘiP which contains the problems that are solvable in polynomial time using a
logarithmic number of calls to a ΣiP oracle [47].
For each merging operator , the main decision problem consists of checking whether ( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) |
φ for given knowledge bases K i , integrity constraints C , weighted knowledge bases M si and a propositional formula φ over
the set of atoms A. Note that the computational complexity is not affected if the sets W al , X al , Y al and Z al are given as
input instead of the weighted knowledge bases M si . The complexity classes of the merging operators that were studied in
Section 5 are summarized in Table 2. From a complexity point of view, it does not matter whether P or P is considered
( Q or Q in the case of penalties). The following proposition summarizes the main results.
Proposition 8. Let K 1 , . . . , K n be propositional knowledge bases over a set of atoms A, C a set of propositional integrity constraints
over A, and let M s1 , . . . , M sn be deﬁned as in (16). For a propositional formula φ , the complexity of deciding whether

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ
holds is summarized for different merging operators

in Table 2.

Note that by exponentially bounded penalties, we mean that there exists an exponential function f of the problem
size n, such that the value of all penalties is at most f (n). In other words, the number of bits required to encode the
penalties is polynomial in n. Polynomially bounded penalties are then upper bounded by g (n) for a polynomial function g.
In other words, the cases of exponentially and polynomially bounded queries respectively assume that penalties are encoded
using binary and unary notation. Furthermore, note that we do not need to make such a distinction in the other cases, as
possibilistic certainty weights, priorities and symbolic weights are all interpreted in an ordinal way.
In some sense, the complexity results are unsurprising, as it is well-known that entailment relations based on e.g.
possibilistic logic or inclusion-based preferred subtheories are Θ2P and Π2P respectively. What the proposition shows is
mainly that the restricted setting in which possibilistic logic, preferred subtheories, and penalty logic are used does not
cause a decrease in complexity.
Overall, we can see that the possibilistic and penalty-logic based interpretations of the weights yield the most eﬃcient
procedures. Moreover, the complexity of poss depends on the number of different weights. The smaller this number, the
fewer calls to the NP-oracle are required. In particular, in the case of k + 1 different weights λ0 , . . . , λk , entailment can
be veriﬁed by solving 1 + log2 (k + 1) instances of the boolean satisﬁability problem SAT, viz. log2 (k + 1) instances
for calculating the inconsistency level and 1 instance for entailment checking. In the same way, the number of required
satisﬁability checks for priorcard is given by k + 2, when the weights are linearly ordered. On the other hand, prior⊆ (with
linearly or partially ordered weights) and priorcard with partially ordered weights remain computationally hard, even in the
case where k = 1. In the case of pen , the number of required satisﬁability checks depends on both k and the actual values
of the penalties. In addition to these effects of restricting the value of k, in all cases, eﬃcient procedures can be obtained
by restricting the number of atoms that may be weakened, i.e. by taking W al = X al = Y al = Z al = {a} for all l and for all but a
few atoms a.
When weights are interpreted as priorities, a number of entailment relations may be considered that are not based
on calculating the merged knowledge bases prior⊆ ( K 1 , . . . , K n ) and priorcard ( K 1 , . . . , K n ). When the weights are linearly
ordered, well-known entailment relations |≈∃⊆ and |≈∃card are deﬁned as follows:

1842

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

Table 3
Complexity of alternative entailment relations based on priorities.
Entailment relation

Complexity

Entailment relation

Complexity

|≈∃⊆
|≈∃∃
⊆
|≈∃∀
⊆
|≈∀∃
⊆

Σ2P
Σ2P
Σ3P
Π3P

|≈∃

Σ2P
Σ2P
Σ2P
Π3P

card

|≈∃∃
card
|≈∃∀
card
|≈∀∃
card

( K 1 , . . . , K n ) |≈∃⊆ φ iff ∃ B ∈ Pref ⊆ ( P ) . B | φ
( K 1 , . . . , K n ) |≈∃card φ iff ∃ B ∈ Pref card ( P ) . B | φ
Hence, rather than looking for conclusions that are common to all preferred subtheories, we only require that φ is a
conclusion of one of the preferred subtheories. Of course, this means that it may happen that both φ and ¬φ can be
derived. When weights are partially ordered, even more alternatives may be conceived:

( K 1 , . . . , K n ) |≈∃∃
⊆ φ iff ∃κ ∃ B ∈ Pref ⊆ κ ( P ) . B | φ
( K 1 , . . . , K n ) |≈∃∀
⊆ φ iff ∃κ ∀ B ∈ Pref ⊆ κ ( P ) . B | φ
( K 1 , . . . , K n ) |≈∀∃
⊆ φ iff ∀κ ∃ B ∈ Pref ⊆ κ ( P ) . B | φ
and similar for the alternatives based on Pref card . Note that |≈∃∀
⊆ could, for instance, be used to verify whether there exists an
assessment of the relative reliability of the sources, such that a formula φ could be concluded after merging the knowledge
bases. Similarly, |≈∀∃
⊆ could be used to verify whether φ is a plausible consequence, independent of which sources are
considered most reliable.
Interestingly, in some cases, the complexity goes up a level in the polynomial hierarchy when these alternative entailment relations are used.
Proposition 9. Let K 1 , . . . , K n be propositional knowledge bases over a set of atoms A, C a set of propositional integrity constraints
over A, and let M s1 , . . . , M sn be deﬁned as before. For a propositional formula φ , the complexity of deciding whether

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) |≈ φ
holds is summarized for different entailment relations |≈ in Table 3.
7. Related work
Although many approaches to merging propositional knowledge bases have already been studied, we are not aware
of any proposals that model the fact that different atoms may be closely related in meaning, or indeed, that we may
be uncertain about how exactly we should understand a given assertion. On the other hand, the fact that uncertainty in
meaning (or if we prefer ‘vagueness’) is pervading social interaction has been recognized early in AI [48]. This phenomenon
has also been extensively studied by (cognitive) linguists, especially in the context of dialogues, which can be considered
as the simplest form of social interaction. The reasons for adopting vague language in conversations may be many. For
instance, vague language may help the listener to determine how much processing effort should be devoted to a given
utterance, focusing him or her to the most relevant information; it may indicate a lack of certainty about the exact state
of affairs; or it may be used to serve social functions, such as softening implicit complaints and criticism [49]. Whether or
not vagueness in conversations lead to misconceptions depends on the participants’ ability to relate what is expressed to
their common ground. Thus, successful communication depends on the establishment of such a common ground, a process
called grounding [50]. This common ground allows for a common language on which the points of view of the speaker and
listener in a dialogue can be aligned. A key issue in communication is that this alignment between speaker and listener may
be faulty, in which case a repair mechanism is needed [51]. In some cases, the misalignment is deliberate. This aspect of
communication is stressed in [52], where a bipolar view on assertability is put forward, distinguishing between situations
in which a statement is deﬁnitively assertable, situations in which it is merely acceptable to assert it, and situations in
which it cannot be asserted without condemnation. As an example where misalignment is deliberate, [48] considers the
example of a hotel manager, who claims that a room of his hotel is ‘quite large’, despite knowing that the client may not
agree with this. In general, misalignments can be discovered because of inconsistencies, but, due to the interactive nature
of dialogues also by means of explicit clariﬁcation requests and reformulation. Clearly, the issue of merging multiple-source
information can be related to this view. However, as the interactive component is missing, the main vehicle to establish
plausible alignments is by detecting and interpreting inconsistencies. In this sense, the integrity constraints C could be seen
as explicit common ground between the sources, which may or may not be properly aligned with them. The weighted
knowledge bases M si then encode strategies for repairing situations of detected misalignment. This point of view is also

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1843

reminiscent of approaches to causal diagnosis [53], which attempt to ﬁnd the most likely disorders, given a set of observed
symptoms. Here, conﬂicts take the role of symptoms and the possible misalignments take the role of disorders.
Another line of related work is the use of similarity relations between interpretations to deﬁne approximate entailment
relations. This idea was ﬁrst proposed in [54] where “p approximately entails q” is understood as “every model of p is
similar to a model of q”. This idea can be contrasted with non-monotonic reasoning, where “if p, generally q" iff the most
plausible models of p are also models of q [55], i.e. in the former case the set of models of q is expanded, whereas in
the latter case the set of models of p is restricted. A detailed comparison between similarity-based and non-monotonic
entailment was made in [56,57], e.g. pointing out that, while similarity-based entailment is monotonic, it fails to satisfy the
so-called cut property: from p |≈ r and p ∧ r |≈ q it does not follow that p |≈ q. The use of a similarity relation between
interpretations was also brieﬂy discussed from a belief revision point of view in [58].
In certain application domains, ideas have been examined that are to some extent similar to the motivation of our
approach, although only within more restrictive settings. To repair inconsistent description logic ontologies, for instance,
[59] looks for overgeneralized concept inclusions that create conﬂicts, and accordingly replaces concepts by more liberal
or more restrictive variants. For instance, the conﬂict that arises from ‘children only like ice cream’ and ‘children only like
chocolate’ is resolved by replacing these assertions by ‘children only like sweeties’, thus taking advantage of known concept
relations to resolve the conﬂict in a meaningful way. In this approach, we can clearly recognize the view that inconsistencies
are often caused by sources that are not suﬃciently cautious when asserting information, although what they claim may
not be far from the truth. Another related application is presented in [60], where the problem of merging networks of
qualitative temporal and spatial relations is considered. The idea is to deal with conﬂicts by ﬁnding spatial or temporal
scenarios that are similar to scenarios that are compatible with what is claimed by each of the sources. For instance, if one
source claims that spatial regions a and b are disjoint while another asserts that in fact a is a part of b, we could conclude
that a overlaps with b. The notion of similarity here operates at the level of the spatial or temporal relations, and is directly
related to the conceptual neighborhood diagrams of Freksa [31]. Somewhat related, [61] discusses an application in which
temporal relations are extracted from web documents, and conﬂicts are solved by reinterpreting the temporal relationships
as fuzzy temporal relations that only hold to some degree. In the presence of a conﬂict, ‘a happened during b’ may then be
interpreted as, e.g., ‘a happened during b to degree 0.6’. Such fuzzy temporal relations can be modeled in a generalization
of Allen’s interval algebra [62], which was proposed in [63]. Intuitively, ‘a happened during b to degree 0.6’ means that the
degree of similarity between the actual state of affairs and a temporal conﬁguration in which a happened during b is 0.6.
A similar approach in the spatial domain, based on a generalization of the region connection calculus [64], was proposed
in [65].
The idea that ﬂexibility of terms may be interpreted in different ways (cf. the four scenarios from Section 4) is somewhat
reminiscent of the study of linguistic hedges such as ‘very’ in the framework of fuzzy set theory [66,67]. In particular,
a linguistic phrase such as ‘more or less old’ can be understood in at least two different ways. There is an inclusive reading,
in which ‘more or less old’ is a liberalization of ‘old’, i.e. everybody who is considered old, is also considered more or less
old. However, there also is an exclusive reading in which the meaning of ‘more or less old’ does not encompass the meaning
of ‘old’, i.e. people who are very old are not considered to be ‘more or less old’. In other words, when modeling linguistic
hedges, there is a choice between expanding (or contracting, depending on the type of modiﬁer) the meaning of a term and
shifting it [68].
Finally, note that preliminary versions of the approach we have presented in this paper can be found in [69] and [70]. In
particular, [69] has introduced the idea of using a similarity graph and of deﬁning merging operators, at the semantic level,
in terms of the operators . R and [.] R deﬁned in (18)–(19). In [70], merging operators were deﬁned at the syntactic level.
The syntactic procedures that were proposed there essentially correspond to the application of Proposition 10 in the speciﬁc
case where the weights are interpreted in a possibilistic setting, or in the setting of inclusion-based preferred subtheories.
8. Conclusions
In this paper, we have proposed a novel approach to merging, which differs from existing approaches in its use of extralogical background information about the semantic relatedness of atoms. The central idea is that in many applications, atoms
correspond to natural language terms that may be understood in a slightly different way by different sources. By exploiting
available knowledge about which of these terms have a similar meaning (or may otherwise be confused), consistency can
be restored in a more informed way. The requirement for this extra-logical information may be seen as a disadvantage, in
the sense that we cannot expect such information to be available in every application. However, in such cases, the merging
operators we have proposed degenerate to existing approaches such as morphological, conﬂict-based or distance-based
merging. In general, our operators reﬁne (special cases of) these existing approaches using whatever information is available
about the relatedness of terms.
Rather than deﬁning one speciﬁc merging operator, we have proposed a general framework which is based on (i) assuming a disjoint vocabulary for each source to trivially restore consistency, and (ii) introducing an additional weighted
knowledge base to encode ﬂexible constraints on how the vocabularies of different sources relate to each other. Subsequently, we have analyzed how different interpretations of these weights naturally lead to merging operators with a
different behavior. The interpretations we have considered range from purely qualitative approaches, in which weights are
taken from an abstract, partially-ordered scale, to purely quantitative approaches with a probabilistic ﬂavor. This diversity

1844

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

allows us to appropriately handle cases where more, or less information is available about the reliability of sources, the
strength of known semantic relationships, etc.
Each of the proposed merging operators is based on the intuition of ﬁnding the interpretations that are most plausible,
given what is asserted by the sources. In this sense, they are tailored towards merging beliefs, rather than towards merging
preferences or goals. While the exploitation of semantic relatedness between terms is clearly of interest for the latter
problem as well, the scenario of merging preferences additionally requires to consider principles from social choice theory
to ensure that the result is fair.
Acknowledgements
We would like to thank the anonymous reviewers, whose detailed comments have greatly improved the readability of
this paper.
Appendix A. Tools for managing inconsistency
A.1. Possibilistic logic
A possibility distribution in a universe X is an X − [0, 1] mapping, used as a convenient way to encode a complete ordering that models plausibility or preference. Given a possibility distribution π on the set of all possible interpretations 2 A ,
the possibility Π(φ) and necessity N (φ) of a formula φ are deﬁned as

Π(φ) = max π ( I )
I ∈❏φ❑

N (φ) = 1 − Π(¬φ) = min 1 − π ( I )
I ∈❏¬φ❑

A possibilistic logic [25] formula (φ, λ) is a pair made of a classical logic formula φ , and a weight λ ∈ ]0, 1] expressing its
certainty or priority. The formula (φ, λ) is semantically interpreted as N (φ) λ, where N is a necessity measure. Necessity
measures N are characterized by the decomposability property N (φ ∧ ψ) = min( N (φ), N (ψ)). A possibilistic logic base, i.e.
a set of possibilistic logic formulas can always be put in clausal form thanks to this property. The basic inference rule is the
following resolution rule, here written in the propositional case:

(¬φ ∨ ψ, λ); (φ ∨ γ , μ)

ψ ∨ γ , min(λ, μ)

Given a possibilistic logic base K and a certainty level λ, two particular classical knowledge bases can be deﬁned:

K λ = φ (φ, μ) ∈ K and μ

λ

K λ = φ (φ, μ) ∈ K and μ > λ
A possibilistic logic base K = {(φi , λi ) | i = 1, n} is semantically equivalent to a possibility distribution π K which restricts
the set of interpretations that are more or less compatible with K . The possibility distribution π K is the min-based conjunctive combination of the representations π(φi ,λi ) of each formula in K . An interpretation I is all the more possible as it
does not violate any formula φi with a high certainty level λi :

π(φi ,λi ) ( I ) =

1
1 − λi

if I
if I

φi
¬φi

π K ( I ) = min π(φi ,λi ) ( I )
i

An important feature of possibilistic logic is its ability to cope with inconsistency. The inconsistency level of K is deﬁned as

inc( K ) = max{λ | K λ inconsistent}

(A.1)

with max ∅ = 0. We have inc( K ) = λ iff max I π K ( I ) = 1 − λ. Formulas in K inc( K ) are safe from inconsistency.
A.2. Penalty logic
Penalty logic [27,28] is similar in spirit to possibilistic logic, but with an additive interpretation of the weights. A penalty
logic formula (φ, p ) consists of a classical formula φ and a penalty p ∈ [0, +∞]. In particular, weights are not restricted to
the unit interval anymore, but can be any non-negative real number or +∞. The intuition behind a penalty logic formula
(φ, p ) is that p is the cost that has to be paid for having φ violated. Given a penalty logic base K , we can naturally associate
penalties to classical interpretations I :

pen K ( I ) =

p
(φ, p )∈ K , I | φ

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1845

Let us write K ∗ for the set of classical formulas that appear in K , i.e. K ∗ = {φ | ∃ p . (φ, p ) ∈ K }. The penalty of a set of
formulas B ⊆ K ∗ can then be deﬁned as

pen K ( B ) =

p
(α , p )∈ K , α ∈
/B

Like possibilistic logic, penalty logic can naturally deal with inconsistency. In particular, a consistent subset B ⊆ K ∗ is called
preferred if its penalty pen K ( B ) is minimal among all consistent subsets of K ∗ . An inconsistency-tolerant entailment relation
|≈ can then be deﬁned as K |≈ φ iff B | φ for all preferred subsets of K ∗ . Note that in this setting, multiplying all penalties
by a strictly positive constant does not affect the treatment of inconsistency. Without any practical restriction, we may thus
assume that all penalties are taken from N ∪ {+∞} which has clear computational advantages.
Appendix B. Variable forgetting
In this appendix, we introduce an eﬃcient syntactic procedure that can be used, under some restrictions, to implement
the step of variable forgetting. In particular, for a source si , an atom a, × ∈ { W , X , Y , Z }, B ∈ Pref ( P ) and B ∈ Pref ( P ), let
w (a, si , ×; B ) and w (a, si ; B ) be deﬁned as follows:

min{l ∈ {0, . . . , k} | α(×l,s ,a) ∈ B } if α(×k,s ,a) ∈ B
i
i
otherwise
k+1

w (a, si , ×; B ) =
w a, s i ; B

=

min{l ∈ {0, . . . , k} | α(l,si ,a) ∈ B } if α(k,si ,a) ∈ B
k+1

otherwise

Now, let us assume that all knowledge bases are in conjunctive-normal form. For any given B ∈ Pref ( P ), we then deﬁne the
B-weakened version K iB of knowledge base K i as the propositional knowledge base which is obtained from K i by replacing
w (a,s , W ; B )

w (a,s ,Y ; B )

i
i
Wa
and all occurrences of negative literals ¬a by {¬ y | y ∈ Y a
},
all occurrences of positive literals a by
B
k+1
k+1
where we deﬁne W a = { } and Y a = {⊥}. Similarly, for B ∈ Pref ( P ), we deﬁne the B -weakened version K i as the

propositional knowledge base which is obtained from K i by replacing all occurrences of positive literals a by
w (a,si ; B )
Ya
}.

w (a,si ; B )

Wa

and all occurrences of negative literals ¬a by {¬ y | y ∈
It is easy to see that the operations of B-weakening
and B -weakening indeed weaken a knowledge base K i , in the sense that its set of models is increased. Intuitively, it is thus
clear that by suﬃciently weakening all knowledge bases in this way, consistency can be restored. In some cases, the merging
operators f ( K 1 , . . . , K n ) and f ( K 1 , . . . , K n ), in fact, implement this idea, as made explicit in the following proposition.
Proposition 10. Assume that for each a ∈ A and l ∈ {0, . . . , k}, it holds that X al = {a,
each knowledge base K i is equal to its set of prime implicates. It holds that
f (K 1, . . . , Kn )

} and Z al = {a, ⊥}. Furthermore, assume that

K 1B ∪ · · · ∪ K nB ∪ C

≡

(B.1)

B ∈Pref ( P )
f (K 1, . . . , Kn )

K 1B ∪ · · · ∪ K nB ∪ C

≡

(B.2)

B ∈Pref ( P )

Recall that a clause γ is an implicate of propositional knowledge base Φ iff Φ | γ , and a prime implicate if furthermore
for every other implicate γ , if γ | γ then also γ | γ . Moreover, we additionally require that no literals are repeated
in prime implicates (e.g. if a ∨ b is a prime implicate of Φ then a ∨ a ∨ b is not considered as prime implicate). Every
propositional knowledge base can then be represented by its set of prime implicates. Also note that the condition that
X al = {a, } and Z al = {a, ⊥} is, in fact, satisﬁed in two of the four scenarios considered in Section 4: liberalization and
restriction. Indeed, although Table 1 e.g. suggest rather X al = {a} in the case of liberalization, we may equivalently take
X al = {a, } since Y al = {a} and a → a@si is equivalent to ¬a@si → ¬a.
Interestingly, the technique we have proposed in [70] to weaken propositional knowledge bases corresponds to a special
case of Proposition 10, thus revealing that the approach we take in this paper is compatible with the approach from [70],
although it is more general.
Note that the condition from Proposition 10 that each knowledge base K i should be equal to its set of prime implicates
is not redundant, as illustrated by the following example.
Example 17. Let C = ∅, K 1 = {a ∨ b, ¬b}, K 2 = {¬a}, where W a1 = Y a1 = {a, c } and W b1 = Y b1 = {b, c } and the sets X a1 , Z a1 ,

X b1 , Z b1 , W c1 , X c1 , Y c1 and Z c1 are trivial. Clearly, K 1 is not equal to its set of prime implicates, which is given by {a, ¬b}. Let
pref ( P ) = { B }, where
@s1

B = K1

∪ K 2@s2 ∪ {α(1,s1 ,a) , α(1,s1 ,b) , α(1,s1 ,c ) , α(0,s1 ,c ) , α(1,s2 ,a) , α(1,s2 ,b) , α(1,s2 ,c ) , α(0,s2 ,a) , α(0,s2 ,b) , α(0,s2 ,c ) }

1846

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

Then we have

K 1B = {a ∨ b ∨ c , ¬b ∨ ¬c }
K 2B = {¬a}
The right-hand side of (B.2) is then equivalent to ¬a ∧ ((b ∧ ¬c ) ∨ (¬b ∧ c )). To ﬁnd the left-hand side of (B.2), which
corresponds to forgetting the variables from A @s1 ∪ A @s2 in B, we may write K 1 as {a} and apply Proposition 10 to obtain
f (K1, K2)

≡ (a ∨ c ) ∧ ¬a ≡ c ∧ ¬a

Clearly we have ¬a ∧ ((b ∧ ¬c ) ∨ (¬b ∧ c )) ≡ c ∧ ¬a.
Note that due to the restriction to sets of prime implicates, the result of the merging operation does not depend on the
syntactic structure of the knowledge bases. Finally note that when the families W al and Y al are trivial, instead of X al and Z al ,
a counterpart to Proposition 10 can be obtained, which is based on knowledge bases in disjunctive-normal form. Because
W al and Y al are not simultaneously trivial in any of the four scenarios discussed in Section 4, we omit the details.
Appendix C. Proofs
C.1. Proof of Proposition 1
1. We ﬁrst show (21). Assume that a ∈ ( J \ I W l ); we ﬁnd a ∈
/ I W l and hence for every w ∈ I , (a, w ) ∈
/ W l . Therefore,
l
l
@s i
l
if w ∈ W a we have w ∈
/ I , which means I |
{ w | w ∈ W a } and also ( I ∪ J ) |
{ w | w ∈ W a }. From a ∈ J on the
other hand we ﬁnd J @si | a@si and I ∪ J @si | a@si . Together this means that I ∪ J @si | (a@si → { w | w ∈ W al }).
{ w | w ∈ W al },
Conversely, assume I ∪ J @si | (a@si → { w | w ∈ W al }). This implies that I ∪ J @si | a@si and I ∪ J @si |
@s i
@si
l
@s i
@s i
or equivalently J | a
and I |
{ w | w ∈ W a }. From J | a we immediately derive a ∈ J . From I |
{w | w ∈
W al } we ﬁnd that there is no w ∈ I such that (a, w ) ∈ W l , and thus a ∈
/ I W l . We conclude that a ∈ ( J \ I W l ).
2. In entirely the same way, we can show (22).
/ coI Y l and
3. To show (23), ﬁrst note that ([ I ]Y l \ J ) = co coI Y l ∩ co J = co J \ coI Y l . If a ∈ (co J \ coI Y l ), we ﬁnd a ∈
hence for every y ∈ coI , (a, y ) ∈
/ Y l . Therefore, if y ∈ Y al we have y ∈ I , which means I |
{¬ y | y ∈ Y al } and also
( I ∪ J @si ) |
{¬ y | y ∈ Y al }. From a ∈ co J on the other hand we ﬁnd J @si | ¬a@si and I ∪ J @si | ¬a@si . Together this
means that I ∪ J @si | (¬a@si → {¬ y | y ∈ Y al }).
Conversely, assume I ∪ J @si | (¬a@si → {¬ y | y ∈ Y al }). This implies that I ∪ J @si | ¬a@si and I ∪ J @si |
{¬ y | y ∈
Y al }, or equivalently J @si | ¬a@si and I |
{¬ y | y ∈ Y al }. From J @si | ¬a@si we immediately derive a ∈ co J . From
I |
{¬ y | y ∈ Y al } we ﬁnd that there is no y ∈ coI such that (a, y ) ∈ Y l , and thus a ∈
/ coI Y l . We conclude that
a ∈ (co J \ coI Y l ).
4. Finally, (24) is shown in the same way as (23).
C.2. Proof of Lemma 1

(⇒) Assume that (26) holds for all a ∈ A. First assume that a@si is true. Then we clearly have that
S is true for some S
S la is the case. We conclude a@si →
S la using the deduction theorem.
satisfying {a} ⊆ S ⊆ S la , which means that
Next assume that a is true. This means that
S will be true for every S satisfying {a} ⊆ S ⊆ S la and consequently that
@s i
@s i
a
will be true. Hence we have shown a → a .
(⇐) Assume that the implications in (27) hold for all a ∈ A. First assume that a@si holds. Then we know from a@si → S la
that some b ∈ S la is satisﬁed, and hence also (26). Conversely, if a@si is false it is suﬃcient to show that
S is false for
some S satisfying {a} ⊆ S ⊆ S la . In particular for S = {a} we immediately have
S false from ¬a@si → ¬a.
C.3. Proof of Lemma 2

(⇒) Assume that (28) holds for all a ∈ A. First assume that a@si is false. Then we clearly have that
S is false for some
{¬s | s ∈ S la }. We
S satisfying {a} ⊆ S ⊆ S la , which means that for some s ∈ S la , ¬s is the case, or in other words
conclude ¬a@si → {¬s | s ∈ S la } using the deduction theorem. Next assume that a is false. This means that
S will
be false for every S satisfying {a} ⊆ S ⊆ S la and consequently that a@si will be false. Hence we obtain ¬a → ¬a@si .
(⇐) Assume that the implications in (29) hold for all a ∈ A. First assume that a@si holds. Then we know from ¬a → ¬a@si ,
S holds, and thus a@si ≡
S.
which is equivalent to a@si → a, that a is the case. This means that for S = {a},
@s i
l
is false, we know from (29) that there is an s ∈ S a such that s is false. This means that
S will be
Conversely, if a
S.
false for some S satisfying {a} ⊆ S ⊆ S la , and thus a@si ≡

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1847

C.4. Proof of Proposition 2
We show that

I∈

q

poss

②
( K 1 , . . . , K n ; C ; M s1 , . . . , M sn )

(C.1)

is equivalent to

I∈

q

morph

②
( K 1 , . . . , K n ; C ; B ( W r , X r ,Y r , Z r ) )

(C.2)

Clearly, by deﬁnition of variable forgetting, (C.1) is equivalent to

∃ J 1 , . . . , J n ∈ 2 A . I ∪ J 1@s1 ∪ · · · ∪ J n@sn ∈ P inc( P )
or, by deﬁnition of r

∃ J 1 , . . . , J n ∈ 2 A . I ∪ J 1@s1 ∪ · · · ∪ J n@sn ∈ P λr
Applying the deﬁnition of P , this corresponds to
@s i

②
q
q @s ②
@s
∈ ( M si )λr ∧ J i i ∈ K i i

@s i

②
q
∈ ( M si )λr ∧ J i ∈ ❏ K i ❑

∃ J 1 , . . . , J n ∈ 2 A . I ∈ ❏ C ❑ ∧ ∀i . I ∪ J i
which is equivalent to

I ∈ ❏ C ❑ ∧ ∃ J 1 , . . . , J n ∈ 2 A . ∀i . I ∪ J i

From Proposition 1, we know that this is equivalent to

I ∈ ❏ C ❑ ∧ ∃ J 1 , . . . , J n ∈ 2 A . ∀i . J i ⊆ I

Wl

∧ I ⊆ Ji

Xr

∧ [ I ]Y r ⊆ J i ∧ [ J i ] Z r ⊆ I ∧ J i ∈ ❏ K i ❑

or in other words

I ∈ ❏C ❑ ∧ ∃ J 1 , . . . , J n ∈ 2 A . ∀i . σ(Y r , W r ) ( I , J i ) ∧ σ( Z r , X r ) ( J i , I ) ∧ J i ∈ ❏ K i ❑
By deﬁnition of B ( W r , X r ,Y r , Z r ) we ﬁnd

I ∈ ❏ C ❑ ∧ ∃ J 1 , . . . , J n ∈ 2 A . ∀i . J i ∈ B ( W r , X r , Y r , Z r ) ( I ) ∧ J i ∈ ❏ K i ❑
or, equivalently

I ∈ ❏ C ❑ ∧ ∀i . B ( W r , X r , Y r , Z r ) ( I ) ∩ ❏ K i ❑ = ∅
which corresponds to (C.2).
C.5. Proof of Proposition 3
As an example, we show (41); the proof of (42) is entirely analogous.
Note that I ∈ ❏ prior⊆ ( K 1 , . . . , K n )❑ is equivalent to
@s i

∃ J 1, . . . , Jn ∈ 2 A . I ∪

Ji

|

Pref ⊆ ( P )

i

which means that for some B ∈ Pref ⊆ ( P )
@s i

∃ J 1, . . . , Jn ∈ 2 A . I ∪

Ji

| B

i

Let us write B l for the formulas from B that appear in M si for some i with weight λl . The fact that B is a preferred
subtheory means that I | C (since the integrity constraints C are assumed to be consistent), J i | K i (since each knowledge
base is assumed to be individually consistent), and furthermore, that there can be no r ∈ {0, . . . , k} and I , J 1 , . . . , J n ∈ 2 A
such that I | C , J i | K i and such that B r ⊂ B r while B l = B l for all l > r, where we write B l for the set of formulas which

appear in P with weight λl and that are satisﬁed by I ∪
for all i ∈ {1, . . . , n} the following four inclusions hold:

Ji \ I

Wl

⊆ Ji \ I

[ I ]Y l \ J i ⊆ I

Yl

Wl

\ Ji,

,

I \ Ji

Xl

⊆ I \ Ji

[ J i ]Zl \ I ⊆ J i

Zl

Xl

\I

i

J

@s i

i

. Using Proposition 1, B l ⊆ B l is equivalent to asserting that

1848

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

In other words, for all i

diff aW l ( I , J i ) ⊆ diff aW l I , J i ,

diff bXl ( I , J i ) ⊆ diff bXl I , J i

diff cY l ( I , J i ) ⊆ diff cY l I , J i ,

diff dZ l ( I , J i ) ⊆ diff dZ l I , J i

or, equivalently

∀i ∈ {1, . . . , n} . diff 1Sl ( I , J i ) ⊆4 diff 1Sl I , J i
which means

diff 1S ( I , J 1 ), . . . , diff 1S ( I , J n )

diff 1S I , J 1 , . . . , diff 1S I , J n

1
l

Thus, we obtain

∃c ∈ diff 1S ( I , K) . ¬∃r . ∃ I ∈ 2 A . ∃c ∈ diff 1S I , K . ∀l > r . c =l1 c ∧ c <r1 c
where we write c =l1 c for c

1
l

c ∧c

1
l

c, and we write c <r1 c for c

∃c ∈ diff 1S ( I , K) . ∀ I ∈ 2 A . ∀c ∈ diff 1S I , K . c, c ∈ lex

1
r

1
k,...,

c∧c
1
0

1
r

c . But this is nothing else than saying
1
k,...,

∨ c ,c ∈
/ lex

1
0

and thus, since I moreover satisﬁes the integrity constraints C , we have

I∈

q

conﬂ2

K 1 , . . . , K n ; C ; diff 1S , lex

1
k,...,

1
0

②

C.6. Proof of Proposition 4
As an example, we show (46); the proof of (47) is entirely analogous.
Note that I ∈ ❏ priorcard ( K 1 , . . . , K n )❑ is equivalent to
@s i

∃ J 1, . . . , Jn ∈ 2 A . I ∪

Ji

|

Pref card ( P )

i

which means that for some B ∈ Pref card ( P )
@s i

∃ J 1, . . . , Jn ∈ 2 A . I ∪

Ji

| B

i

Let us write B l for the formulas from B that appear in M si for some i with weight λl . The fact that B is a cardinalitybased preferred subtheory means that I | C (since the integrity constraints C are assumed to be consistent), J i | K i (since
each knowledge base is assumed to be individually consistent), and furthermore, that there can be no r ∈ {0, . . . , k} and
I , J 1 , . . . , J n ∈ 2 A such that I | C , J i | K i and such that | B r | < | B r | while | B l | = | B l | for all l > r, where we write B l for

the set of formulas which appear in P with weight λl and that are satisﬁed by I ∪
that:

| Bl | =

Ji \ I

Wl

+ I \ Ji

Xl

+ [ I ]Y l \ J i + [ J i ] Z l \ I

Ji \ I

Wl

+ I \ Ji

Xl

+ I

i

J

@s i

i

. Using Proposition 1, we ﬁnd

i

Bl =

Yl

\ Ji +

Ji

Zl

\I

i

In other words,

| B l | = dl1 ( I , K),

| B l | = dl1 I , K

From which we immediately ﬁnd that I being a model of a cardinality-based preferred subtheory is equivalent to the fact
that there cannot be an interpretation I such that I 1 I while I 1 I , or in other words, I ∈ ❏ dist ( K 1 , . . . , K n ; C ; 1 )❑.
C.7. Proof of Proposition 5
As an example, we show (52)–(54). The proof of (49)–(51) is analogous.
First, note that I ∈ ❏ conﬂ2 ( K 1 , . . . , K n ; C ; diff 2S , par( k2 , . . . , 20 ))❑ means that I ∈ ❏C ❑ and that there is a c 1 , . . . , cn ∈

diff 2S ( I , K) such that for every I ∈ ❏C ❑, we have either for every i ∈ {0, . . . , n} and l ∈ {0, . . . , k}

c i ⊆ diff 2Sl I , K i

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1849

or for at least one i ∈ {0, . . . , n} and l ∈ {0, . . . , k}

diff 2Sl I , K i

ci

Furthermore, by construction, each c i corresponds to a model J i of K i . Thus, we have that there exist models J 1 , . . . , J n of
K 1 , . . . , K n such that for all models J 1 , . . . , J n of K 1 , . . . , K n , it holds that either for every i ∈ {0, . . . , n} and l ∈ {0, . . . , k}

diff aW l ( I , J i ) ∪ diff bX l ( I , J i ) ∪ diff cY l ( I , J i ) ∪ diff dZ l ( I , J i )

⊆ diff aW l I , J i ∪ diff bX l I , J i ∪ diff cY l I , J i ∪ diff dZ l I , J i
or for some i ∈ {0, . . . , n} and l ∈ {0, . . . , k}

diff aW l I , J i ∪ diff bX l I , J i ∪ diff cY l I , J i ∪ diff dZ l I , J i
diff aW l ( I , J i ) ∪ diff bX l ( I , J i ) ∪ diff cY l ( I , J i ) ∪ diff dZ l ( I , J i )
Using Proposition 1, we ﬁnd that this is equivalent to stating that the set B ( I , J 1 , . . . , J n ) of formulas from M s1 ∪ · · · ∪ M sn
@s
@s
that are satisﬁed by I ∪ J 1 1 ∪ · · · ∪ J n n is not properly included in the set B ( I , J 1 , . . . , J n ) of formulas from M s1 ∪ · · · ∪ M sn
@s1
that are satisﬁed by I ∪ J 1
∪ · · · ∪ J n @sn .
We now show (52), (53) and (54).
1. The fact that B ( I , J 1 , . . . , J n ) ⊂ B ( I , J 1 , . . . , J n ) means that

B(I , J 1, . . . , Jn) ⊇ B I , J 1, . . . , Jn

or

B(I , J 1, . . . , Jn)

B I , J 1, . . . , Jn

Now due to the structure of the possibilistic knowledge bases M si and the choice of the ordering on Λ, for (α , λ(l,s,a) )
and (α , λ(l ,s ,a ) ) in M s1 ∪ · · · ∪ M sn , it holds that

λ(l,s,a)

λ(l ,s ,a ) implies α → α

(C.3)

In particular, this means that whenever a formula α is satisﬁed which appears with weight λ(l,s,a) in M s1 ∪
· · · ∪ M sn , all formulas with a higher weight are also satisﬁed. Therefore, B ( I , J 1 , . . . , J n ) ⊇ B ( I , J 1 , . . . , J n ) is the
same as B ( I , J 1 , . . . , J n )
B ( I , J 1 , . . . , J n ), and B ( I , J 1 , . . . , J n )
B ( I , J 1 , . . . , J n ) is the same as B ( I , J 1 , . . . , J n )
B ( I , J 1 , . . . , J n ). Since I ∈ ❏ poss ( K 1 , . . . , K n )❑ is equivalent to saying that I ∈ ❏C ❑ and there exist models J i ∈ ❏ K i ❑ such
that for all I ∈ ❏C ❑ and all models J i ∈ ❏ K i ❑, B ( I , J 1 , . . . , J n ) B ( I , J 1 , . . . , J n ) or B ( I , J 1 , . . . , J n ) B ( I , J 1 , . . . , J n ),
the stated follows.
2. It is straightforward to construct a linearization κ such that the set B ( I , J 1 , . . . , J n ) is a preferred subtheory of κ ( P ).
Indeed, it suﬃces to rank the weights such that κ (λ(l,s,a) ) > κ (λ(l ,s ,a ) ) whenever the formula with weight λ(l,s,a)
appears in B ( I , J 1 , . . . , J n ) and the formula with weight λ(l ,s ,a ) does not. The fact that such a linearization can always
be obtained follows straightforwardly from (C.3). Conversely, it is furthermore clear that for every linearization κ , and
@s
@s
every model I ∪ J 1 1 ∪ · · · ∪ J n n of a preferred subtheory of κ ( P ), it holds that the set B ( I , J 1 , . . . , J n ) is a maximal
consistent subset of formulas from M s1 ∪ · · · ∪ M sn . In other words, there is a one-on-one correspondence between the
models of Pref ⊆ ( P ) and those of Pref ( P ).
3. By construction we already have Pref card ( P ) ⊆ Pref ⊆ ( P ). To show (54), it therefore suﬃces to show Pref ⊆ ( P ) ⊆
Pref card ( P ). Now let B ∈ Pref ⊆ ( P ) and let B l be the formulas from B that appear in κ ( P ) with weight l, and let B l
the formulas outside B that appear in κ ( P ) with weight l. Let the different weights appearing in κ ( P ) be l1 < · · · < lr .
Without lack of generality, we may assume that l1 > 0, since only the relative ordering of these weights is important.
We now construct a new linearization κ as follows:

κ (λ(l,s,a) ) =

li

if the unique formula from P with weight λ(l,s,a) is in B

li +li −1
2

otherwise

where li = κ (λ(l,s,a) ) and l0 = 0 < l1 . By construction, B is still a preferred subtheory of κ ( P ). Moreover, for each
priority level l in κ ( P ) it holds that either all formulas belong to B or none of these formulas belong to B. As a
consequence, B is also a cardinality-based preferred subtheory of κ ( P ).
C.8. Proof of Proposition 6
As an example, we show (59), the proof of (60) being analogous. If I ∈ ❏ pen ( K 1 , . . . , K n )❑, we have I ∈ ❏C ❑ and for
some models J i ∈ ❏ K i ❑ we have B ( I , J 1 , . . . , J n ) ∈ Pref pen ( Q ), with B ( I , J 1 , . . . , J n ) the formulas from Q that are satisﬁed
@s

@s

by I ∪ J 1 1 ∪ · · · ∪ J n n . By construction, all formulas that appear in Q with weight +∞ are contained in B ( I , J 1 , . . . , J n )
(which is the case exactly when I ∈ ❏C ❑ and J i ∈ ❏ K i ❑ for all i). Let us write B l ( I , J 1 , . . . , J n ) for the set of formulas in
B ( I , J 1 , . . . , J n ) that have weight λl in Q . Now suppose that there was an interpretation I ∈ ❏C ❑ such that for some models
J i ∈ ❏ K i ❑, | B k ( I , J 1 , . . . , J n )| > | B k ( I , J 1 , . . . , J n )|. Then we have

1850

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

p (α , p ) ∈ Q ,

α ∈/ B k ( I , J 1 , . . . , J n ) = γk rk + γk−1 rk−1 + · · · + γ0

p (α , p ) ∈ Q , α ∈
/ Bk I , J 1, . . . , J n

= γk r k + γk−1 r k−1 + · · · + γ0

where

γl = B l ( I , J 1 , . . . , J n ) ,

γl = B l I , J 1 , . . . , J n

{ p | (α , p ) ∈ Q , α ∈
/
k−1
B k ( I , J 1 , . . . , J n )} > { p | (α , p ) ∈ Q , α ∈
/ B k ( I , J 1 , . . . , J n )} as soon as r is suﬃciently large. Indeed, if γk−1 r
+ · · · + γ0
γk−1 rk−1 + · · · + γ0 , this is trivial. On the other hand, if γk−1 rk−1 + · · · + γ0 < γk−1 rk−1 + · · · + γ0 , it suﬃces to choose
From | B k ( I , J 1 , . . . , J n )| > | B k ( I , J 1 , . . . , J n )| we thus know that

r>

k

γk > γk , which implies that

(γk−1 r k−1 + · · · + γ0 ) − (γk−1 r k−1 + · · · + γ0 )

γk − γk

We may proceed in entirely the same fashion when | B l ( I , J 1 , . . . , J n )| = | B l ( I , J 1 , . . . , J n )| for all l > l0 and | B l0 ( I , J 1 , . . . ,
J n )| > | B l0 ( I , J 1 , . . . , J n )|. Thus, provided that r is suﬃciently large, B ( I , J 1 , . . . , J n ) is minimal w.r.t. p iff B ( I , J 1 , . . . , J n ) ∈
priorcard
, from which the stated immediately follows.
C.9. Proof of Proposition 7
As an example, we show (61); (62) is shown entirely analogously.
The fact that I ∈ ❏ pen ( K 1 , . . . , K n )❑ means that I ∈ ❏C ❑ and there exist models J i ∈ ❏ K i ❑ such that the subset
@s
@s
B ( I , J 1 , . . . , J n ) of formulas from Q that are satisﬁed by I ∪ J 1 1 ∪ · · · ∪ J n n is preferred, i.e. for any other I ∈ ❏C ❑ and
J i ∈ ❏ K i ❑, it holds that B ( I , J 1 , . . . , J n ) p B ( I , J 1 , . . . , J n ). From Proposition 1, we know that (α(W
, p) ∈
/ B(I , J 1, . . . , Jn)
l,s ,a)
is equivalent to a ∈ J i \ I

i

and similar for formulas of the form (α(Xl,s ,a) , p ), (α(Yl,s ,a) , p ) and (α(Zl,s ,a) , p ), and for
i
i
i
B ( I , J 1 , . . . , J n ). Thus, we immediately have

p (α , p ) ∈ Q ,
and consequently, that I
B(I , J 1, . . . , Jn)

p

Wl ,

α ∈/ B ( I , J 1 , . . . , J n ) = d1Q ( I , J 1 ) + · · · + dnQ ( I , J 1 )
d1Q

I iff there exist models J i ∈ ❏ K i ❑ such that for all models I ∈ ❏C ❑ and J i ∈ ❏ K i ❑ it holds that

B ( I , J 1 , . . . , J n ). In other words, we have that I ∈ ❏

dist

(K 1, . . . , Kn ; C ;

d1Q

)❑ iff I ∈ ❏

pen

( K 1 , . . . , K n )❑.

C.10. Proof of Proposition 8
1. Let us ﬁrst consider the case of poss with linear weights. Membership in Θ2P follows straightforwardly from the fact
that deciding whether K inc( K ) | φ for a possibilistic knowledge base K and propositional formula φ is in Θ2P (see [71]).
We show Θ2P -hardness by reduction from PARITY(SAT) [47,72]. Given n instances S 1 , . . . , S n of the boolean satisﬁability problem SAT, the problem PARITY(SAT) consists of deciding whether the number of satisﬁable instances among
S 1 , . . . , S n is odd. This problem is Θ2P -hard, even under the assumption that the instances are such that whenever S i is
unsatisﬁable, S i +1 , . . . , S n are unsatisﬁable as well. We will now show that this problem, under the latter assumption,
can be reduced to the problem of deciding poss ( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ in polynomial time.
Let us choose K i = S i ∪ {ai } for all i ∈ {1, . . . , n}, where a1 , . . . , an are atoms which do not occur in any of the instances
S 1 , . . . , S n . Furthermore, we choose C = ∅, and the weighted knowledge bases M si such that

×∈{ W , X ,Y , Z }

α(×l,si ,a) ≡

if l > k − i + 1
(a@si ≡ a) otherwise

The weights are chosen such that λ×
(l,si ,a) = λl for some λl ∈ [0, 1]. Clearly, P λk is satisﬁable iff S 1 is satisﬁable, P λk−1
is satisﬁable iff S 1 ∪ S 2 is satisﬁable, which, due to the assumption on the instances S i , is equivalent to the condition
that S 2 is satisﬁable. In general, we ﬁnd that P λk−i is satisﬁable iff S i +1 is satisﬁable. By taking φ = (a1 ∧ ¬a2 ) ∨ (a3 ∧
¬a4 ) ∨ · · · we have that poss ( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ iff the number of λs in {λ1 , . . . , λk } for which P λ is
satisﬁable is odd, which is in turn equivalent to the fact that an odd number among S 1 , . . . , S k are satisﬁable.
2. Next, we consider poss with partially ordered weights. To prove membership in Π2P , we provide a Σ2P procedure for
checking
poss

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ

• Guess a subset K of P ∗ ;
• Verify that K is consistent using one call to the NP oracle;

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1851

• Verify that K | φ using one call to the NP oracle;
• Verify that K is minimal w.r.t. . First, construct in polynomial time a minimal subset K of K such that K
K and
K
K , by removing from K all formulas (α , λ) such that for some (α , λ ) ∈ ( P ∗ \ K ), it holds that λ λ . It is clear
that removing any formula from K will then lead to a subset which is strictly less preferred than K or K . To verify
that K is minimal, it now suﬃces to consider all weights λ such that K contains no formula with weight λ, and, in
each case, check whether adding all formulas (α , λ ) with λ
λ to K leads to an inconsistent subset. Clearly, the
number of required satisﬁability checks is at most polynomial in the size of P ∗ .
To show that entailment checking for poss with partially ordered weights is Π2P -hard, we provide a reduction from
quantiﬁed boolean formula problems of the form ∀a . ∃b . F (a, b).
Deﬁne K 1 = {a1 , . . . , am , c }, K 2 = {¬a1 , . . . , ¬am }, C = { F (a, b) ≡ c }, k = 1, W a1i = X a1i = {ai , } and Y a1i = Z a1i = {ai , ⊥}

and W c1 = X c1 = {c , } and Y c1 = Z c1 = {c , ⊥}. Let P be deﬁned in terms of K 1 , K 2 , C , M s1 and M s2 as before, and let
the ordering on Λ be deﬁned as in Proposition 5. It is clear that for any -preferred subset B, and for every i, we
either have B | ai or B | ¬ai . Indeed, if neither B | ai nor B | ¬ai we could construct a new consistent subset B =
B ∪{α(W
B but not B B . Furthermore, it is clear that for any subset A 0 ⊆
0,s1 ,ai ) }, for which it holds that B | ai and B
{a1 , . . . , am } there exists a consistent subset B of P ∗ such that B | ai if ai ∈ A 0 and B | ¬ai otherwise. We furthermore
have B | c for such a minimal subset B iff α(W
0,s ,c ) ∈ B in which case we also have B | F (a1 , . . . , am , b 1 , . . . , br ). Due
1

to the minimality of B w.r.t. , α(W
0,s1 ,c ) ∈ B will hold as soon as there exists a truth valuation for the atoms b 1 , . . . , br
which makes F (a1 , . . . , am , b1 , . . . , br ) true, given that the atoms in A 0 are true and those in {a1 , . . . , am } \ A 0 are not.
Thus, we can conclude that poss ( K 1 , K 2 ; C ; M s1 , M s2 ) | F (a1 , . . . , am , b1 , . . . , br ) iff B | F (a1 , . . . , am , b1 , . . . , br ) for
every -preferred B, which is equivalent to ∀a1 , . . . , am . ∃b1 , . . . , br . F (a1 , . . . , am , b1 , . . . , br ).
Note that using Proposition 5, it follows from this result that also the following decision problems are Π2P -hard when
the weights are partially ordered:
prior⊆

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ

priorcard

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ

(C.4)
(C.5)

3. For the operator prior⊆ with linear weights, membership in Π2P follows from the Π2P -completeness of the problem
UNI-INCL considered in [73]. The Π2P -hardness is shown in entirely the same way as the Π2P -hardness of entailment
checking for poss with partially ordered weights.
4. Entailment checking for prior⊆ with partially ordered weights is Π2P -hard, which follows by restricting to the special
case where all weights are totally ordered. Membership in Π2P is proven by showing that the complement problem is
in Σ2P . Indeed, to decide whether
prior⊆

( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ

we may use the following Σ2P procedure (inspired by the Π2P -hardness proof of PBR revision [74]):
• Guess a linearization κ ;
• Guess a subset K of κ ( P 0 );
• Verify that K is consistent by calling the NP oracle;
• Verify that K | φ by calling the NP oracle;
• Verify that K is preferred w.r.t. inclusion. For this step, let λ1 < · · · < λr ∈ [0, 1] be the weights that appear in κ ( P ):
– For each formula φ in P λr \ K , check that ( K ∩ P λr ) ∪ {φ} is inconsistent;
– For each formula φ in P λr −1 \ ( K ∪ P λr ), check that ( K ∩ P λr −1 ) ∪ {φ} is inconsistent;
– ...
– For each formula φ in P λ1 \ ( K ∪ P λ2 ), check that K ∪ {φ} is inconsistent;
Clearly, the number of calls to the NP oracle is at most linear in the number of formulas in P .
5. Membership in 2P of entailment checking for priorcard with linearly ordered weights follows from the 2P -completeness
of the UNI-LEX problem considered in [73]. We show hardness by reduction from the problem ALM, following an analogous approach as the proof of 2P -hardness in [73]. Given a satisﬁable set of clauses Ψ over the variables a1 , . . . , an ,
I iff I = I or there is an i such that I ∩ {a1 , . . . , ai } = I ∩ {a1 , . . . , ai },
models of Ψ can be ordered as follows: I
ai +1 ∈
/ I and ai +1 ∈ I . ALM is then the problem of deciding whether the model I of Ψ that is maximal w.r.t. this
ordering makes an true.
In particular, let C = Ψ , K 1 = {a1 , . . . , an }, and k = n − 1. We furthermore deﬁne W al i = X al i = Y al i = Z al i = {ai } for i n − l
and W al i = X al i = {ai ,

} and Y al i = Z al i = {ai , ⊥} otherwise. Then there is only one preferred subset of P according to
which it will ﬁrst be tried to satisfy a1 , then a2 , etc. Thus we ﬁnd that priorcard ( K 1 ; C ; M s1 ) | an iff the maximal model
of Ψ in the sense described above is such that an is true.
6. For partially ordered weights, the fact that entailment checking for priorcard is Π2P -hard was already established in (C.5).
Membership in Π2P follows analogously as for prior⊆ , using the fact that the MAX-GSAT-ARRAY problem considered
in [73] is NP-complete.

1852

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

7. Finally, let us consider the problem of checking entailment for

pen

. Let m =
@s

(α , p )∈ Q , p <+∞ p. By construction, the

only formulas in Q with weight +∞ are those in the knowledge bases K i i and C , which together are consistent by
assumption. This means that the optimal consistent subsets of Q ∗ will have a penalty which is at most m. Checking
whether there exists a consistent subset B of Q ∗ such that pen( B ) p for a given p is clearly in NP. Using a number of
calls to an NP-oracle which is logarithmic in m, we can thus ﬁnd the smallest value p ∗ such that a consistent subset B
of Q ∗ exists with pen( B ) = p ∗ . To decide pen ( K 1 , . . . , K n ; C ; M s1 , . . . , M sn ) | φ it then suﬃces to verify whether B | φ
for all consistent subsets of Q ∗ for which pen( B ) = p ∗ . This can be done in coNP; indeed, the complement can be
shown by guessing an interpretation I , verifying that pen( I ) = p ∗ and that I | φ . This means that overall, we have a
procedure which takes polynomial time on a deterministic machine, and makes a number of calls to an NP oracle which
is logarithmic in m. Hence, if the penalties are bounded by an exponential function of the problem size, then log(m) is
polynomial in the problem size, yielding membership in 2P , and if the penalties are bounded by a polynomial function,
then log(m) is logarithmic in the problem size, yielding membership in Θ2P .
In the case of exponentially bounded penalties, hardness follows from Proposition 6 and the fact that entailment checking for priorcard with linear weights was shown to be 2P -complete. The Θ2P -hardness is the case of polynomially
bounded penalties follows from the Θ2P -completeness of distance-based merging with the Hamming distance and the
sum as aggregation operator [9]. Indeed, by restricting to the case k = 0, we know from Proposition 7 that pen degenerates to standard Hamming-distance based merging.
C.11. Proof of Proposition 9
1. Membership of |≈∃⊆ and |≈∃card in Σ2P follows from the Σ2P -completeness of the EXI-INCL and EXI-LEX problems studied
in [73]. To show hardness, we give a reduction from quantiﬁed boolean formula problems of the form ∃a . ∀b . F (a, b).
1
1
In particular let K 1 = {a1 , . . . , am }, K 2 = {¬a1 , . . . , ¬am }, C = ∅, λ×
(l,s,a) = λl ∈ ]0, 1[, k = 1, and W ai = X ai = {ai , } and
Y a1i = Z a1i = {ai , ⊥}. Then it is easy to see that there is a preferred subtheory B such that B | F (a, b) iff the QBF
∃a . ∀b . F (a, b) is satisﬁed. Moreover, all preferred subtheories will satisfy exactly half of the formulas that appear in
M s1 and M s2 with weight λ0 , and all formulas that appear with weight λ1 (as the latter are all trivial). As a consequence
the hardness results holds both for |≈∃⊆ and |≈∃card .

∃∃
∃
∃
2. Σ2P membership of |≈∃∃
⊆ and |≈card is shown in the same way as for |≈⊆ and |≈card . Hardness follows immediately by
restriction to the linearly ordered case.
P
3. Membership of |≈∃∀
⊆ in Σ3 is straightforward: guess a linearization κ and verify whether the entailment holds for all

B ∈ Pref card (κ ( P )) using a Σ2P oracle. Hardness is shown by reduction of quantiﬁed boolean formula problems of the
form ∃a . ∀b . ∃c . H (a, b, c). Consider knowledge bases

K 1 = {a1 , . . . , am , b1 , . . . , br , z}
K 2 = {¬a1 , . . . , ¬am , ¬b1 , . . . , ¬br }
C = z ≡ H (a, b, c)
∗
} and Y x1 = Z x1 = {x, ⊥} for all atoms x. Furthermore, let us take λ×
(1,s,x) = λ for all atoms x,
×
−
×
×
= λi , λ(0,s2 ,ai ) = λi , λ(0,s j ,bi ) = μ and λ(0,s1 ,z) = μ, where the ordering on the weights is such that μ <
, λ+
, λ−
, λ+
are all incomparable for i = j. Then it is clear that any choice for
λi , λi < λ∗ for all i, and the weights λ−
i
i
j
j

and W x1 = X x1 = {x,

λ×
(0,s1 ,ai )
− +

+

the truth values of the atoms ai is enforced by some speciﬁc linearization, and thus that ( K 1 , . . . , K n ) |≈ H (a, b, c) iff
there is a choice of truth values for the atoms ai such that H (a, b, c) is consistent with every choice of truth values for
the atoms b i , or indeed iff ∃a . ∀b . ∃c . H (a, b, c) is satisﬁable.
4. Membership of |≈∃∀
follows from the following procedure: guess a linearization κ and verify that the entailment
card
holds for all B ∈ Pref card (κ ( P )). The complexity of this last step is
polynomial time using an NP-oracle. Hardness is shown by taking

K 1 = {a1 , . . . , am },

K 2 = {¬a1 , . . . , ¬am },

P
2

(see Proposition 8), hence it can be performed in

C =∅

×
∗
∗
and W a1i = X a1i = {a, } and Y a1i = Z a1i = {a, ⊥}. Furthermore, let λ×
(1,s,ai ) = λ and λ(0,s,ai ) = λi such that λi < λ and
λi is incomparable to λ j for i = j. Then every choice of truth values for the atoms ai is enforced by some speciﬁc
linearization, hence we clearly have that ∃a . ∀b . F (a, b) iff there is a linearization κ such that B | F (a, b) for every
B ∈ Pref card (κ ( P )).
P
5. Membership in Π3P for |≈∀∃
⊆ is easily seen, by showing that the complement problem is in Σ3 . Indeed, it suﬃces to

guess a linearization κ and then use a Σ2P oracle to verify wether for every preferred subtheory B of κ ( P ), it is the
case that B | φ . This is not the case if ∃ B ∈ Pref ⊆ (κ ( P )) . B | φ , and verifying this latter expression was already shown
is in Π3P .
to be in Σ2P (in the ﬁrst item). For the same reason, we have that |≈∀∃
card

To show Π3P -hardness we simulate QBFs of the form ∀a . ∃b . ∀c . H (a, b, c). The knowledge bases here are chosen as

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1853

K 1 = {a1 , . . . , am , b1 , . . . , br }
K 2 = {¬a1 , . . . , ¬am , ¬b1 , . . . , ¬br }
C =∅
where the weights and the families W xl , X xl , Y xl and Z xl are deﬁned as in the hardness proof of |≈∃∀
⊆ . Again we have
that every choice of the truth values of the atoms ai is enforced by some linearization κ . Given such a linearization κ ,
each preferred subtheory corresponds to a choice of the truth values of the atoms b i . Hence, we have ( K 1 , . . . , K n ) |≈∀∃
⊆
H (a, b, c) iff ∀a . ∃b . ∀c . H (a, b, c) holds. Since every preferred subtheory satisﬁes the same number of formulas at each
priority level, Π3P -hardness for |≈∀∃
follows as well.
card
C.12. Proof of Proposition 10
As an example, we show (B.1), the proof of (B.2) being entirely analogous. Since clearly forgetVar(Φ1 ∨ Φ2 , X ) ≡
forgetVar(Φ1 , X ) ∨ forgetVar(Φ2 , X ), it suﬃces to show that for every B ∈ Pref ( P ), we have

forgetVar B , A @s1 ∪ · · · ∪ A @sn ≡ K 1B ∪ · · · ∪ K nB ∪ C
Let us write B i for the formulas from M si that are contained in B. We then ﬁnd
@s1

forgetVar B , a@si ≡ K 1

@s

@s

∧ · · · ∧ K i −i1−1 ∧ K i +i1+1 ∧ · · · ∧ K n@sn ∧ B 1 ∧ · · · ∧ B i −1 ∧ B i +1 ∧ · · · ∧ B n
@s i

∧ C ∧ forgetVar K i

∧ B i , a@si

@s

@s

since only K i i and B i contain occurrences of a@si . Without lack of generality, we may assume that K i i is of the form
{αi ∨ a@si | 1 i s} ∪ {βi ∨ ¬a@si | 1 i t } ∪ Φ , where none of the formulas αi , βi or the formulas in Φ contain occurrences
of a. Moreover, note that the only non-trivial formulas in B i that contain occurrences of a@si are a@si → W ar and ¬a@si →
w (a, si , Y ; B ); let Ψ be the set of formulas from B i which do not refer to
{¬ y | y ∈ Y ar } for r w (a, si , W ; B ) and r
@s i
a . We ﬁnd
@si

forgetVar K i
@s i

∧ B i , a@s i

a@si :=

≡ Ki

@s i

∧ B i a@si :=

t

∨ Ki

a@si := ⊥ ∧ B i a@si := ⊥

k

≡

βi ∧

∨

Ψ

t

βi ∧

Ψ

∨

αi ∧

t

Ψ∧

βi ∧

w (a,si , W ; B )

Wa

¬ y y ∈ Ya

s

∧

Ψ

Φ∧

t

i =1 j =1
s

w (a,si , W ; B )

Wa

¬ y y ∈ Ya

i =1

s

(βi ∨ α j ) ∧

Ψ∧

w (a,si ,Y ; B )

αi ∧

∨

i =1
t

∧

w (a,si ,Y ; B )

Φ∧

i =1

Φ∧

≡

Ψ

r = w (a,si ,Y ; B )

s

∧

Wa

i =1

≡

¬ y y ∈ Y ar ∧

Φ∧

i =1

w (a,si , W ; B )

Φ∧

k

αi ∧

r = w (a,si , W ; B )

i =1

≡

s

W ar ∧

Φ∧

βi ∨

w (a,si ,Y ; B )

¬ y y ∈ Ya

i =1
w (a,si , W ; B )

∨ αi ∧

Wa

∨

w (a,si ,Y ; B )

¬ y y ∈ Ya

j =1
w (a,s , W ; B )

w (a,s ,Y ; B )

w (a,s , W ; B )

w (a,s ,Y ; B )

i
i
i
i
Note that
Wa
∨ {¬ y | y ∈ Y a
} is trivially satisﬁed since a ∈ W a
and a ∈ Y a
. Furthermore, for each i and j, we have that Φ contains a formula which is equivalent to βi ∨ α j , from the assumption that K i is
equal to its set of prime implicates. This leads to

t

Φ∧

Ψ∧

βi ∨

w (a,si ,Y ; B )

¬ y y ∈ Ya

i =1

s

∧

w (a,si , W ; B )

Wa

which corresponds exactly to replacing every occurrence of a positive literal a@si by
by {¬ y | y ∈
a negative literal ¬a
leading to the stated equivalence.
@s i

∨ αj

j =1

w (a,si ,Y ; B )
Ya
}.

w (a,si , W ; B )

Wa

and every occurrence of
@s1

This process can be repeated for all other atoms from A 1

∪ · · · ∪ An@sn ,

1854

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

References
[1] M. Abidi, R. Gonzalez, Data Fusion in Robotics and Machine Intelligence, Academic Press, 1992.
[2] I. Matzkevich, B. Abramson, The topological fusion of Bayes nets, in: Proceedings of the Eighth Conference on Uncertainty in Artiﬁcial Intelligence,
1992, pp. 191–198.
[3] L. Cholvy, A logical approach to multi-sources reasoning, in: International Conference on Knowledge Representation and Reasoning Under Uncertainty,
Logic at Work, 1994, pp. 183–196.
[4] S. Benferhat, D. Dubois, H. Prade, How to infer from inconsistent beliefs without revising, in: IJCAI’95: Proceedings of the 14th International Joint
Conference on Artiﬁcial Intelligence, 1995, pp. 1449–1455.
[5] J. Lin, Integration of weighted knowledge bases, Artiﬁcial Intelligence 83 (2) (1996) 363–378.
[6] L. Amgoud, S. Kaci, An argumentation framework for merging conﬂicting knowledge bases, International Journal of Approximate Reasoning 45 (2)
(2007) 321–340.
[7] R. Booth, Social contraction and belief negotiation, Information Fusion 7 (1) (2006) 19–34.
[8] P. Revesz, On the semantics of theory change: arbitration between old and new information, in: Proceedings of the 12th ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Database Systems, 1993, pp. 71–82.
[9] S. Konieczny, J. Lang, P. Marquis, DA2 merging operators, Artiﬁcial Intelligence 157 (1–2) (2004) 49–79.
[10] I. Bloch, J. Lang, Towards mathematical morpho-logics, in: Technologies for Constructing Intelligent Systems, Physica-Verlag GmbH, 2002, p. 380.
[11] D. Dubois, H. Prade, Possibility theory and data fusion in poorly informed environments, Control Engineering Practice 2 (1994) 811–823.
[12] D. Dubois, H. Fargier, H. Prade, Multiple-sources informations fusion – a practical inconsistency-tolerant approach, in: Proc. 8th International Conference
on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’00), Madrid, July 13–17, 2000, pp. 1047–1054.
[13] G. Priest, Paraconsistent logic, in: Handbook of Philosophical Logic, vol. 6, Kluwer Academic Publishers, 2002, pp. 287–393.
[14] W. Carnielli, M. Coniglio, J. Marcos, Logics of formal inconsistency, in: Handbook of Philosophical Logic, vol. 14, Springer, 2005, pp. 1–93.
[15] P. Gärdenfors, Knowledge in Flux, MIT Press, 1988.
[16] N. Rescher, R. Manor, On inference from inconsistent premisses, Theory and Decision 1 (1970) 179–217.
[17] S. Konieczny, R. Pino Pérez, Merging information under constraints: a logical framework, Journal of Logic and Computation 12 (5) (2002) 773–808.
[18] M. Dalal, Investigations into a theory of knowledge base revision: preliminary report, in: Proceedings of the 7th National Conference on Artiﬁcial
Intelligence, 1988, pp. 475–479.
[19] C. Lafage, J. Lang, Propositional distances and compact preference representation, European Journal of Operational Research 160 (3) (2005) 741–761.
[20] P. Everaere, S. Konieczny, P. Marquis, Conﬂict-based merging operators, in: Eleventh International Conference on Principles of Knowledge Representation
and Reasoning (KR’08), 2008, pp. 348–357.
[21] J. Serra, Image Analysis and Mathematical Morphology, Academic Press, 1982.
[22] N. Gorogiannis, A. Hunter, Merging ﬁrst-order knowledge using dilation operators, in: Proceedings of the 5th International Conference on Foundations
of information and knowledge systems, 2008, pp. 132–150.
[23] J. Delgrande, T. Schaub, A consistency-based framework for merging knowledge bases, Journal of Applied Logic 5 (3) (2007) 459–477.
[24] J. Lang, P. Marquis, Reasoning under inconsistency: A forgetting-based approach, Artiﬁcial Intelligence 174 (12–13) (2010) 799–823.
[25] D. Dubois, J. Lang, H. Prade, Possibilistic logic, in: D.N.D. Gabbay, J. Robinson, C. Hogger (Eds.), Handbook of Logic in Artiﬁcial Intelligence and Logic
Programming, vol. 3, Oxford University Press, 1994, pp. 439–513.
[26] P. Liberatore, Merging locally correct knowledge bases: A preliminary report, Tech. rep., Computing Research Repository (CoRR), arXiv:cs.AI/0212053,
2002.
[27] G. Pinkas, Propositional non-monotonic reasoning and inconsistency in symmetric neural networks, in: Proceedings of the 12th International Joint
Conference on Artiﬁcial Intelligence, vol. 1, 1991, pp. 525–530.
[28] F. Dupin de Saint-Cyr, J. Lang, T. Schiex, Penalty logic and its link with Dempster–Shafer theory, in: Proceedings of the 10th International Conference
on Uncertainty in Artiﬁcial Intelligence, 1994, pp. 204–211.
[29] Z. Pawlak, Rough sets, International Journal of Parallel Programming 11 (5) (1982) 341–356.
[30] P. Gärdenfors, Conceptual Spaces: The Geometry of Thought, MIT Press, 2000.
[31] C. Freksa, Conceptual neighborhood and its role in temporal and spatial reasoning, in: M. Singh, L. Travé-Massuyès (Eds.), Decision Support Systems
and Qualitative Reasoning, North-Holland, Amsterdam, 1991, pp. 181–187.
[32] C. Freksa, Temporal reasoning based on semi-intervals, Artiﬁcial Intelligence 54 (1–2) (1992) 199–227.
[33] D. Dubois, H. Prade, R. Yager, Merging fuzzy information, in: J. Bezdek, D. Dubois, H. Prade (Eds.), Fuzzy Sets in Approximate Reasoning and Information
Systems, in: The Handbooks of Fuzzy Sets Series, Kluwer, Boston, Mass., 1999, pp. 335–401.
[34] D. Gabbay, O. Rodrigues, G. Pigozzi (Eds.), Journal of Logic and Computation, special issue on Connections between Belief Revision, Belief Merging and
Social Choice 19 (3) (2009).
[35] F. Lin, R. Reiter, Forget it, in: Working Notes of AAAI Fall Symposium on Relevance, 1994, pp. 154–159.
[36] G. Brewka, Preferred subtheories: An extended logical framework for default reasoning, in: Proc. of the 11th Int. Joint Conf. on, Artiﬁcial Intelligence,
1989, pp. 1043–1048.
[37] T. Eiter, G. Gottlob, The complexity of logic-based abduction, Journal of the ACM 42 (1) (1995) 3–42.
[38] M. Goldszmidt, J. Pearl, Qualitative probabilities for default reasoning, belief revision, and causal modeling, Artiﬁcial Intelligence 84 (1–2) (1996)
57–112.
[39] W. Spohn, Ordinal conditional functions: A dynamic theory of epistemic states, Causation in Decision, Belief Change and Statistics 2 (1988) 105–134.
[40] D. Dubois, H. Prade, Epistemic entrenchment and possibilistic logic, Artiﬁcial Intelligence 50 (2) (1991) 223–239.
[41] S. Benferhat, D. Dubois, S. Kaci, H. Prade, Possibilistic merging and distance-based fusion of propositional information, Annals of Mathematics and
Artiﬁcial Intelligence 34 (1–3) (2002) 217–252.
[42] S. Benferhat, S. Lagrue, O. Papini, Reasoning with partially ordered information in a possibilistic logic framework, Fuzzy Sets and Systems 144 (1)
(2004) 25–41.
[43] A. Dempster, Upper and lower probabilities induced by a multivalued mapping, The Annals of Mathematical Statistics 38 (2) (1967) 325–339.
[44] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.
[45] S. Konieczny, R. Pino-Pérez, On the frontier between arbitration and majority, in: Eighth International Conference on Principles of Knowledge Representation and Reasoning, 2002, pp. 109–120.
[46] C. Papadimitriou, Computational Complexity, John Wiley and Sons Ltd., 2003.
p
[47] T. Eiter, G. Gottlob, The complexity class 2 : Recent results and applications in AI and modal logic, in: Proceedings of the 11th International Symposium on Fundamentals of Computation Theory, in: LNCS, vol. 1279, Springer, 1997, pp. 1–18.
[48] W. Wahlster, Implementing fuzziness in dialogue systems, in: B. Rieger (Ed.), Empirical Semantics, Brockmeyer, Bochum, 1980.
[49] A.H. Jucker, S.W. Smith, T. Lüdge, Interactive aspects of vagueness in conversation, Journal of Pragmatics 35 (12) (2003) 1737–1769.
[50] D. Traum, A computational theory of grounding in natural language conversation, PhD thesis, University of Rochester, 1994.

S. Schockaert, H. Prade / Artiﬁcial Intelligence 175 (2011) 1815–1855

1855

[51] M. Pickering, S. Garrod, Toward a mechanistic psychology of dialogue, Behavioral and Brain Sciences 27 (2) (2004) 169–190.
[52] J. Lawry, Imprecise bipolar belief measures based on partial knowledge from agent dialogues, in: Scalable Uncertainty Management, in: Lecture Notes
in Computer Science, vol. 6379, Springer, 2010, pp. 205–218.
[53] J. Reggia, D. Nau, P. Wang, H. Peng, A formal model of diagnostic inference, Information Sciences 37 (1985) 227–285.
[54] E. Ruspini, On the semantics of fuzzy logic, International Journal of Approximate Reasoning 5 (1991) 45–88.
[55] S. Kraus, D. Lehmann, M. Magidor, Nonmonotonic reasoning, preferential models and cumulative logics, Artiﬁcial Intelligence 44 (1–2) (1990) 167–207.
[56] D. Dubois, H. Prade, F. Esteva, P. Garcia, L. Godo, A logical approach to interpolation based on similarity relations, International Journal of Approximate
Reasoning 17 (1) (1997) 1–36.
[57] D. Dubois, H. Prade, Similarity versus preference in fuzzy set-based logics, in: E. Orlowska (Ed.), Modelling Incomplete Information: Rough Set Analysis,
Physica-Verlag, Heidelberg, 1998, pp. 441–461.
[58] R. Rodriguez, P. Garcia, L. Godo, Relating similarity-based models, counterfactuals and theory change, in: Proc. EUFIT’ 95, 1995, pp. 230–234.
[59] E. Ovchinnikova, T. Wandmacher, K.-U. Kühnberger, Solving terminological inconsistency problems in ontology design, Interoperability in Business
Information Systems 2 (1) (2007) 65–80.
[60] J.-F. Condotta, S. Kaci, N. Schwind, A framework for merging qualitative constraint networks, in: Proc. of the 21st International FLAIRS Conf., 2008,
pp. 586–591.
[61] S. Schockaert, M. De Cock, E.E. Kerre, Reasoning about fuzzy temporal information from the web: towards retrieval of historical events, Soft Computing 14 (8) (2010) 869–886.
[62] J. Allen, Maintaining knowledge about temporal intervals, Communications of the ACM 26 (11) (1983) 832–843.
[63] S. Schockaert, M. De Cock, Temporal reasoning about fuzzy intervals, Artiﬁcial Intelligence 172 (2008) 1158–1193.
[64] D. Randell, Z. Cui, A. Cohn, A spatial logic based on regions and connection, in: Proceedings of the 3rd International Conference on Knowledge
Representation and Reasoning, 1992, pp. 165–176.
[65] S. Schockaert, M. De Cock, E.E. Kerre, Spatial reasoning in a fuzzy region connection calculus, Artiﬁcial Intelligence 173 (258–298).
[66] G. Lakoff, Hedges: A study in meaning criteria and the logic of fuzzy concepts, Journal of Philosophical Logic 2 (4) (1973) 458–508.
[67] L. Zadeh, A fuzzy-set-theoretic interpretation of linguistic hedges, Cybernetics and Systems 2 (3) (1972) 4–34.
[68] P. MacVicar-Whelan, Fuzzy sets, the concept of height and the hedge very, IEEE Transactions on Systems, Man and Cybernetics 8 (1978) 507–511.
[69] S. Schockaert, H. Prade, Merging conﬂicting propositional knowledge by similarity, in: Proceedings of the 2009 21st IEEE International Conference on
Tools with Artiﬁcial Intelligence, 2009, pp. 224–228.
[70] S. Schockaert, H. Prade, An inconsistency-tolerant approach to information merging based on proposition relaxation, in: Proceedings of the TwentyFourth AAAI Conference on Artiﬁcial Intelligence, 2010.
[71] J. Lang, Possibilistic logic: complexity and algorithms, in: J. Kohlas, S. Moral, D. Gabbay, P. Smets (Eds.), Algorithms for Uncertainty and Defeasible
Reasoning, in: Handbook of Defeasible Reasoning and Uncertainty Management Systems, vol. 5, Kluwer Academic Publishers, 2001, pp. 179–220.
[72] K.W. Wagner, More complicated questions about maxima and minima, and some closures of NP, in: International Colloquium on Automata, Languages
and Programming on Automata, Languages and Programming, Springer-Verlag, New York, 1986, pp. 434–443.
[73] C. Cayrol, M.-C. Lagasquie-Schiex, T. Schiex, Nonmonotonic reasoning: from complexity to algorithms, Annals of Mathematics and Artiﬁcial Intelligence 22 (3–4) (1998) 207–236.
[74] B. Nebel, Syntax-based approaches to belief revision, in: P. Gärdenfors (Ed.), Belief Revision, Cambridge University Press, 1992, pp. 52–88.

