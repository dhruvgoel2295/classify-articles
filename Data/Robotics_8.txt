Robotics and Autonomous Systems 73 (2015) 155–170

Contents lists available at ScienceDirect

Robotics and Autonomous Systems
journal homepage: www.elsevier.com/locate/robot

A survey of sensor fusion methods in wearable robotics
Domen Novak ∗ , Robert Riener
Sensory–Motor Systems Lab, ETH Zurich, Tannenstrasse 1, CH-8092 Zurich, Switzerland

highlights
•
•
•
•

Overview of sensor fusion in wearable robots like prostheses and exoskeletons.
Main sensors: electromyography, electroencephalography, and mechanical sensors.
Emphasizes multimodality, adaptation and switching between sensor fusion schemes.
Online evaluation of sensor fusion methods is crucial.

article

info

Article history:
Available online 16 September 2014
Keywords:
Sensor fusion
Assistive robotics
Rehabilitation robotics
Classification
Electromyography
Electroencephalography

abstract
Modern wearable robots are not yet intelligent enough to fully satisfy the demands of end-users,
as they lack the sensor fusion algorithms needed to provide optimal assistance and react quickly to
perturbations or changes in user intentions. Sensor fusion applications such as intention detection have
been emphasized as a major challenge for both robotic orthoses and prostheses. In order to better examine
the strengths and shortcomings of the field, this paper presents a review of existing sensor fusion methods
for wearable robots, both stationary ones such as rehabilitation exoskeletons and portable ones such as
active prostheses and full-body exoskeletons. Fusion methods are first presented as applied to individual
sensing modalities (primarily electromyography, electroencephalography and mechanical sensors), and
then four approaches to combining multiple modalities are presented. The strengths and weaknesses of
the different methods are compared, and recommendations are made for future sensor fusion research.
© 2014 Elsevier B.V. All rights reserved.

1. Introduction
Wearable robots have developed rapidly over the last decades
and have demonstrated their ability to assist humans in a variety of military, medical, and industrial applications. Perhaps the
most iconic examples of such wearable robots are full-body exoskeletons such as the Hybrid Assistive Limb (HAL) [1], but smaller
powered orthoses are no less important. Furthermore, powered
prosthetic arms [2] and legs [3] also represent a type of wearable
robot.
Existing wearable robots face numerous challenges with regard
to both hardware and software. One major challenge is that the
robot usually lacks the capability to adequately recognize the actions and intentions of the human wearer. Consequently, it cannot
properly assist the wearer, a drawback that has been emphasized
both in exoskeletons [4] and prosthetics [5]. In an effort to overcome these challenges, engineers have used numerous sensors and

∗

Corresponding author. Tel.: +41 774470158.
E-mail address: domen.novak@hest.ethz.ch (D. Novak).

http://dx.doi.org/10.1016/j.robot.2014.08.012
0921-8890/© 2014 Elsevier B.V. All rights reserved.

inference methods to obtain information about the wearer’s intentions.
In many cases, the sensors used are those already built into
the wearable robot, such as joint angle sensors. More advanced
approaches incorporate electrophysiological measurements such
as electromyography (EMG) or electroencephalography (EEG), or
alternatively mechanical sensors placed on a part of the body that
is not covered by the wearable robot. Recently, there has been
a push to combine multimodal information, combining different
sensor types to obtain a more complete picture of the user [6–9].
Multimodal information, however, also requires new sensor fusion
algorithms.
This paper presents a review of sensor fusion algorithms for
wearable robots, both robotic orthoses (e.g. exoskeletons) and
prostheses. It is aimed primarily at engineers who need to convert
raw sensor data to information about the wearer’s actions and
motor intentions and is divided into two larger sections. The first
covers unimodal systems, where multiple signals are obtained
from a single type of sensor (though multiple sensors of the same
type are generally used). Section 2 covers multimodal systems,
where it is necessary to combine signals from different types of
sensors altogether. While there have not been detailed reviews

156

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

Fig. 1. Overall structure of the robot’s decision-making process.

of sensor fusion for multimodal systems, several reviews do exist
for unimodal systems [10,11]. We attempt to go beyond these
reviews by providing an overview of algorithms for different
signals and different devices together, emphasizing areas where,
e.g., sensor fusion for prosthetics may be better developed than
for exoskeletons or where, e.g., sensor fusion for EEG could learn
a lesson from EMG systems.
As the state of the art is extensive, we narrowed the focus in a
few ways.
– Classic control strategies such as impedance control are only
mentioned briefly, as they have been very well-reviewed in
other publications and are generally not considered sensor fusion.
– Not every example is referenced for every sensor fusion algorithm; if numerous systems use essentially the same algorithms, only the most informative examples are given.
– We only review sensor fusion approaches that are used with a
wearable robot or in similar conditions. For example, systems
that use EEG to control a robot arm are included; systems that
control a cursor on a screen are not.
– Sensor fusion approaches must be used for real-time robot control or clearly suitable for real-time use.
We begin the paper by introducing some general terms related
to sensor fusion in Section 2. Section 3 presents unimodal
sensor fusion, where a single modality is used in classification
or regression. Section 4 then presents multimodal sensor fusion,
where two or more modalities are used. Section 5 briefly discusses
sensor fusion performance evaluation, regardless of modality or
sensor fusion algorithm. Finally, Section 6 concludes the paper
with a summary and general discussion of the state of the art in
wearable robotics.
2. General terms
The general process of robotic decision making is shown in
Fig. 1. The first step of the process, signal acquisition, will not be
covered in this paper.
Filtering is the first, preprocessing stage of sensor fusion. It almost always includes bandpass filtering, which removes all components of the raw digital signal except those in a defined pass
band (e.g. 20–500 Hz for EMG). This removes low-frequency mechanical artefacts and high-frequency aliasing effects. Other possible types of filtering include notch filtering to remove electrical
noise at 50 or 60 Hz or spatial filtering to remove unwanted signal
components in the same frequency band as the useful signal [12].
Feature extraction is the process of extracting useful information (‘features’) from filtered signals. This can be as simple as
rectification, but more complex features such as spectral power
distribution are also common. Notably, features do not need to
have the same sampling frequency as the raw signals. Instead, feature extraction commonly includes segmentation, which divides
the raw signals into windows—intervals of a defined length. Features are extracted over the entire window and are output at the
end of the window. A window can optionally overlap with the previous window, which allows more frequent commands to the robot
and fewer sudden changes in sensor fusion output. An example of
feature extraction from windows is shown in Fig. 2.

Fig. 2. Windowing and feature extraction for a signal x(t ) with 1-s windows. w
represent windows while f (T ) represent features extracted at time T from the
corresponding window. In the first example, windows do not overlap; in the second
example, each window overlaps the preceding one by 50%.

Classification and regression are alternatives to each other,
and a wearable robot generally utilizes one or the other. Multiple
features are used as inputs simultaneously. A good introduction
to both approaches is available from Bishop [13], but in wearable
robotics:
– Classification assigns a discrete label to extracted features
(e.g. ‘‘hand closing’’, ‘‘leg lifting’’). This discrete label generally
represents the action that the user wants to perform, and
a high-level robot controller is necessary to decide how to
react to this desired action. The high-level controller outputs
the velocity/torque the robot should apply, and a low-level
controller ensures that this velocity/torque is applied.
– Regression converts features to continuous values (e.g. joint
torques). These values represent either the velocity/torque the
user is trying to apply or directly the velocity/torque the robot
should apply. Therefore, only low-level rather than high-level
robot control is required.
In prosthetics, classification is sometimes referred to as ‘‘pattern recognition based control’’ while regression is sometimes
referred to as ‘‘proportional control’’ or ‘‘continuous decoding’’.
For simplicity’s sake, we refer to both by their general, fieldindependent names.
Most classifiers are based on supervised machine learning: they
learn classification rules from a set of previously recorded and
labelled training data [13]. The accuracy of such classifiers is then
defined as the percentage of correct class assignments. Regression
is also often based on supervised machine learning, but can also
utilize manually defined regression rules. As continuous output
values allow smoother control, the sampling frequency of features
for regression is generally higher than for classification and can be
as high as that of the raw signals.
Robot control takes the results of classification or regression
and converts them into the command given to the wearable robot’s
actuators. Though it will not be described in detail, readers should
keep in mind that, as mentioned above, classification requires
more complex (high-level) robot control algorithms than regression.

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

157

Table 1
Steps involved in classification and regression of EMG data.
Classification
Preprocessing
Windowing
Feature
extraction
Sensor fusion

Bandpass filter
150–250 ms, overlapping
Time-domain, root-mean-square, autoregressive
Single
nonadaptive
classifier

Output

Single
Parallel
adaptive
classifiers
classifier
User’s desired action

Regression
Bandpass filter
No windowing
Rectification and smoothing
Simple
proportional
control

3. Unimodal sensor fusion
Wearable robots that utilize a single measurement modality
are largely based on either EMG (Section 3.1) or electrical brain
activity (Section 3.2). There has recently also been a push to include
mechanical sensors such as inertial measurement units (IMUs)
and pressure-sensitive insoles attached to other parts of the body
(e.g. for a prosthetic leg, placing an IMU on the intact leg). For
purposes of this review, these are considered as a single modality
and are described in Section 3.3. Finally, sensor fusion for other,
less common individual modalities is described in Section 3.4.
3.1. Electromyography (EMG)
EMG is the measurement of electrical activity produced by
muscle cells during muscle activation, which is typically visible
about 100 ms before muscle movement occurs [14]. For prostheses,
the main advantage of EMG is that, for example, if a hand is
amputated, the muscles used to open and close the hand are still
present. The EMG of these muscles can be directly interfaced with a
powered prosthesis to provide natural control: the same muscles
used to open and close the hand are used to open and close the
prosthesis. Such simple on–off myoelectric control was used in
the first powered prostheses and is still used in most commercial
prostheses [15]. For an exoskeleton, it is possible to identify
mathematical models that describe the relationship between EMG
and the forces and moments of the limbs. By predicting these
forces and moments from EMG (which is visible prior to muscle
movement), the robot can rapidly apply proportional assistive
moments, reinforcing the motion [16].
EMG can be recorded on various muscles with invasive or
noninvasive electrodes. While the signal quality is higher for
invasive electrodes and for some muscles, similar sensor fusion
algorithms are used for both invasive and noninvasive electrodes.
The algorithms can be broadly divided into classification and
regression, each of which consists of multiple steps. These steps are
summarized in Table 1 and described in the following subsections:
Section 3.1.1 covers the initial bandpass filtering, Section 3.1.2
covers classification and Section 3.1.3 covers regression.
The choice between classification and regression depends
on the application and type of wearable robot. Classification is
dominant in arm prosthetics, where usually a limited number of
discrete postures are defined for the prosthesis. As the limb is
no longer present, its dynamics do not need to be considered,
which allows for an easy link between the classifier output and
the prosthesis controller. Exoskeletons, on the other hand, more
commonly use regression: as the human limb is already applying
certain forces and torques to the exoskeleton, the challenge is to
identify and augment these as accurately as possible, which cannot
be done with a limited selection of discrete classes.
Both classification and regression represent an alternative to
so-called on–off control systems, where the robot only has one
function that is turned on or off depending on whether one EMG
signal exceeds a threshold [9]. While on–off control is still used
in many commercial prostheses, it is outdated from a scientific
perspective and is not discussed in this paper.

Muscle
models

150–250 ms, overlapping
Time-domain, root-mean-square, autoregressive
Muscle
synergies

Neural
networks

State-space
models

Control signal(s) for robot joint(s)

3.1.1. Filtering
Both classification and regression are preceded by bandpass
filtering. The lower cutoff frequency of the filter is generally 10
or 20 Hz while the upper cutoff is between 400 and 500 Hz,
as recommended by the European SENIAM project [17]. As the
dominant frequency range of a limb EMG signal is between 50 and
150 Hz, these cutoffs remove low-frequency motor artefacts and
high-frequency aliasing while preserving useful information [17].
Though some studies have set the lower cutoff to as low as
5 Hz [18,19], a 20-Hz cutoff removes more motor artefacts and
is thus preferred. In fact, studies with powered prostheses have
shown that even a 60–250 Hz bandpass filter does not result
in significantly lower EMG classification accuracy compared to a
conventional 20–500 Hz filter [20].
An additional source of noise are electrocardiographic artefacts.
While these are not taken into account by many myoelectric
control studies, they do degrade control performance [21] and can
be removed by a variety of other filters [22].
3.1.2. Classification
3.1.2.1. Windowing and feature extraction. Appropriate windowing and feature extraction are crucial for EMG classification. Using longer windows generally increases classification accuracy, but
also means that actions are taken with a longer delay [23]. While
some delays are necessary for signal processing, there is general
consent that delays above 100 ms result in decreased functional
performance of prostheses and that delays over 300 ms are unacceptable for real-time use [24]. Windows of 150–250 ms have
been suggested as the optimal tradeoff between classification accuracy and delay [23], and this range is used by most publications
on EMG-based classification, with overlapping windows being the
most common.
There are three well-established feature types for EMG: timedomain features, the root-mean-square value, and autoregressive
model coefficients. Time-domain features were initially popularized by Hudgins et al. [25], with the most popular being the mean
absolute value, number of zero crossings, waveform length and
number of slope sign changes. The root-mean-square (RMS) value,
while also theoretically a time-domain feature, is often described
in EMG studies as separate from time-domain features [26,27]. Autoregressive coefficients are slightly less common than RMS and
time-domain features, but are considered to be more robust to
electrode displacement than time-domain features [28,29]. Other,
less common features include, e.g., wavelet-based features [30,31].
Several studies have compared different features with regard
to classification performance. One issue with these studies is that
they frequently introduce a new feature type and compare it to established features, with the new feature exhibiting better performance (e.g. Shi et al. [32]). While we do not question their results,
it is likely that similar studies, where a new feature type is found
to be ineffective, simply do not report using it. Additionally, as long
as well-established feature types are used, the difference between
them is often not large [19,33,34]. While it is likely that some features are indeed more robust with regard to electrode shift [28] or
noise [35], these effects have not been studied on sufficiently large
sample groups for strong conclusions to be drawn.

158

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

3.1.2.2. A single time-invariant classifier. The simplest and most
popular classification approach is to use a single time-invariant
classifier. A variety of classifiers have been used in the literature: linear discriminant analysis (LDA) [19,30,34,36], support vector machines (SVM) [36,37], artificial neural networks [30,34],
nearest-neighbour classifiers [38], neurofuzzy classifiers [39] etc.
They are sometimes preceded by dimension reduction methods
such as principal component analysis to reduce the amount of input features and prevent overfitting [31,36,40,41].
While dimension reduction is an important step, especially
if many muscles and/or features are used, the choice of specific
classifier is generally considered to be less important than the
choice of features [42,43]. This has also been demonstrated in
wearable robotics, where the effect of classifier type on accuracy
is generally small [44–46], especially compared to factors such as
window length, feature types and electrode placement [34].
While the type of an individual classifier is not critical, any
time-invariant classifier suffers from several issues. First, the EMG
signal’s temporal (amplitude) and frequency (power spectrum)
characteristics vary slowly over time due to many factors: electrode shift, sweat, muscle fatigue, sensor failure etc. Second, the
signal may rapidly change from informative to useless due to
events such as sudden sensor fault. A time-invariant classifier cannot take these issues into account and suffers from decreased accuracy over time. Finally, a classifier trained on many subjects is likely
to show decreased accuracy on a new user that was not included in
the training group. All of these issues can be addressed using either
adaptive classifiers or multiple classifiers working together.
3.1.2.3. Adaptive classifiers. Adaptive classifiers were first used to
adapt to slow EMG variations over time. Such adaptation can be
either supervised, where retraining sessions are performed by
the users as conditions change, or unsupervised, automatically
updating the classifier when it is sufficiently certain that a
correct or incorrect class has been assigned [47]. Unsupervised
adaptation is commonly performed using entropy as an indicator
of confidence in the classifier’s decision [47,48]. Both adaptation
types can reduce classification error in case of slowly time-varying
signals, though supervised adaptation is more accurate [47].
Adaptive classifiers can also adapt to rapid EMG variations using
so-called sensor fault detection modules that identify probable
sensor faults [49,50]. If a fault is detected in one of the input signals,
features extracted from that signal are omitted and the classifier is
retrained to work without them. While this requires training data
to be kept in memory, it represents an easy way to reduce error
due to hardware failure.
Finally, adaptive classifiers can be applied when a new user
starts using the wearable robot for the first time. Rather than
retraining the robot for the new user, the classifier starts with
classification rules learned from many previous subjects and
adapts them to a new subject. The approach of Tommasi et al. [51],
for example, assumes that at least one of the previous subjects will
be similar to the new user. After acquiring some data from the new
subject, this approach identifies one or several similar subjects in
the training data and uses them as a starting point for adaptation to
the new user. The method is based on previous work by Matsubara
et al. [52], who combined task- and user-specific information in a
stylistic myoelectric model.
Though computationally complex and requiring additional
memory to store training data rather than simply the classifier,
adaptive classifiers are likely a crucial prerequisite for complex
myoelectric control to gain acceptance among end-users. As they
are still at an early stage, we encourage researchers in the field to
focus on them.
3.1.2.4. Parallel classifiers. Multiple classifiers can be used in
parallel to control different functions of a wearable robot. For
example, if we wish to control multiple fingers of a hand prosthesis,

a classifier can be trained for each individual finger, and the outputs
of the classifiers can be used simultaneously to create the desired
hand posture [53]. Similarly, separate classifiers can be constructed
for wrist and hand motions [54].
Alternatively, the robot can switch between different classifiers
depending on the situation. The switching is generally performed
based on a second sensing modality (e.g. accelerometers) and is
thus covered in Section 4.
Finally, parallel classifiers can be used simply to improve reliability. Hargrove et al. [55] implemented multiple binary classifiers and forced an artificial limb to take no action if the individual
classifiers disagree with each other. In a follow-up study, Scheme
et al. [46] introduced a selective one-versus-one classification approach, which similarly forces no output when classifiers disagree,
but produces fewer total errors than the multiple binary classifier
scheme.
The advantage of all parallel classifications approaches is that
they do not simply aim to improve classification accuracy over single classifiers, but also try to improve overall performance of the
wearable robot by allowing more functions to be performed simultaneously or rejecting false positives. Like adaptive classifiers, they
are an essential yet still underdeveloped field of research.
3.1.2.5. Error reduction in postprocessing. Whatever the classifier,
it will always make some errors. A final postprocessing step can be
used after classification to reduce the amount of these errors. One
common postprocessing approach is the majority vote [56]. In this
case, the majority vote decision at a given time step consists of the
most commonly chosen decision over the last several time steps; it
is a ‘smoothed’ version of the original decision stream and prevents
spurious classification errors due to brief noise from affecting the
movement of the robot. However, while the approach improves
classification accuracy, some studies have suggested that it does
not have a practical effect on real-time task performance [57].
A second postprocessing approach is the decision-based
velocity ramp [57]. In this case, every possible class has a counter
associated with it. When a class decision is made, the value of the
associated counter increases while the values of all other counters
decrease. The speed with which the wearable robot moves is then
proportional to the counter values. In practice, this means that
when a new motion is begun, the robot initially assists very slowly
due to the low value of the associated counter, but quickly speeds
up if the classifier output is consistent. A single error therefore
has little effect on the overall motion of the wearable robot. This
velocity ramp has been shown to have significant performance
advantages over the majority vote, improving task completion
speed in amputees with powered prostheses [57].
3.1.3. Regression
3.1.3.1. Rectification, smoothing and feature extraction. EMG feature extraction for regression is usually simple and consists only
of full-wave rectification and smoothing of the preprocessed EMG
signals. The smoothing is done with either a moving average filter [58] or a low-pass filter [59]. The rectified and smoothed values
are then fed directly to the regression algorithm. An optional additional step is to normalize the signal with respect to a reference.
For example, the current value can be divided by the value obtained
during a maximum voluntary contraction, resulting in a signal that
is always between 0 and 1 [16,58]. Alternatively, for lower limb
robots, the current value can be divided by the maximum value obtained during passive walking (i.e. when the robot is worn but not
powered) [60] or divided by the maximum value obtained during
the last several gait cycles.
Some studies nonetheless do perform windowing and complex
feature extraction prior to regression. In such cases, common features are similar to classification features: the root-mean-square

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

value [61,62], time-domain features [63,64] and autoregressive
coefficients [63]. The windows can be non-overlapping, with a
control output sent, for example, every 100 ms [62], but are more
commonly highly overlapping, with the feature signal sometimes
having the same frequency as the raw EMG signal [64].
3.1.3.2. Simple proportional control. The simplest way to provide
a continuous output signal from the EMG to the wearable robot
is to multiply the rectified and smoothed signal by a gain factor,
then directly use this as the desired velocity, torque or other
robot output. Usually, specific muscles are linked to specific joints
of the wearable robot. While not a very elegant approach, the
computational simplicity has led to its adoption in several devices,
particularly exoskeletons like the HAL [65].
In this case, sensor fusion consists only of the gain factor, and
the challenge Is how to set it in order to achieve efficient control. If
the EMG signal has been normalized, it always has a value between
0 and 1. The robot can then respond proportionally, remaining stationary at 0 and providing the maximum possible torque/velocity
at 1. If the signal has not been normalized, the gain factor can be set
manually by the user [66] or a gain can be calculated using linear
regression in a calibration phase [65].
3.1.3.3. Muscle models. Several mathematical models describe
muscle mechanics and electrical muscle activation, the most famous being the Hill muscle model [67]. An influential early study
with an exoskeleton and Hill model was performed by Rosen
et al. [16], who used the model to predict human elbow moments
and apply a proportional assistive moment by the exoskeleton.
The parameters of the model need to be optimized for each subject, which can be done using techniques such as genetic algorithms [68].
Following Rosen’s work, several groups have implemented
either variants of the Hill model or other neuromuscular models
in order to predict joint moments and reinforce them using
wearable robots [14,69,70]. The main advantage of such models
is that they should be universal, being valid for most subjects
and able to handle a variety of situations [71]. Most notably,
recent work by Sartori et al. [72] demonstrated that muscle models
can be calibrated rapidly online and calculate muscle forces and
moments at frequencies around 100 Hz while accounting for threedimensional muscle kinematics and muscle fibre physiology.
3.1.3.4. Neural networks. While the Hill model attempts to describe the physical phenomena in the muscle, many researchers instead use supervised machine learning to create black-box models
that estimate quantities such as joint moments from EMG without any knowledge of the underlying physical process. By far the
most common supervised learning algorithm used with EMG in
wearable robotics is the neural network. Variants such as multilayer perceptrons [63,73], time-delayed networks [74] or neurofuzzy networks [61] have been applied to various wearable robots,
with good results.
It is not clear, however, how neural networks compare to Hilltype models. A comparison by Rosen et al. [71] found better performance with neural networks, while a later comparison [75] in a
different application found better performance with muscle models. Due to the lack of other comparisons, no definitive statement
can be made about accuracy. Nonetheless, compared to muscle
models, neural networks are a black-box approach that is taskspecific and requires large amounts of training data [71].
Simpler supervised learning alternatives to neural networks
have been proposed, particularly in the form of linear regression [76,77]. Due to its simplicity, linear regression is less accurate
than neural networks if sufficient training data is available, but performs better when training data is limited.
3.1.3.5. Muscle synergies. The basic principle of muscle synergies states that the central nervous system activates groups of

159

muscles in coordination to perform a certain task, and achieves
motor goals by controlling muscle synergies rather than individual muscles [78]. In wearable robotics, identifying muscle synergies would allow a larger number of degrees of freedom to be controlled with a small number of EMG signals, which would be more
efficient than the simple proportional approach of linking one degree of freedom to one EMG signal.
Muscle synergies are generally modelled using non-negative
matrix factorization (NMF), a multivariate statistical data analysis technique that is used with EMG due to similarity with the
underlying neurophysiological processes. It attempts to extract
muscle synergies from multichannel EMG signals, using the lowest possible number of synergies to control as many degrees of
freedom of the wearable robot as possible. The approach is semiunsupervised: while training is necessary, the only required information is which degrees of freedom are activated at a given
time [78]. While this semi-supervised nature leads to lower regression accuracy, the effect on online task performance is unclear [76].
3.1.3.6. State-space model. The state-space model estimates motion variables from EMG data using a classic state-space model
with intrinsic, unobservable system states. The model has the following form [79]:
xt +1 = Axt + But + vt
yt = Cxt + vt
where x represent hidden states, u represent EMG inputs, y
represent output motion variables, and vt and vt represent random
noise at time t. The matrices A, B and C are calculated from a
training data set.
The advantage of such a model is that it is robust to EMG
changes over time due to fatigue and other factors [79]. Furthermore, it can be upgraded with a switching regime that switches between several different models of the above form depending on the
current situation [64]. However, although the state-space model is
common for fusion of electrical brain measurements (Section 3.2),
it has only recently been introduced to EMG fusion for wearable
robots, and its practical advantages are not yet proven.
3.2. Brain measurements
Brain–machine interfaces have been proposed for control of
wearable robots since they allow paralysed users to control a robot
without any motor activity [80,81]. Furthermore, rehabilitation
robots controlled by brain signals can enhance plastic changes in
the brain during rehabilitation [82,83]. Such wearable robots are
generally controlled using three types of electrical brain activity:
EEG, electrocorticography (ECoG), and intracortical electrodes.
Non-electrical brain measurements such as magnetic resonance
imagining are generally not suitable for wearable robots due to
their nonportability.
Unlike the EMG section, which is divided according to sensor
fusion approach, this section is divided primarily according to
signal type, as the choice between classification and regression is
implicit in the choice of brain signal. The different possibilities are
summarized in Table 2.
EEG, which is recorded noninvasively, does not have the information content needed for precise regression, so a small number of
discrete classes are used instead. Steady-state visually evoked potentials (SSVEPs—Section 3.2.1) and P300 potentials (Section 3.2.2)
are elicited using controlled external stimuli and require less complex sensor fusion. Motor imagery (Section 3.2.3) is not connected
to external stimuli and requires more complex sensor fusion to
recognize, as does the use of SSVEPs and motor imagery together
(Section 3.2.4). The ECoG (Section 3.2.5) and intracortical signals
(Section 3.2.6), however, are recorded with invasive sensors and
enable either classification or regression due to the higher information content.

160

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

Table 2
Characteristics of different brain measurements.
Electroencephalogram

Invasive?
Requires external stimulus?
Bandpass filtering
Sensor fusion approach

SSVEP

P300

Motor imagery

No
Yes
5–30 Hz
Classification

No
Yes
0.5–30 Hz
Classification

No
No
1–100 Hz
Classification

3.2.1. EEG: steady-state visually evoked potentials
SSVEPs are the brain’s natural responses to visual stimulation
at different frequencies [84]. In brief, if we look at a light that is
flashing with a particular frequency, our visual cortex responds
with EEG activity at the same frequency. This principle is used
in brain–computer interfaces as a gaze tracking method. The user
is presented with multiple symbols on the screen, with each
symbol flashing at a different frequency. By measuring the SSVEP
frequency, the computer can identify which symbol the user is
looking at.
SSVEPs are used in wearable robotics to send commands to the
robot. The wearer is presented with possible commands on the
screen (e.g. move forward, stop) and selects one by looking at it.
The approach is noninvasive and easy to use, but problematic for
a truly mobile robot since it requires a screen. Nonetheless, it has
been applied to both artificial limbs [85] and exoskeletons [80,86].
Sensor fusion for a SSVEP system consists of extracting the
stimulus frequency (frequency of flashing lights) and begins with
bandpass filtering. The cutoff frequencies are generally not critical
with respect to classification performance, as the possible stimulus
frequencies are defined by the hardware designer, and other
frequency bands are not examined for information. As common
SSVEP frequencies are between 6 and 40 Hz [85], the filters can
be as wide as 0.5–100 Hz [80,85] or as narrow as 5–30 Hz [86].
After filtering, the frequency band of interest is checked to find
the frequency with the highest activity. This can be done with
a Fourier transformation [80,86] or canonical correlation analysis [85,86]. Identifying the stimulus frequency is generally only a
matter of finding the peak in the power spectrum. The command
corresponding to this frequency is then sent to the robot. The correct command is generally selected 75%–90% of the time for two or
three possible commands [80,85,86]. The user can also choose not
to send a command by simply not focusing on the screen, though
false positives are common since users still see the screen at the
edge of their vision [80].
3.2.2. EEG: P300
The P300 is an electrical potential that appears about 300 ms
after the user has observed a rare relevant stimulus [84]. Similarly
to SSVEPs, it is used to select one of several possible commands.
Possible commands flash on the screen, and the command that
the user desires will evoke a P300 response. The P300 requires no
training to utilize, but has a lower information transfer rate than
SSVEPs—approximately 20–25 bits/min compared to 60–100 with
SSVEPs [84].
The sensor fusion goal is to determine which command the user
has selected by locating the P300 in the EEG signal immediately
after a stimulus (presentation of a command). The EEG is bandpass
filtered at either 0.5–30 Hz [87] or 1–16 Hz [88]. Spatial filtering
can also be applied to enhance detection [87,88]. Immediately after
a stimulus, EEG is sampled for approximately 0.5 s, and a classifier
is applied to this interval to classify as P300/no P300 [87,88]. If a
P300 is detected, the current command is considered selected and
sent to the robot. Again, the user can send no command by not
focusing on the screen.
Though obtained with a humanoid rather than wearable robot,
results of Bell et al. [87] suggest that a P300-based interface can

Electrocorticogram

Intracortical electrodes

Yes
No
Not critical
Classification or regression

Yes
No
Not critical
Regression

transmit about 25 bits of information per minute with about 95%
correct selection rate among 4 possible commands. False positives
are problematic, as P300 responses also occur naturally in the
absence of visual stimuli. The P300 also suffers from the same
disadvantage as the SSVEP: a screen must be used to present
the stimulus. Additionally, developers should consider whether a
SSVEP or P300 based control system has any advantages over an
eye tracking device in their particular application.
3.2.3. EEG: motor imagery
The principle of motor imagery is, on paper, simple: the user
thinks of making a motion, and their brain waves change as a result
of the imagined motion. These changes can be monitored with EEG
and used to control a wearable robot. For example, imagined left
hand movement could be used to trigger the left hand of a robot.
The advantage of motor imagery over SSVEP and P300 is that no
additional screen is required, but generating useful motor imagery
requires special user training, and only a small number of motor
images can be distinguished using EEG [84].
As motor imagery is not necessarily connected to external
stimuli, the sensor fusion system should ideally operate in asynchronous mode, regularly checking for the presence of motor imagery [89]. As with SSVEP and P300, bandpass filtering is not critical
and is commonly done at 1–100 Hz [89,90]. Spatial filtering can be
added to improve signal quality [89]. For asynchronous operation,
features are extracted from windows of various possible lengths:
200 ms [91], 250 ms [89], 750 ms [92] and 1000 ms [85,90]. No
specific length has been shown to have an advantage in wearable
robotics.
Motor imagery causes power to decrease in some frequency
bands and increase in others, so extracted features are generally
estimates of power in different frequency bins, calculated using
Fourier transformation, autoregressive analysis [89,93], wavelet
analysis [93]. The number of features is optionally reduced using
principal component analysis [92] or LDA [91,92]. A single static
classifier such as LDA [85,89,90,93], SVM [93] or Bayesian classifier [92] is then applied to the remaining features to identify the
presence and type of motor imagery. This information is sent to
the wearable robot, which may execute a predefined command
or select from several possible commands depending on its current state. For example, in the work of Rohm et al., the system
activates electrical stimulation when motor imagery is detected,
but the type of stimulation depends on the current position of the
wearable robot [94].
Though asynchronous operation is ideal for wearable robots,
some robots do operate in synchronous mode where a stimulus is
given to the user to begin motor imagery, and EEG is only analysed
for a period of time after the stimulus [82,83,93]. These robots
are generally rehabilitation devices, with the goal of encouraging
neuroplasticity and associative learning by triggering the robot
in response to motor imagery [82,83]. Except for only analysing
EEG once per stimulus, sensor fusion proceeds identically to
asynchronous devices.
Finally, motor imagery can also be detected non-electrically
using functional near-infrared spectroscopy, a portable measurement of haemodynamic brain activity. One study of such measurements was conducted by Zimmermann et al. [95] with the goal of

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

triggering a rehabilitation robot. The study obtained good classification ability between rest and action. However, results have only
been published for healthy subjects, the delay between onset and
detection is significantly greater than with EEG, and several issues
remain to be resolved before the approach can be used in practice.
3.2.4. EEG: SSVEP + motor imagery
SSVEPs and motor imagery can be used together in a single system, with each controlling a different function of the device. This
approach is still relatively new in both wearable robotics and general brain–computer interfaces, and only a few robotic examples
exist. In the study of Pfurtscheller et al. [90], motor imagery is used
as a ‘switch’ to activate or deactivate the SSVEP portion of the system, and SSVEPs are then used to input a specific command. In
the study of Horki et al. [85], motor imagery controls the grasping while SSVEPs control the elbow joint of a robotic arm. Sensor
fusion for each component is identical to that presented above.
3.2.5. ECoG
Unlike EEG, the ECoG is recorded invasively, with electrodes
placed on the surface of the brain using a surgical procedure. Due to
the invasiveness, its applications to wearable robotics are mainly
limited to severely impaired users. Like EMG, ECoG can be used for
classification or regression.
As ECoG is recorded inside the skull, it is less contaminated by
noise. For initial bandpass filtering, the lower cutoff frequency can
be below 0.1 Hz [96–99] or even omitted altogether [96]. In asynchronous operating mode, windows of 100 ms [100], 250 ms [99]
to 1000 ms [97,101] are used, with no known optimal length.
3.2.5.1. Classification. For classification of ECoG data, common
features include power in the beta band [99], power in the gamma
band [99,101,102] and Savitsky–Golay filter-based features [96].
Classification is performed with a single time-invariant classifier
such as LDA [96,99], SVM [101] or naive Bayesian classifier [102].
Classification of ECoG data in wearable robotics has achieved
accuracies of 75%–95% for two-class problems [96,99,102] and
70%–80% for up to five-class problems [102]. However, studies have noted significant differences between offline and online
performance due to, for example, signal nonstationarity [102].
Furthermore, users with motor dysfunction display substantially
lower classification accuracy [101], which may be problematic for
practical applications.
3.2.5.2. Regression. A limited number of studies have attempted
to continuously decode limb motions from ECoG for purposes
of wearable robotics. Similarly to classification, the approaches
are mainly based on frequency analysis. After bandpass-filtering,
signals are commonly re-referenced with a common average
reference [97,100,103], and frequency features are extracted using
autoregressive analysis [98,100], wavelet transformation [97] or
by simply bandpass-filtering the signals into individual frequency
bands and smoothing them [103].
Features are transformed into trajectory information by means
of either linear regression (which incorporates features from the
current time step as well as previous time steps) [97,103] or
by more complex methods such as convolutive mixtures [98] or
dynamic state-space models [100].
Currently, ECoG classification and regression suffer from the
problems that different sensor fusion methods are rarely compared on the same data and that performance is generally evaluated only with metrics such as classification/regression accuracy.
This limitation is to be expected due to the difficulty of obtaining ECoG measurements, which require an invasive surgical procedure. However, it does mean that it is difficult to identify an
optimal sensor fusion approach. Additionally, because of its invasiveness, the approach is currently probably unsuitable for use
with wearable robots. It would only be useful for tetraplegics, who

161

are more likely to benefit from non-wearable robotic arms or intelligent wheelchairs rather than wearable robots.
3.2.6. Intracortical electrodes
Intracortical electrodes have the highest invasiveness as well
as the highest signal quality among measurements of electrical
brain activity, as they penetrate inside the brain. They have
achieved considerable interest in the robotics community due to
several high-impact publications showing that they can be used
to control devices such as powered prostheses and multijointed
robotic arms [2,104]. Due to the high signal quality, they generally
use regression rather than classification, and extracted features
are usually the firing rates of individual neurons. Regression is
performed asynchronously using a Kalman filter [2,105,106] or
other type of linear filter [81,107].
As with ECoG, intracortical electrodes are unsuitable for wearable robotics in the near future, as their invasiveness only makes
them useful for tetraplegics, and most studies are currently performed with primates rather than humans. However, other sensor
modalities can learn a few things from these studies. First, intracortical systems have proposed several adaptive sensor fusion methods, as the implanted nature of the electrodes forces the system to
be able to adapt to gradual changes [106]. Second, they have shown
that users will quickly adapt to certain types of error (particularly
directional bias) that can therefore be considered less problematic
for sensor fusion with any data [108]. Third, parameters such as
window length do not have the same optimal parameters in online
and offline use, as the real-time feedback received online by users
affects the entire process [105].
3.3. Mechanical sensors
We define mechanical sensors in the context of this review as
those that directly measure forces and kinematics. Common examples are pressure-sensitive insoles, which measure foot–ground
contact, and IMUs, which are a combination of accelerometer and
gyroscope. They are separated in this section according to their
specific application. The most common application is to use these
measurements as a direct input to the controller (Section 3.3.1),
though this does not truly represent sensor fusion. The measurements can also be used to switch between different controllers for
different operating modes of the wearable robot (Section 3.3.2) and
detect unwanted conditions such as instability (Section 3.3.3). Finally, they can be placed on entirely different limbs in order to control the wearable robots using the principle of complementary limb
motion estimation (Section 3.3.4).
3.3.1. Robot control using physical human–robot interaction measurements
By far the most common application of sensors in wearable
robotics is to measure the interaction forces applied by the
wearer’s limbs and use them as a control input to the robot.
Measurements are performed with multi-degree-of-freedom force
sensors [109], distributed pressure sensors [110] and so on. The information from these sensors is then directly used in control strategies such as impedance control and zero-torque control [111,112].
These classic control strategies are, however, well-described in existing reviews [111,112] and will not be discussed further.
3.3.2. Gait phase and mode detection
Aside from directly serving as a control input to the robot, mechanical sensors can be used to switch between different operating modes of a wearable robot depending on the current situation.
This is commonly done in lower limb robots, which vary power to
different joints at different phases of a gait cycle as well as during different types of gait: level walking, stair ascent, stair descent,
ramp ascent or ramp descent. Such switching can in principle be
performed manually, but should ideally be done automatically.

162

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

3.3.2.1. Sensors built into the wearable robot. The easiest way to
switch between phases is using the joint angle and ground reaction
forces measured by the wearable robot’s built-in sensors, as is
done by the HAL exoskeleton [113]. In a more complex control
system for a fully sensorized prosthesis, Varol et al. [114] switched
between gait phases, sitting and quiet standing based on limb
joint angles and ground contact forces. Signals were segmented
into 100-ms windows, from which means and standard deviations
were extracted. Features were reduced using LDA and classified
with a Gaussian mixture model. Finally, a majority vote scheme
(Section 3.1.2.5) was used to smooth the control output.
3.3.2.2. Stand-alone wearable sensors. While the sensors from the
previous subsection were built into the wearable robot, similar
switching between gait phases and modes can be done with standalone wearable sensors on the limb itself. The identification of
gait phases using such wearable sensors has a long history in
biomedical engineering [115,116].
Specifically to identify gait phases for assistive devices, Crea
et al. [117] used pressure-sensitive insoles to measure the ground
reaction force and centre of pressure of both feet, then applied
thresholds to these signals to identify gait phases. Caltran and
Siqueira [118] similarly used thresholds on insole outputs to identify gait phases, but also corrected the angle measurements of an
ankle–foot orthosis using orientation measurements from an IMU.
To switch between gait modes in wearable robotics, Wentink
et al. detected the transition from standing to walking using
thresholds placed on IMU and footswitch signals [119] while
Novak et al. [120] detected transitions between standing and walking from IMUs and insoles using a decision tree. Li and HsiaoWecksler [121] similarly switched between level walking and stair
ascent/descent for an ankle–foot orthosis by measuring the vertical
position of an IMU and activating stair ascent/descent if the change
in vertical position was sufficiently high. In an innovative sensing approach, Zheng et al. [122] placed capacitive sensors into the
socket between the human limb and the prosthesis and used them
to classify different gait modes using quadratic discriminant analysis with features from 250-ms windows. Finally, Zhang et al. [123]
used a laser distance sensor mounted on the waist to detect stairs
in front of the user, then switch between level walking and ascent/descent.
While all these systems achieved good accuracies, many were
only tested with healthy subjects. There is a pressing need to test
such systems with the target population for wearable robots, as
impaired subjects may have different gait patterns. A promising
step was taken in this direction by Yuan et al. [124], who developed
a fuzzy classifier to distinguish between five gait modes based
on IMU and insole data. They achieved classification accuracies
over 95% for both able-bodied subjects and a transtibial amputee.
However, further tests are needed to evaluate both classification
accuracy and practical performance in wearable robotics.
3.3.3. Gait stability enhancement
Similarly to switching between gait modes such as level and
ascending walking, the wearable robot should be able to identify
unstable gait where the user is in danger of falling and requires
assistance. The best-known implementation of such stability enhancement is the prosthesis control system of Lawson et al. [125],
an upgrade of their previous switching controller [114]. An IMU
added to the prosthesis measures the ground slope and adapts the
prosthesis joint impedances to ensure stability. Experimental work
has shown that the controller quickly adapts to the ground slope
in a single amputee.
Another approach for lower limb assistance, the perceptionassist robot, was presented by Hayashi and Kiguchi [126]. In this
case, a laser distance searches for bumps in front of the user while
joint angles and foot–ground contact sensors are used to calculate
the zero moment point. If the user is judged to be in danger of
stumbling, the robot engages additional assistance.

3.3.4. Complementary limb motion estimation
Complementary limb motion estimation (CLME) is the idea that
a lower limb prosthesis can be controlled based on the motion of
the residual leg. An early approach in this direction was so-called
echo control, where the motion of the residual leg is ‘replayed’ by
the prosthesis with a time shift [127]. However, this introduces a
delay that should ideally be avoided. CLME [128] instead observes
residual limb motion and continuously complements the motion
for the missing (or impaired) limb based on principles of interjoint
coordination: activities of the entire body are coordinated during
motion, and observing the rest of the body allows the required
motion of the missing limb to be predicted.
CLME was first applied to leg rehabilitation exoskeletons [128],
then to prosthetic legs [129]. Instead of filtering and extracting
features, it operates directly with raw signals (joint angles and
angular velocities) that are obtained using gyroscopes on the intact
limb. Coupling between limbs is extracted from these signals using
either principal component analysis or linear regression to obtain
reference joint angles and angular velocities for the missing or
impaired limb. As reference estimates are subject to uncertainty,
a Kalman filter is commonly added to provide reference values to
the robot controller. CLME-based control of a powered prosthesis
has been shown to be feasible and provides additional, robust
assistance compared to a commercial control scheme [129].
An important feature of CLME is that the regression algorithm
does not need to have the same number of inputs and outputs.
In principle, other parts of the body besides the complementary
limb could be used to measure interjoint coordination (e.g. the hip
or the upper body) [129]. However, while some IMUs developed
specifically for use with wearable robots have mentioned this as
a possible application [130], there is currently only one practical
implementation, where sensors from a walking cane are included
in regression and used to control a leg exoskeleton [131].
3.4. Other sensors
Though EMG, brain measurements and mechanical sensors
represent by far the most common sensors in wearable robotics,
other modalities have been explored to some degree.
3.4.1. Intraneural electrodes
Amputees can control powered prostheses with EMG signals
recorded from residual muscles. However, as an alternative, signals can be recorded directly from the residual nerves, creating an
almost physiological condition where the nerves that had previously controlled the human hand now control the prosthesis [132].
While electrodes must be inserted into the nerve, the invasiveness
is relatively low compared to, e.g., ECoG.
In the approach proposed by Micera et al. [132], signals from
intraneural electrodes are segmented into 1000-ms windows and
denoised with a wavelet transformation. Afterwards, different
spike types are detected from the windows. Features consist of the
number of times each different type occurs, divided by the total
number of spikes. These are classified with a single SVM classifier,
which achieves 85% accuracy in a 4-class problem (rest, 3 grasp
types).
Intraneural electrodes have significant potential for the control
of prostheses and other wearable robots, as they achieve a good
trade-off between invasiveness and signal quality. They could in
principle be easily combined with EMG, increasing accuracy with
additional information. Therefore, while only a few studies have
been conducted with them, we believe that they will be a quickly
expanding control modality.
3.4.2. Non-electrical muscle measurements
Several ways of measuring muscle activity have been proposed as alternatives to electromyography for powered prosthesis
control. The two main methods are mechanomyography (MMG)

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

and sonomyography (SMG). Mechanomyography is the measurement of low-frequency vibrations produced by muscle contractions while sonomyography measures changes in muscle structure
using ultrasound.
A simple implementation of SMG in prosthetics was demonstrated by Chen et al. [133], who measured the percentage deformation of a single muscle from its resting state using A-mode
ultrasound. The percentage was then proportional to the openness
of a prosthetic hand. To evaluate the controllability of the system,
healthy users tracked a sinusoidal reference signal by opening and
closing the prosthesis. In a follow-up study, the authors used SMG
to measure the deformation of one flexor and one extensor muscle [134], then used linear regression to estimate wrist angle from
the two muscle deformations. Although the work was performed
with healthy subjects only, a parallel study showed that amputees
also have the potential to control a prosthetic hand with SMG, although they were not directly linked to such a hand [135].
A MMG-controlled prosthesis was developed by Silva et al.
[136]. They recorded MMG with three sensors inside the prosthesis, then bandpass-filtered the signals (5–50 Hz) and extracted the
RMS values from overlapping 300-ms windows. Linear regression
was used to output flexion and extension signals for the prosthesis. A more advanced sensor fusion approach was proposed by Xie
et al. [137]. After bandpass-filtering (5–400 Hz), the signals were
segmented into 256-ms windows. A wavelet transformation was
applied, followed by singular value decomposition to reduce the
number of features. Classification of forearm motions using LDA
achieved an accuracy of approximately 90% for a 4-class problem
with healthy subjects.
Several advantages over EMG have been suggested for SMG and
MMG. The main advantage of SMG is that it can monitor several
individual muscles even when they neighbour each other or are
located at different depths [133]. The advantage of MMG is that
it can be recorded distally to the muscle, requires no skin preparation, and is not affected by skin factors such as moisture [137].
Nonetheless, neither modality has achieved acceptance. The exact
reason for this is unknown, but we recommend that future studies
focus specifically on directly demonstrating superiority over EMG
in various conditions.
3.4.3. Eye tracking
Eye tracking is an unobtrusive measurement modality that
can nonetheless be used by the severely impaired since few
pathologies completely block eye movement. The eyes can be
measured using electrooculography (EOG), a low-frequency signal
whose baseline value changes depending on the orientation of the
eyes. While this does not allow the target of the person’s gaze to
be identified, it does allow eye orientation to be measured. An
alternative to EOG are camera-based eye trackers, which allow
both eye orientation gaze target to be obtained, but are much more
expensive and require more advanced image processing methods.
An EOG-controlled meal assistance orthosis was developed by
Gotu and Nakamura [138]. They measured horizontal and vertical
EOG signals, lowpass-filtered them, and used the vertical signal to
detect blinks. If no blink was detected, the horizontal value of EOG
was used to select a dish from the table. Additionally, a USB camera
was pointed at the table and recognized the relative positions of
the different dishes, allowing the EOG signal to be matched to an
appropriate dish and the orthosis to be moved toward it. A similar
approach was suggested by Novak and Riener [139] for an arm
rehabilitation exoskeleton that detects the gaze target and moves
the user’s arm to the target, and by Hao et al. [140] for a prosthesis
that adapts its grasp to an object selected via EOG, but all three
studies are very preliminary.
3.4.4. Artificial vision
The grasping function of powered hand prostheses can be controlled using inputs such as EMG. However, a prosthesis equipped

163

with artificial vision could in principle recognize an object in front
of it and automatically adapt the shape of the hand to grasp the
object, reducing the need for voluntary control. Such a system was
presented by Došen et al. [141], who used a camera to choose
among nine combinations of grasp type and size for a prosthetic
hand controlled by healthy subjects. The optimal combination was
chosen in 84% of the trials, and in another 6% of the trials the hand
was still able to grasp the object. In a second technical evaluation,
the system was able to operate in dynamic conditions (while the
hand was moving), but accuracy decreased significantly [142].
3.4.5. Workload recognition
Rehabilitation robots commonly operate in an assist-as-needed
mode where the user should be sufficiently challenged to benefit
from rehabilitation, but should not be overworked so as not
to cause frustration. Workload recognition from physiological
responses such as heart rate, skin conductance or EEG has been
proposed as a method of automatically balancing rehabilitation
task difficulty to ensure maximum rehabilitation benefit.
The approach has so far been tested with two wearable robots,
the Lokomat leg orthosis [143] and the ARMin arm exoskeleton [144]. In the Lokomat, features extracted from autonomic nervous system responses were classified into workload levels using
adaptive LDA. In the ARMin, features extracted from autonomic
nervous system responses and EEG were used to obtain a
continuous workload estimate using random forests. However,
results were mixed: while the workload recognition usefully supplemented other measurements in the Lokomat setup [143], it provided little information in the ARMin setup that could not have
been obtained from nonphysiological measurements [144].
4. Multimodal sensor fusion
Multimodal sensor fusion combines information from different
sensor modalities to overcome the shortcomings of each sensor.
This can be done in different ways, which are illustrated in Fig. 3:
– Single fusion algorithm: Features are extracted for each
modality separately, then input all features into a single sensor
fusion algorithm (Fig. 3(a)).
– Unimodal switching: One modality is used to automatically
detect transitions between operating modes and switch between different sensor fusion algorithms. Each of the sensor algorithms uses only the second modality as the input (Fig. 3(b)).
– Multimodal switching: One modality is used to automatically
detect transitions between operating modes and switch between different sensor fusion algorithms. Each of the sensor algorithms uses multiple modalities as inputs (Fig. 3(c)).
– Mixing: Multiple sensor fusion algorithms run in parallel, each
incorporating one or more modalities. The outputs of these
algorithms are added together (‘mixed’), with the weight of
each output determined based on one modality (Fig. 3(d)).
While the second and third approach are used when operating
modes are easy to distinguish (e.g. discrete gait phases separated
by footswitches), the last approach is used when operating modes
are difficult to distinguish due to, e.g., sensor uncertainty [9] or
partial overlap [145].
In wearable robotics, multimodal sensor fusion is by far the
best explored in the context of fusing EMG and mechanical sensors
(Section 4.1), where examples of all four approaches can be found.
Another frequent combination is eye tracking with either EEG or
EMG (Section 4.2), which is mainly used for upper extremity applications using either triggering or mixing. EEG can also be combined with either EMG or intraneural muscle signals (Section 4.3).
Finally, fusion of more than two modalities is rare, but does exist
(Section 4.4).

164

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

Fig. 3. Four approaches to fusing two modalities in wearable robotics: a single classifier (a), unimodal switching (b), multimodal switching (c) and mixing (d). Illustrated
for classification, but analogously applies to regression.

4.1. EMG and mechanical sensors
EMG commonly suffers from the limb position effect, where
EMG signals for the same motion are different in different limb
positions (either arm postures or phases of a gait cycle). Mechanical
sensors like IMUs can provide information about the limb position,
therefore compensating for the weaknesses of EMG. This is by far
the most common application of EMG-mechanical sensor fusion,
and all four approaches have been used for it.
4.1.1. Single classification or regression algorithm
Several upper limb studies have used a single multimodal fusion
algorithm to compensate for the limb position effect, demonstrating better performance than using either EMG or mechanical sensors alone [146–148]. For the lower limbs, such algorithms have
been used to provide exoskeleton control signals [6] and predict
prosthesis mode transitions for the lower extremities [149], with
the latter study also demonstrating better performance with multimodal rather than unimodal fusion. However, such an approach
is likely to be less effective than unimodal or multimodal switching
(next sections).
4.1.2. Unimodal switching
Unimodal switching is more commonly used for lower limb
wearable robots, where Huang et al. [34] compensated for the limb
position effect by detecting gait phase changes using footswitches
and switching between different EMG classifiers depending on
gait phase. Kawamoto and Sankai [1] also detected different gait
phases using joint angles and footswitches, then switched between
different EMG sensor fusion algorithms, with successful reduction
of muscle activity due to robotic assistance. As an alternative to
automatic switching, the user can utilize one modality to manually
send switching commands to the wearable robot, which then
performs sensor fusion with the second modality. This was done
by Au et al. [150], who used voluntarily generated EMG to send
switching commands to a powered prostheses that then generated
gait patterns based on angle and force data.
For upper limb robots, Fougner et al. [151] and Geng et al. [152]
measured limb positions using accelerometers and switched
between EMG classifiers depending on limb position. Such

switching provided more robust classification of limb motions than
simply inputting both EMG and accelerometer features in the same
classifier [152].
Aside from compensating the limb position effect, Zhang
et al. [153] used such switching for stumble detection. First, an acceleration sensor detects whether or not a stumble has occurred. If
a stumble is detected, an EMG-based classifier is triggered to identify the stumble type. As with the limb position effect compensation, one modality controls the sensor fusion for the other.
4.1.3. Multimodal switching
Multimodal switching of EMG and mechanical sensors has only
been used for gait phase recognition in lower limb robots. The first
example by Huang et al. [7] separated gait into four phases based
on pressure-sensitive insoles, then created a SVM classifier for each
phase, with EMG and load cell data as inputs. Such multimodal
sensor fusion outperformed unimodal fusion in various walking
modes (level ground, stair and ramp ascent/descent), but exhibited
unstable and delayed decisions during mode transitions. For this
reason, an extension of the system incorporated prior knowledge
of the walking terrain into a LDA-based classifier [154]. This
prior knowledge improved performance over the terrain-unaware
version in amputee tests with both powered and unpowered
prostheses. A version of the algorithm was also successfully tested
with a single multiple sclerosis patient in a leg exoskeleton [155].
4.1.4. Mixing algorithm outputs
It can sometimes be difficult to identify precisely when the
sensor fusion system should switch from one algorithm to another.
While gait phases are easy to separate using footswitches, it is
more difficult to separate, for example, arm positions. If we switch
between two EMG classifiers for two defined arm positions, what
should be the output when the arm is halfway between the two
defined positions?
This problem was first addressed for a robotic exoskeleton by
Kiguchi et al. [145], who proposed a three-stage neurofuzzy algorithm. The first stage consists of multiple parallel neurofuzzy
networks that fuse EMG and force signals into robot controller
signals, each network for a particular posture. The second stage
then selects the posture region, fusing the outputs of the different

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

first-stage networks using fuzzy rules into a ‘mixed’ controller signal. Finally, the third neurofuzzy stage maps the controller signal
from the second stage to torques applied to the robotic actuators.
A relatively complex nonlinear sensor fusion approach, its fuzzy
rules allow it to smoothly transition between arm postures.
Later, a similar approach was proposed by Dutta et al. to
deal with different gait modes (level walking, ascent, descent) in
leg prosthetics. A separate Kalman filter with EMG as input was
created for each gait mode. The filters’ outputs are weighted by a
Bayesian inference system according to the probability of each gait
mode, then summed to create a ‘mixture’ of filter outputs [156].
Both probabilistic and fuzzy mixing represent a continuous,
robust sensor fusion approach that has great potential for wearable
robotics. It is also not limited to EMG and mechanical sensors, as we
shall see in the next section.

165

data for leg exoskeletons, though accuracies were not compared.
Finally, Kiguchi and Hayashi [161] evaluated the performance of an
assistive exoskeleton using EMG and EEG. If the EMG applied by the
human is high, the assistance provided is incorrect, and the robot
should change its approach. The study showed that incorrectly
provided assistance can be better recognized using a combination
of EMG and EEG than using just EMG, with improvements in
classification accuracy of up to 25% (e.g. from 60% to 85%) in a twoclass problem.
Alternately, EEG can be used as a trigger for other signals.
Tombini et al. [162] found that the start of a motion is difficult to
identify in signals recorded from intraneural electrodes. They thus
used EEG to identify the start of a motion with the same approach
used for motor imagery (Section 3.2.3) and began windowing
intraneural signals when the start was detected, with a marked
increase in recognition accuracy.

4.2. Eye tracking and EMG/EEG
4.4. EEG, EMG and IMUs
4.2.1. Triggering
Interfaces that utilize eye tracking commonly suffer from the
‘Midas touch’ problem [157]: they can determine where the user
is looking, but do not know if or when the user wishes to interact
with the gaze target. Thus, a robot guided entirely by gaze does not
know when to begin assisting a motion [139]. EEG can be used as a
trigger in such cases: once motor imagery is detected by the EEG,
the robot can assist movement toward the gaze target.
Frisoli et al. [8] implemented this approach with EEG and an eye
tracker, detecting motion onset with an approach similar to motor
imagery detection (Section 3.2.3) and then having an assistive
exoskeleton move the user’s arm to the gaze target. The system was
effective with both healthy and stroke subjects, though only a few
possible gaze targets were included. Onose et al. [158] also used
such a combination of EEG and eye tracker in clinical trials with
chronic tetraplegics and found that it can usefully assist disabled
patients in activities of daily living.
4.2.2. Mixing
As we saw earlier, the limb position effect in EMG analysis can
be partially compensated by using mechanical sensors to identify
the arm posture or gait mode. However, an alternative for upper
limb robots is to predict the endpoint of an arm motion and use this
information to enhance EMG fusion. Since humans generally look
at the target of a reaching motion before and during the motion,
eye tracking can be used to predict the target.
Corbett et al. [9] applied mixing to prediction of arm trajectories
from EMG using a Kalman filter. Multiple potential targets of the
motion were defined, and a separate Kalman filter was created for
each one. An eye tracker then provided a probability distribution
of the potential targets, and the final output trajectory was a
probability-weighted average of the outputs of all Kalman filters.
Incorporating gaze information in this way greatly improved
sensor fusion, particularly for few available EMG signals.
4.3. EEG and EMG or intraneural electrodes
When EMG and EEG are used together to operate wearable
robots, features are generally extracted from the two independently and separate classification/regression model is used for each
modality. The models’ outputs are then mixed together.
Leeb et al. [159] used a Bayesian rule to weigh EEG and
EMG classifiers and produce the final output. While classification
accuracy was 73% with EEG and 87% for EMG (with two classes),
multimodal classification achieved an accuracy of 91% and was
more robust to fatigue. Similarly, Cheron et al. [160] created
separate dynamic neural networks to interpret EEG and EMG

In an advanced implementation of sensor fusion, Gallego
et al. [163] combined EEG, EMG and IMUs to monitor and suppress
tremor using a wearable neurorobot. EEG is first used to detect
an intended motion with the same approach used for motor
imagery (Section 3.2.3). Once onset is detected, EMG is used to
identify voluntary and tremulous activity while IMUs are used to
parametrize tremor and modulate the feedback provided by the
neurorobot. The approach has been successfully tested with five
patients and represents a rare but promising case of more than two
sensor modalities being used to accomplish a larger goal using the
strengths and drawbacks of each modality.
5. Sensor fusion performance evaluation
Regardless of the modality or algorithm, the end goal of sensor
fusion is to send commands to the wearable robot. In offline
analysis, where real interaction with the robot is not possible,
the classic way to evaluate sensor fusion performance is through
classification accuracy (the percentage of correctly assigned class
labels) or regression accuracy (the difference between regression
outputs and actual values). However, higher offline accuracy
does not necessarily translate to better online performance or
usability [55,76], so other performance metrics are required to
truly evaluate the effectiveness of sensor fusion.
Arm prostheses have perhaps the best-developed performance
metrics in wearable robotics, as classification performance has
been evaluated using standardized motor function tests like the
Box and Block Test [55] as well as tests created specifically for
prostheses, such as the Target Achievement Control test [164].
These tests have shown that classification-based control outperforms the on–off control that is still used in many commercial prostheses [165]. Even without standardized tests, real-time
performance of a prosthesis can be evaluated as the time needed to
complete a task or the percentage of successfully completed tasks
in both arm [38,76] and leg prosthetics [166].
Exoskeletons, on the other hand, assist rather than replace a
limb. Performance metrics have therefore focused not on task
completion time, but on whether the limb expends less energy
during a task when using the exoskeleton. The most common
performance metric is the reduction in EMG activity [60,61,66,167]
or overall metabolic cost [168–170] with an active exoskeleton
compared to a passive one. Studies of wearable robots in rehabilitation also compare functional parameters such as human range of
motion before and after the rehabilitation process [58], but these
parameters depend on so many factors that they cannot truly be
used to evaluate sensor fusion effectiveness.

166

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

Finally, many studies perform tests on healthy volunteers and
assume that results will generalize to impaired subjects. This is,
however, not guaranteed and depends greatly on the impairment.
Perhaps the most instructive example of this is EMG-based prosthesis control. While some studies have found essentially no differences between healthy subjects and amputees when operating
a prosthesis with EMG [171], amputees are not a homogeneous
group [38,172], and residual muscles do not necessarily provide
enough information for complex prosthesis control [18]. EMG may
also not be practical for robot control in stroke victims [40], and
developers should therefore test on the target population in early
stages of development.
6. Summary and conclusions
6.1. From single modalities to multimodality
There is at least some sensor fusion overlap between modalities
and applications. For example, both EMG and brain measurements
first used basic classification approaches based on machine
learning, both eventually advanced toward regression to decode
movement trajectories, and both are aware of the growing need
for adaptive sensor fusion methods to take factors such as fatigue
and sensor fault into account. Nonetheless, it is still very common
for studies to focus on a single measurement modality (e.g. EEG)
and use a single classifier or regression function.
We expect that such simple unimodal fusion will become less
and less common as designers realize the importance of switching and mixing fusion methods. Wearable robots will probably
only achieve widespread real-world use if they can automatically
switch between possible operating modes and have several classification or regression algorithms running in parallel to control
different parts of the robots. At the moment, the most practical
emphasis should be placed on using the robot’s sensors as well as
other mechanical sensors to switch between different sensor fusion algorithms for, e.g., EMG, as this represents a relatively ‘‘easy’’
approach that nonetheless has the potential to greatly improve
robot performance.
6.2. Features and classifiers
Regardless of sensing modality or application, it has become
clear that it is no longer sufficient to evaluate sensor fusion purely
offline and focus on differences between individual classifiers or
features. These differences are, in most cases, small and depend
on so many other factors that best practice guidelines would
be extremely difficult to identify. We do not wish to dissuade
developers from testing several different classifiers to obtain the
best one for their application, but unless the differences are truly
large, they may not be generalizable to other wearable robots.
6.3. Online performance evaluation
Rather than focusing on offline comparison of different parameters, we should instead focus on online evaluations. Differences
between online and offline sensor fusion accuracy can be very significant, and parameters (e.g. window length) that work well in offline evaluation may require different settings online. We should
also no longer focus on just sensor fusion accuracy; as several
studies have shown, differences in classification accuracy do not
necessary translate to changes in practical performance. Tests of
functional motor ability would provide a more realistic evaluation for sensor fusion methods, and would also allow modalities to
be compared; for example, to determine whether adding an EEGbased trigger signal to an exoskeleton would actually improve performance enough to justify the additional sensors.

6.4. The next generation of prosthetics and orthotics
The new generation of commercial arm prostheses will likely
utilize a switching algorithm to change between possible operating
modes and will have several classification algorithms running
in parallel to control different parts of the arm with discrete
commands. A very useful addition to some prostheses may be a
prosthesis-mounted camera to recognize the shape of graspable
objects, which would greatly simplify the control problem at the
expense of less flexibility.
On the other hand, exoskeletons and leg prostheses have focused more strongly on regression approaches, estimating desired
joint torques and other control variables in a continuous fashion.
For a single degree of freedom, this could in principle be done with
very simple proportional control, but more complex methods like
neuromuscular models or machine learning techniques are necessary for multijoint control. As EEG does not have sufficient signal
quality for regression, the main control modalities for exoskeletons
and leg prostheses are EMG and mechanical sensors.
6.5. Final words
At the moment, the key topics of sensor fusion research in wearable robotics are multimodality, adaptability and performance
evaluation. Essentially, we should focus on multimodal sensor fusion and take advantage of adaptive classifiers as well as switching and mixing approaches. Furthermore, we should strive to test
sensor fusion methods online using practical performance metrics
rather than simply classification accuracy. Thankfully, the community seems to be well-aware of the challenges, and steady work is
being done in all aspects. One final thing to remember, however,
is that developers of exoskeletons should not ignore sensor fusion
work being done in prosthetics, and vice-versa—different types of
wearable robots may have more similarities than differences when
it comes to sensor fusion!
Acknowledgements
This work was supported by the Swiss National Science Foundation through the National Centre of Competence in Research
Robotics and by the Clinical Research Priority Program ‘‘NeuroRehab’’ University of Zurich.
References
[1] H. Kawamoto, Y. Sankai, Power assist method based on phase sequence and
muscle force condition for HAL, Adv. Robot. 19 (7) (2005) 717–734.
[2] L.R. Hochberg, D. Bacher, B. Jarosiewicz, N.Y. Masse, J.D. Simeral, J. Vogel, S.
Haddadin, J. Liu, S.S. Cash, P. van der Smagt, J.P. Donoghue, Reach and grasp
by people with tetraplegia using a neurally controlled robotic arm, Nature
485 (7398) (2012) 372–375.
[3] M. Goldfarb, B.E. Lawson, A.H. Shultz, Realizing the promise of robotic leg
prostheses, Sci. Transl. Med. 5 (210) (2013) 210ps15.
[4] P. Heo, G.M. Gu, S. Lee, K. Rhee, J. Kim, Current hand exoskeleton technologies
for rehabilitation and assistive engineering, Int. J. Precis. Eng. Manuf. 13 (5)
(2012) 807–824.
[5] R.G.E. Clement, K.E. Bugler, C.W. Oliver, Bionic prosthetic hands: a review of
present technology and future aspirations, Surgeon 9 (6) (2011) 336–340.
[6] Y.H. Yin, Y.J. Fan, L.D. Xu, EMG and EPP-integrated human–machine interface
between the paralyzed and rehabilitation exoskeleton, IEEE Trans. Inf.
Technol. Biomed. 16 (4) (2012) 542–549.
[7] H. Huang, F. Zhang, L.J. Hargrove, Z. Dou, D.R. Rogers, K.B. Englehart,
Continuous locomotion-mode identification for prosthetic legs based on
neuromuscular-mechanical fusion, IEEE Trans. Biomed. Eng. 58 (10) (2011)
2867–2875.
[8] A. Frisoli, C. Loconsole, D. Leonardis, F. Banno, M. Barsotti, C. Chisari, M.
Bergamasco, A new gaze-BCI-driven control of an upper limb exoskeleton for
rehabilitation in real-world tasks, IEEE Trans. Syst. Man Cybern. Part C Appl.
Rev. 42 (6) (2012) 1169–1179.
[9] E. Corbett, K. Körding, E. Perreault, Real-time evaluation of a noninvasive
neuroprosthetic interface for control of reach, IEEE Trans. Neural Syst.
Rehabil. Eng. 21 (4) (2013) 674–683.

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170
[10] P.R. De Almeida Ribeiro, F.L. Brasil, M. Witkowski, F. Shiman, C. Cipriani,
N. Vitiello, M.C. Carrozza, S.R. Soekadar, Controlling assistive machines
in paralysis using brain waves and other biosignals interfaces: a general
overview, Adv. Human–Comput. Interact. 2013 (369425) (2013).
[11] E. Scheme, K. Englehart, Electromyogram pattern recognition for control of
powered upper-limb prostheses: state of the art and challenges for clinical
use, J. Rehabil. Res. Dev. 48 (6) (2011) 643–660.
[12] J.G. Proakis, D.K. Manolakis, Digital Signal Processing, fourth ed., Prentice
Hall, 2006.
[13] C.M. Bishop, Pattern Recognition and Machine Learning, first ed., Springer
Science+Business Media, New York, USA, 2006.
[14] C. Fleischer, G. Hommel, A human–exoskeleton interface utilizing electromyography, IEEE Trans. Robot. 24 (4) (2008) 872–882.
[15] A. Fougner, Ø. Stavdahl, P.J. Kyberd, Y.G. Losier, P.A. Parker, Control of
upper limb prostheses: terminology and proportional myoelectric control—a
review, IEEE Trans. Neural Syst. Rehabil. Eng. 20 (5) (2012) 663–677.
[16] J. Rosen, M. Brand, M.B. Fuchs, M. Arcan, A myosignal-based powered
exoskeleton system, IEEE Trans. Syst. Man Cybern. A 31 (3) (2001) 210–222.
[17] H.J. Hermens, B. Frerisk, R. Merletti, D. Stegeman, J. Blok, G. Rau,
C. Disselhorst-Klug, G. Hägg, European Recommendations for Surface
Electromyography: Results of the SENIAM project, Roessingh Research and
Development, Enschede, Netherlands, 1999.
[18] G. Li, A.E. Schultz, T.A. Kuiken, Quantifying pattern recognition-based
myoelectric control of multifunctional transradial prostheses, IEEE Trans.
Neural Syst. Rehabil. Eng. 18 (2) (2010) 185–192.
[19] P. Zhou, M.M. Lowery, K.B. Englehart, H. Huang, G. Li, L. Hargrove, J.P.A.
Dewald, T.A. Kuiken, Decoding a new neural-machine interface for control
of artificial limbs, J. Neurophysiol. 98 (2007) 2974–2982.
[20] G. Li, Y. Li, L. Yu, Y. Geng, Conditioning and sampling issues of EMG signals in
motion recognition of multifunctional myoelectric prostheses, Ann. Biomed.
Eng. 39 (6) (2011) 1779–1787.
[21] L. Hargrove, P. Zhou, K. Englehart, T.A. Kuiken, The effect of ECG interference
on pattern-recognition-based myoelectric control for targeted muscle
reinnervated patients, IEEE Trans. Biomed. Eng. 56 (9) (2009) 2197–2201.
[22] P. Zhou, B. Lock, T.A. Kuiken, Real time ECG artifact removal for myoelectric
prosthesis control, Physiol. Meas. 28 (4) (2007) 397–413.
[23] L.H. Smith, L.J. Hargrove, B.A. Lock, T.A. Kuiken, Determining the optimal
window length for pattern recognition-based myoelectric control: balancing
the competing effects of classification error and controller delay, IEEE Trans.
Neural Syst. Rehabil. Eng. 19 (2) (2011) 186–192.
[24] T.R. Farrell, R.F. Weir, The optimal controller delay for myoelectric prostheses,
IEEE Trans. Neural Syst. Rehabil. Eng. 15 (1) (2007) 111–118.
[25] B.S. Hudgins, P.A. Parker, R.N. Scott, A new strategy for multifunction
myoelectric control, IEEE Trans. Biomed. Eng. 40 (1993) 82–94.
[26] T.R. Farrell, R.F.F. Weir, A comparison of the effects of electrode implantation
and targeting on pattern classification accuracy for prosthesis control, IEEE
Trans. Biomed. Eng. 55 (9) (2008) 2198–2211.
[27] J. Liu, P. Zhou, A novel myoelectric pattern recognition strategy for hand
function restoration after incomplete cervical spinal cord injury, IEEE Trans.
Neural Syst. Rehabil. Eng. 21 (1) (2013) 96–103.
[28] L. Hargrove, K. Englehart, B. Hudgins, A training strategy to reduce
classification degradation due to electrode displacements in pattern
recognition based myoelectric control, Biomed. Signal Process. Control 3 (2)
(2008) 175–180.
[29] A.J. Young, L.J. Hargrove, T.A. Kuiken, Improving myoelectric pattern
recognition robustness to electrode shift by changing interelectrode distance
and electrode configuration, IEEE Trans. Biomed. Eng. 59 (3) (2012) 645–652.
[30] J.-U. Chu, I. Moon, Y.-J. Lee, S.-K. Kim, M.-S. Mun, A supervised featureprojection-based real-time EMG pattern recognition for multifunction
myoelectric hand control, IEEE/ASME Trans. Mechatronics 12 (3) (2007)
282–290.
[31] K. Englehart, B. Hudgins, P.A. Parker, A wavelet-based continuous classification scheme for multifunction myoelectric control, IEEE Trans. Biomed. Eng.
48 (3) (2001) 302–311.
[32] J. Shi, Y. Cai, J. Zhu, J. Zhong, F. Wang, SEMG-based hand motion recognition
using cumulative residual entropy and extreme learning machine, Med. Biol.
Eng. Comput. 51 (4) (2013) 302–311.
[33] Z. Ju, G. Ouyang, M. Wilamowska-Korsak, H. Liu, Surface EMG based
hand manipulation identification via nonlinear feature extraction and
classification, IEEE Sens. J. 13 (9) (2013) 3302–3311.
[34] H. Huang, T.A. Kuiken, R.D. Lipschutz, A strategy for identifying locomotion
modes using surface electromyography, IEEE Trans. Biomed. Eng. 56 (1)
(2009) 65–73.
[35] J.M. Hahne, B. Graimann, K.-R. Müller, Spatial filtering for robust myoelectric
control, IEEE Trans. Biomed. Eng. 59 (5) (2012) 1436–1443.
[36] A.H. Al-Timemy, G. Bugmann, J. Escudero, N. Outram, Classification of
finger movements for the dexterous hand prosthesis control with surface
electromyography, IEEE J. Biomed. Health Inform. 17 (3) (2013) 608–618.
[37] C. Castellini, A.E. Fiorilla, G. Sandini, Multi-subject/daily-life activity EMGbased control of mechanical hands, J. Neuroeng. Rehabil. 6 (41) (2009).
[38] C. Cipriani, C. Antfolk, M. Controzzi, G. Lundborg, B. Rosen, M.C. Carrozza,
F. Sebelius, Online myoelectric control of a dexterous hand prosthesis by
transradial amputees, IEEE Trans. Neural Syst. Rehabil. Eng. 19 (3) (2011)
260–270.

167

[39] M. Khezri, M. Jahed, A neuro-fuzzy inference system for sEMG-based
identification of hand motion commands, IEEE Trans. Ind. Electron. 58 (5)
(2011) 1952–1960.
[40] B. Cesqui, P. Tropea, S. Micera, H.I. Krebs, EMG-based pattern recognition
approach in post stroke robot-aided rehabilitation: a feasibility study,
J. Neuroeng. Rehabil. 10 (75) (2013).
[41] L.J. Hargrove, G. Li, K.B. Englehart, B.S. Hudgins, Principal components
analysis preprocessing for improved classification accuracies in patternrecognition-based myoelectric control, IEEE Trans. Biomed. Eng. 56 (5) (2009)
1407–1414.
[42] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Recognition, second ed., WileyInterscience, New York, 2001.
[43] D. Novak, M. Mihelj, M. Munih, A survey of methods for data fusion
and system adaptation using autonomic nervous system responses in
physiological computing, Interact. Comput. 24 (2012) 154–172.
[44] P. Geethanjali, K.K. Ray, Identification of motion from multi-channel EMG
signals for control of prosthetic hand, Australas. Phys. Eng. Sci. Med. 34 (3)
(2011) 419–427.
[45] M.V. Liarokapis, P.K. Artemiadis, P.T. Katsiaris, K.J. Kyriakopoulos, E.S.
Manolakos, Learning human reach-to-grasp strategies: towards EMG-based
control of robotic arm-hand systems, in: 2012 IEEE Int. Conf. Robot. Autom.,
May 2012, pp. 419–427.
[46] E.J. Scheme, K.B. Englehart, B.S. Hudgins, Selective classification for improved
robustness of myoelectric control under nonideal conditions, IEEE Trans.
Biomed. Eng. 58 (6) (2011) 1698–1705.
[47] J.W. Sensinger, B.A. Lock, T.A. Kuiken, Adaptive pattern recognition of
myoelectric signals: exploration of conceptual framework and practical
algorithms, IEEE Trans. Neural Syst. Rehabil. Eng. 17 (3) (2009) 270–278.
[48] O. Fukuda, T. Tsuji, M. Kaneko, A. Otsuka, A human-assisting manipulator
teleoperated by EMG signals and arm motions, IEEE Trans. Robot. Autom. 19
(2) (2003) 210–222.
[49] H. Huang, F. Zhang, Y.L. Sun, H. He, Design of a robust EMG sensing interface
for pattern classification, J. Neural Eng. 7 (056005) (2010).
[50] X. Zhang, H. Huang, Q. Yang, Real-time implementation of a self-recovery
EMG pattern recognition interface for artificial arms, in: Proceedings of the
2013 Annual International Conference of the IEEE Engineering in Medicine
and Biology Society, 2013, pp. 5926–5929.
[51] T. Tommasi, F. Orabona, C. Castellini, B. Caputo, Improving control of
dexterous hand prostheses using adaptive learning, IEEE Trans. Robot. 29 (1)
(2013) 207–219.
[52] T. Matsubara, S.-H. Hyon, J. Morimoto, Learning and adaptation of a stylistic
myoelectric interface: EMG-based robotic control with individual user
differences, in: Proceedings of the 2011 IEEE International Conference on
Robotics and Biomimetics, 2011, pp. 390–395.
[53] J.J. Baker, E. Scheme, K. Englehart, D.T. Hutchinson, B. Greger, Continuous
detection and decoding of dexterous finger flexions with implantable
myoelectric sensors, IEEE Trans. Neural Syst. Rehabil. Eng. 18 (4) (2010)
424–432.
[54] A.J. Young, L.H. Smith, E.J. Rouse, L.J. Hargrove, Classification of simultaneous
movements using surface EMG pattern recognition, IEEE Trans. Biomed. Eng.
60 (5) (2013) 1250–1258.
[55] L.J. Hargrove, E.J. Scheme, K.B. Englehart, B.S. Hudgins, Multiple binary
classifications via linear discriminant analysis for improved controllability
of a powered prosthesis, IEEE Trans. Neural Syst. Rehabil. Eng. 18 (1) (2010)
49–57.
[56] K. Englehart, B. Hudgins, A robust, real-time control scheme for multifunction
myoelectric control, IEEE Trans. Neural Syst. Rehabil. Eng. 50 (7) (2003)
848–854.
[57] A.M. Simon, L.J. Hargrove, B.A. Lock, T.A. Kuiken, A decision-based velocity
ramp for minimizing the effect of misclassifications during real-time pattern
recognition control, IEEE Trans. Biomed. Eng. 58 (8) (2011) 848–854.
[58] R. Song, K. Tong, X. Hu, W. Zhou, Myoelectrically controlled wrist robot for
stroke rehabilitation, J. Neuroeng. Rehabil. 10 (52) (2013).
[59] S.M. Cain, K.E. Gordon, D.P. Ferris, Locomotor adaptation to a powered anklefoot orthosis depends on control method, J. Neuroeng. Rehabil. 4 (48) (2007).
[60] C.R. Kinnaird, D.P. Ferris, Medial gastrocnemius myoelectric control of a
robotic ankle exoskeleton, IEEE Trans. Neural Syst. Rehabil. Eng. 17 (1) (2009)
31–37.
[61] K. Kiguchi, M.H. Rahman, M. Sasaki, K. Teramoto, Development of a 3DOF
mobile exoskeleton robot for human upper-limb motion assist, Robot. Auton.
Syst. 56 (2008) 678–691.
[62] N. Jiang, H. Rehbaum, I. Vujaklija, B. Graimann, D. Farina, Intuitive, online,
simultaneous and proportional myoelectric control over two degrees of
freedom in upper limb amputees, IEEE Trans. Neural Syst. Rehabil. Eng. 22
(3) (2014) 501–510.
[63] J.L.G. Nielsen, S. Holmgaard, N. Jiang, K.B. Englehart, D. Farina, S. Member, P.A.
Parker, Simultaneous and proportional force estimation for multifunction
myoelectric prostheses using mirrored bilateral training, IEEE Trans. Biomed.
Eng. 58 (3) (2011) 681–688.
[64] P.K. Artemiadis, K.J. Kyriakopoulos, A switching regime model for the EMGbased control of a robot arm, IEEE Trans. Syst. Man Cybern. B 41 (1) (2011)
53–63.
[65] T. Hayashi, H. Kawamoto, Y. Sankai, Control method of robot suit HAL
working as operator’s muscle using biological and dynamical information, in:
2005 IEEE/RSJ Int. Conf. Intell. Robot. Syst., Vol. 2, No. 1, 2005, pp. 3063–3068.
[66] T. Lenzi, S.M.M. De Rossi, N. Vitiello, M.C. Carrozza, Intention-based EMG
control for powered exoskeletons, IEEE Trans. Biomed. Eng. 59 (8) (2012)
2180–2190.

168

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170

[67] A.V. Hill, The heat of shortening and the dynamic constants of muscle, Proc.
R. Soc. Lond. B Biol. Sci. 126 (1938) 136–195.
[68] E.E. Cavallaro, J. Rosen, J.C. Perry, S. Burns, Real-time myoprocessors for a
neural controlled powered exoskeleton arm, IEEE Trans. Biomed. Eng. 53 (11)
(2006) 2387–2396.
[69] W. Hassani, S. Mohammed, Y. Amirat, Real-time EMG driven lower
limb actuated orthosis for assistance as needed movement strategy, in:
Proceedings of the 2013 Robotics: Science and Systems Conference, 2013.
[70] S.-K. Wu, G. Waycaster, X. Shen, Electromyography-based control of active
above-knee prostheses, Control Eng. Pract. 19 (8) (2011) 875–882.
[71] J. Rosen, M.B. Fuchs, M. Arcan, Performances of Hill-type and neural network
muscle models—toward a myosignal-based exoskeleton, Comput. Biomed.
Res. 32 (5) (1999) 415–439.
[72] M. Sartori, M. Reggiani, E. Pagello, D.G. Lloyd, Modeling the human knee for
assistive technologies, IEEE Trans. Biomed. Eng. 59 (9) (2012) 2642–2649.
[73] N. Jiang, J.L.G. Vest-Nielsen, S. Muceli, D. Farina, EMG-based simultaneous and
proportional estimation of wrist/hand kinematics in uni-lateral trans-radial
amputees, J. Neuroeng. Rehabil. 9 (42) (2012).
[74] C.L. Pulliam, J.M. Lambrecht, R.F. Kirsch, EMG-based neural network control
of transhumeral prostheses, J. Rehabil. Res. Dev. 48 (6) (2011) 739–754.
[75] S.K. Au, P. Bonato, H. Herr, An EMG-position controlled system for an active
ankle-foot prosthesis: An initial experimental study, in: Proceedings of the
9th International Conference on Rehabilitation Robotics, 2005, pp. 375–379.
[76] N. Jiang, I. Vujaklija, H. Rehbaum, B. Graimann, D. Farina, Is accurate mapping
of EMG signals on kinematics needed for precise online myoelectric control?
IEEE Trans. Neural Syst. Rehabil. Eng. 22 (3) (2014) 549–558.
[77] J.M. Hahne, H. Rehbaum, F. Biessmann, F.C. Meinecke, K.-R. Müller, N. Jiang,
D. Farina, L.C. Parra, Simultaneous and proportional control of 2D wrist
movements with myoelectric signals, in: 2012 IEEE International Workshop
on Machine Learning for Signal Processing, 2012.
[78] N. Jiang, K.B. Englehart, P.A. Parker, Extracting simultaneous and proportional
neural control information for multiple-DOF prostheses from the surface
electromyographic signal, IEEE Trans. Biomed. Eng. 56 (4) (2009) 1070–1080.
[79] P.K. Artemiadis, K.J. Kyriakopoulos, An EMG-based robot control scheme
robust to time-varying EMG signal features, IEEE Trans. Inf. Technol. Biomed.
14 (3) (2010) 582–588.
[80] R. Ortner, B.Z. Allison, G. Korisek, H. Gaggl, G. Pfurtscheller, An SSVEP BCI to
control a hand orthosis for persons with tetraplegia, IEEE Trans. Neural Syst.
Rehabil. Eng. 19 (1) (2011) 1–5.
[81] J.L. Collinger, B. Wodlinger, J.E. Downey, W. Wang, E.C. Tyler-Kabara, D.J.
Weber, A.J.C. McMorland, M. Velliste, M.L. Boninger, A.B. Schwartz, Highperformance neuroprosthetic control by an individual with tetraplegia,
Lancet 381 (2013) 557–564.
[82] D. Broetz, C. Braun, C. Weber, S.R. Soekadar, A. Caria, N. Birbaumer,
Combination of brain–computer interface training and goal-directed physical
therapy in chronic stroke: a case report, Neurorehabil. Neural Repair 24 (7)
(2010) 674–679.
[83] A. Ramos-Murguialday, D. Broetz, M. Rea, L. Läer, Ö. Yilmaz, F.L. Brasil, G.
Liberati, M.R. Curado, E. Garcia-Cossio, A. Vyziotis, W. Cho, M. Agostini, E.
Soares, S.R. Soekadar, A. Caria, L.G. Cohen, N. Birbaumer, Brain–machine
interface in chronic stroke rehabilitation: a controlled study, Ann. Neurol. 74
(1) (2013) 100–108.
[84] L.F. Nicolas-Alonso, J. Gomez-Gil, Brain computer interfaces, a review,
Sensors 12 (2) (2012) 1211–1279.
[85] P. Horki, T. Solis-Escalante, C. Neuper, G. Müller-Putz, Combined motor
imagery and SSVEP based BCI control of a 2 DoF artificial upper limb, Med.
Biol. Eng. Comput. 49 (5) (2011) 567–577.
[86] T. Sakurada, T. Kawase, K. Takano, T. Komatsu, K. Kansaku, A BMI-based
occupational therapy assist suit: asynchronous control by SSVEP, Front.
Neurosci. 7 (172) (2013).
[87] C.J. Bell, P. Shenoy, R. Chalodhorn, R.P.N. Rao, Control of a humanoid robot
by a noninvasive brain–computer interface in humans, J. Neural Eng. 5 (2)
(2008) 214–220.
[88] M. Duvinage, T. Castermans, R. Jimenez-Fabian, T. Hoellinger, C. De Saedeleer,
M. Petieau, K. Seetharaman, G. Cheron, O. Verlinden, T. Dutoit, A five-state
P300-based foot lifter orthosis: proof of concept, in: 2012 ISSNIP Biosignals
and Biorobotics Conference, 2012, pp. 1–6.
[89] Y. Chae, J. Jeong, S. Jo, Toward brain-actuated humanoid robots: asynchronous direct control using an EEG-based BCI, IEEE Trans. Robot. 28 (5)
(2012) 1131–1144.
[90] G. Pfurtscheller, T. Solis-Escalante, R. Ortner, P. Linortner, G.R. MüllerPutz, Self-paced operation of an SSVEP-Based orthosis with and without an
imagery-based ‘brain switch’: a feasibility study towards a hybrid BCI, IEEE
Trans. Neural Syst. Rehabil. Eng. 18 (4) (2010) 409–414.
[91] A. Kilicarslan, S. Prasad, R.G. Grossman, J.L. Contreras-Vidal, High accuracy
decoding of user intentions using EEG to control a lower-body exoskeleton,
in: Proceedings of the 35th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society, 2013, pp. 5606–5609.
[92] A.H. Do, P.T. Wang, C.E. King, S.N. Chun, Z. Nenadic, Brain–computer interface
controlled robotic gait orthosis, J. Neuroeng. Rehabil. 10 (111) (2013).
[93] B. Xu, S. Peng, A. Song, R. Yang, L. Pan, Robot-aided upper-limb rehabilitation
based on motor imagery EEG, Int. J. Adv. Robot. Syst. 8 (4) (2011) 88–97.
[94] M. Rohm, M. Schneiders, C. Müller, A. Kreilinger, V. Kaiser, G.R. Müller-Putz,
R. Rupp, Hybrid brain–computer interfaces and hybrid neuroprostheses for
restoration of upper limb functions in individuals with high-level spinal cord
injury, Artif. Intell. Med. 59 (2) (2013) 133–142.

[95] R. Zimmermann, L. Marchal-Crespo, J. Edelmann, O. Lambercy, M.-C. Fluet,
R. Riener, M. Wolf, R. Gassert, Detection of motor execution using a hybrid
fNIRS-biosignal BCI: a feasibility study, J. Neuroeng. Rehabil. 10 (4) (2013).
[96] T. Milekovic, J. Fischer, T. Pistohl, J. Ruescher, A. Schulze-Bonhage, A.
Aertsen, J. Rickert, T. Ball, C. Mehring, An online brain–machine interface
using decoding of movement direction from the human electrocorticogram,
J. Neural Eng. 9 (046003) (2012).
[97] Z.C. Chao, Y. Nagasaka, N. Fujii, Long-term asynchronous decoding of arm
motion using electrocorticographic signals in monkeys, Front. Neuroeng. 3
(3) (2010).
[98] R. Vinjamuri, D.J. Weber, Z. Mao, J.L. Collinger, A.D. Degenhart, J.W. Kelly, M.L.
Boninger, E.C. Tyler-Kabara, W. Wang, Toward synergy-based brain–machine
interfaces, IEEE Trans. Inf. Technol. Biomed. 15 (5) (2011) 726–736.
[99] T. Pistohl, A. Schulze-Bonhage, A. Aertsen, C. Mehring, T. Ball, Decoding
natural grasp types from human ECoG, Neuroimage 59 (1) (2012) 248–260.
[100] Z. Wang, Q. Ji, K.J. Miller, G. Schalk, Prior knowledge improves decoding of
finger flexion from electrocorticographic signals, Front. Neurosci. 5 (127)
(2011).
[101] T. Yanagisawa, M. Hirata, Y. Saitoh, H. Kishima, K. Matsushita, T. Goto, R.
Fukuma, H. Yokoi, Y. Kamitani, T. Yoshimine, Electrocorticographic control of
a prosthetic arm by paralyzed patients, Ann. Neurol. 71 (3) (2012) 353–361.
[102] C.A. Chestek, V. Gilja, C.H. Blabe, B.L. Foster, K.V. Shenoy, J. Parvizi, J.M.
Henderson, Hand posture classification using electrocorticography signals
in the gamma band over human sensorimotor brain areas, J. Neural Eng. 10
(026002) (2013).
[103] Y. Nakanishi, T. Yanagisawa, D. Shin, R. Fukuma, C. Chen, H. Kambara, N.
Yoshimura, M. Hirata, T. Yoshimine, Y. Koike, Prediction of three-dimensional
arm trajectories based on ECoG signals recorded from human sensorimotor
cortex, PLoS One 8 (8) (2013).
[104] L.R. Hochberg, M.D. Serruya, G.M. Friehs, J.A. Mukand, M. Saleh, A.H. Caplan,
A. Branner, D. Chen, R.D. Penn, J.P. Donoghue, Neuronal ensemble control of
prosthetic devices by a human with tetraplegia, Nature 442 (7099) (2006)
164–171.
[105] J.P. Cunningham, P. Nuyujukian, V. Gilja, C.A. Chestek, S.I. Ryu, K.V. Shenoy,
A closed-loop human simulator for investigating the role of feedback control
in brain–machine interfaces, J. Neurophysiol. 105 (2011) 1932–1949.
[106] Z. Li, J.E. O’Doherty, M.A. Lebedev, M.A.L. Nicolelis, Adaptive decoding
for brain–machine interfaces through Bayesian parameter updates, Neural
Comput. 23 (12) (2011) 3162–3204.
[107] C.A. Chestek, V. Gilja, P. Nuyujukian, J.D. Foster, J.M. Fan, M.T. Kaufman, M.M.
Churchland, Z. Rivera-Alvidrez, J.P. Cunningham, S.I. Ryu, K.V. Shenoy, Longterm stability of neural prosthetic control signals from silicon cortical arrays
in rhesus macaque motor cortex, J. Neural Eng. 8 (045005) (2011).
[108] S.M. Chase, A.B. Schwartz, R.E. Kass, Bias, optimal linear estimation, and the
differences between open-loop simulation and closed-loop performance of
spiking-based brain–computer interface algorithms, Neural Netw. 22 (2009)
1203–1213.
[109] M. Guidali, U. Keller, V. Klamroth-Marganska, T. Nef, R. Riener, Estimating the
patient’s contribution during robot-assisted therapy, J. Rehabil. Res. Dev. 50
(2013) 379–394.
[110] S.M.M. De Rossi, N. Vitiello, T. Lenzi, R. Ronsse, B. Koopman, A. Persichetti,
F. Vecchi, A.J. Ijspeert, H. van der Kooij, M.C. Carrozza, Sensing pressure distribution on a lower-limb exoskeleton physical human–machine interface,
Sensors 11 (1) (2011) 207–227.
[111] L. Marchal-Crespo, D.J. Reinkensmeyer, Review of control strategies for
robotic movement training after neurologic injury, J. Neuroeng. Rehabil. 6
(20) (2009).
[112] R. Jimenez-Fabian, O. Verlinden, Review of control algorithms for robotic
ankle systems in lower-limb orthoses, prostheses and exoskeletons, Med.
Eng. Phys. 34 (2012) 397–408.
[113] K. Suzuki, G. Mito, H. Kawamoto, Y. Sankai, Intention-based walking support
for paraplegia patients with Robot Suit HAL, Adv. Robot. 21 (12) (2007)
1441–1469.
[114] H.A. Varol, F. Sup, M. Goldfarb, Multiclass real-time intent recognition of
a powered lower limb prosthesis, IEEE Trans. Biomed. Eng. 57 (3) (2010)
542–551.
[115] A. Mannini, A.M. Sabatini, Machine learning methods for classifying human
physical activity from on-body accelerometers, Sensors 10 (2) (2010)
1154–1175.
[116] A. Mannini, A.M. Sabatini, Computational methods for the automatic
classification of postures and movements from acceleration data, Gait
Posture 30 (2009) S68–S69.
[117] S. Crea, M. Donati, S.M.M. De Rossi, C.M. Oddo, N. Vitiello, A wireless flexible
sensorized insole for gait analysis, Sensors 14 (1) (2014) 1073–1093.
[118] C. Caltran, A.A.G. Siqueira, Sensor fusion applied to position estimation and
gait identification of an ankle foot orthosis, in: Biosignals and Biorobotics
Conference, 2011.
[119] E.C. Wentink, V.G.H. Schut, E.C. Prinsen, J.S. Rietman, P.H. Veltink, Detection of
the onset of gait initiation using kinematic sensors and EMG in transfemoral
amputees, Gait Posture 39 (1) (2014) 391–396.
[120] D. Novak, P. Reberšek, S.M.M. De Rossi, M. Donati, J. Podobnik, T. Beravs,
T. Lenzi, N. Vitiello, M.C. Carrozza, M. Munih, Automated detection of gait
initiation and termination using wearable sensors, Med. Eng. Phys. 35 (12)
(2013) 1713–1720.

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170
[121] Y.D. Li, E.T. Hsiao-Wecksler, Gait mode recognition and control for a portablepowered ankle-foot orthosis, in: 2013 IEEE International Conference on
Rehabilitation Robotics, 2013.
[122] E. Zheng, L. Wang, Y. Luo, K. Wei, Q. Wang, Non-contact capacitance sensing
for continuous locomotion mode recognition: design specifications and
experiments with an amputee, in: 2013 IEEE International Conference on
Rehabilitation Robotics, 2013.
[123] X. Zhang, D. Wang, Q. Yang, H. Huang, An automatic and user-driven training
method for locomotion mode recognition for artificial leg control, in: 34th
Annual International Conference of the IEEE EMBS, 2012, pp. 6116–6119.
[124] K. Yuan, S. Sun, Z. Wang, Q. Wang, L. Wang, A fuzzy logic based terrain
identification approach to prosthesis control using multi-sensor fusion, in:
2013 IEEE International Conference on Robotics and Automation, 2013, pp.
3376–3381.
[125] B.E. Lawson, H.A. Varol, M. Goldfarb, Standing stability enhancement with an
intelligent powered transfemoral prosthesis, IEEE Trans. Biomed. Eng. 58 (9)
(2011) 2617–2624.
[126] Y. Hayashi, K. Kiguchi, A lower-limb power-assist robot with perceptionassist, in: 2011 IEEE International Conference on Rehabilitation Robotics,
2011.
[127] D. Grimes, W.C. Flowers, M. Donath, Feasibility of an active control scheme
for A/K prostheses, J. Biomech. Eng. 99 (1977) 215–221.
[128] H. Vallery, E.H.F. van Asseldonk, M. Buss, H. van der Kooij, Reference
trajectory generation for rehabilitation robots: complementary limb motion
estimation, IEEE Trans. Neural Syst. Rehabil. Eng. 17 (1) (2009) 23–30.
[129] H. Vallery, R. Burgkart, C. Hartmann, J. Mitternacht, R. Riener, M. Buss,
Complementary limb motion estimation for the control of active knee
prostheses, Biomed. Technol. 56 (1) (2011) 45–51.
[130] T. Beravs, P. Reberšek, D. Novak, J. Podobnik, M. Munih, Development and
validation of a wearable inertial measurement system for use with lower
limb exoskeletons, in: 11th IEEE-RAS International Conference on Humanoid
Robots, 2011, pp. 212–217.
[131] M. Hassan, H. Kadone, K. Suzuki, Y. Sankai, Wearable gait measurement
system with an instrumented cane for exoskeleton control, Sensors 14 (1)
(2014) 1705–1722.
[132] S. Micera, L. Citi, J. Rigosa, J. Carpaneto, S. Raspopovic, G. Di Pino, L. Rossini, K.
Yoshida, L. Denaro, P. Dario, P.M. Rossini, Decoding information from neural
signals recorded using intraneural electrodes: toward the development of a
neurocontrolled hand prosthesis, Proc. IEEE 98 (3) (2010) 407–417.
[133] X. Chen, Y.-P. Zheng, J.-Y. Guo, J. Shi, Sonomyography (SMG) control for
powered prosthetic hand: a study with normal subjects, Ultrasound Med.
Biol. 36 (7) (2010) 1076–1088.
[134] X. Chen, S. Chen, G. Dan, Control of powered prosthetic hand using
multidimensional ultrasound signals: a pilot study, in: UAHCI’11 Proceedings
of the 6th International Conference on Universal Access in Human–Computer
Interaction: Applications and Services—Volume Part IV, 2011, pp. 322–327.
[135] J. Shi, Q. Chang, Y.-P. Zheng, Feasibility of controlling prosthetic hand using
sonomyography signal in real time: preliminary study, J. Rehabil. Res. Dev.
47 (2) (2010) 87–98.
[136] J. Silva, W. Heim, T. Chau, A self-contained, mechanomyography-driven
externally powered prosthesis, Arch. Phys. Med. Rehabil. 86 (10) (2005)
2066–2070.
[137] H.-B. Xie, Y.-P. Zheng, J.-Y. Guo, Classification of the mechanomyogram signal
using a wavelet packet transform and singular value decomposition for
multifunction prosthesis control, Physiol. Meas. 30 (5) (2009) 441–457.
[138] S. Gotu, M. Nakamura, Development of meal assistance orthosis for disabled
persons using EOG signal and dish image, Int. J. Adv. Robot. Syst. 1 (2) (2008)
107–115.
[139] D. Novak, R. Riener, Enhancing patient freedom in rehabilitation robotics
using gaze-based intention detection, in: Proceedings of the 2013 IEEE
International Conference on Rehabilitation Robotics, 2013.
[140] Y. Hao, M. Controzzi, C. Cipriani, D.B. Popovic, X. Yang, W. Chen, X. Zheng, M.C.
Carrozza, Controlling hand-assistive devices: utilizing electrooculography as
a substitute for vision, IEEE Robot. Autom. Mag. 20 (1) (2013) 40–52.
[141] S. Došen, C. Cipriani, M. Kostić, M. Controzzi, M.C. Carrozza, D.B. Popović, Cognitive vision system for control of dexterous prosthetic hands: experimental
evaluation, J. Neuroeng. Rehabil. 7 (42) (2010).
[142] S. Došen, D.B. Popović, Transradial prosthesis: artificial vision for control of
prehension, Artif. Organs 35 (1) (2011) 37–48.
[143] A. Koenig, D. Novak, X. Omlin, M. Pulfer, E. Perreault, L. Zimmerli, M. Mihelj,
R. Riener, Real-time closed-loop control of cognitive load in neurological
patients during robot-assisted gait training, IEEE Trans. Neural Syst. Rehabil.
Eng. 19 (4) (2011) 453–464.
[144] D. Novak, B. Beyeler, X. Omlin, R. Riener, Workload estimation in physical human–robot interaction using physiological measurements, Interact. Comput.
(2014) http://dx.doi.org/10.1093/iwc/iwu021. in press.
[145] K. Kiguchi, T. Tanaka, T. Fukuda, Neuro-fuzzy control of a robotic exoskeleton
with EMG signals, IEEE Trans. Fuzzy Syst. 12 (4) (2004) 481–490.
[146] K. Kiguchi, S. Kariya, K. Watanabe, K. Izumi, T. Fukuda, An exoskeletal robot
for human elbow motion support—sensor fusion, adaptation and control,
IEEE Trans. Syst. Man Cybern. B 31 (3) (2001) 353–361.
[147] A. Fougner, E. Scheme, A.D.C. Chan, K. Englehart, Ø. Stavdahl, A multimodal approach for hand motion classification using surface EMG and
accelerometers, in: 33rd Annual International Conference of the IEEE
Engineering in Medicine and Biology Society, 2011, pp. 4247–4250.

169

[148] A. Gijsberts, B. Caputo, Exploiting accelerometers to improve movement
classification for prosthetics, in: 2013 IEEE International Conference on
Rehabilitation Robotics, 2013.
[149] D.C. Tkach, L.J. Hargrove, Neuromechanical sensor fusion yields highest
accuracies in predicting ambulation mode transitions for trans-tibial
amputees, in: 35th Annual International Conference of the IEEE Engineering
in Medicine and Biology Society, 2013, pp. 3074–3077.
[150] S. Au, M. Berniker, H. Herr, Powered ankle-foot prosthesis to assist levelground and stair-descent gaits, Neural Netw. 21 (2008) 654–666.
[151] A. Fougner, E. Scheme, A.D.C. Chan, K. Englehart, O. Stavdahl, Resolving the
limb position effect in myoelectric pattern recognition, IEEE Trans. Neural
Syst. Rehabil. Eng. 19 (6) (2011) 644–651.
[152] Y. Geng, P. Zhou, G. Li, Toward attenuating the impact of arm positions
on electromyography pattern-recognition based motion classification in
transradial amputees, J. Neuroeng. Rehabil. 9 (74) (2012).
[153] F. Zhang, S.E.D. Andrea, M.J. Nunnery, S.M. Kay, H. Huang, Towards design of
a stumble detection system for artificial legs, IEEE Trans. Neural Syst. Rehabil.
Eng. 19 (5) (2011) 567–577.
[154] L. Du, F. Zhang, M. Liu, H. Huang, Toward design of an environment-aware
adaptive locomotion-mode-recognition system, IEEE Trans. Biomed. Eng. 59
(10) (2012) 2867–2875.
[155] F. Zhang, H. Huang, Decoding movement intent of patient with multiple
sclerosis for the powered lower extremity exoskeleton, in: 35th Annual
International Conference of the IEEE Engineering in Medicine and Biology
Society, 2013, pp. 4957–4960.
[156] A. Dutta, K. Koerding, E. Perreault, L. Hargrove, Sensor-fault tolerant control of
a powered lower limb prosthesis by mixing mode-specific adaptive Kalman
filters, in: 33rd Annual International Conference of the IEEE Engineering in
Medicine and Biology Society, 2011, pp. 3696–3699.
[157] R.J.K. Jacob, The use of eye movements in human–computer interaction
techniques: what you look at is what you get, ACM Trans. Inf. Syst. 9 (2)
(1991) 152–169.
[158] G. Onose, C. Grozea, A. Anghelescu, C. Daia, C.J. Sinescu, A.V. Ciurea, T.
Spircu, A. Mirea, I. Andone, A. Spânu, C. Popescu, A.-S. Mihăescu, S. Fazli, M.
Danóczy, F. Popescu, On the feasibility of using motor imagery EEG-based
brain–computer interface in chronic tetraplegics for assistive robotic arm
control: a clinical test and long-term post-trial follow-up, Spinal Cord 50 (8)
(2012) 599–608.
[159] R. Leeb, H. Sagha, R. Chavarriaga, J.D.R. Millán, A hybrid brain–computer
interface based on the fusion of electroencephalographic and electromyographic activities, J. Neural Eng. 8 (2) (2011) 025011.
[160] G. Cheron, M. Duvinage, C. De Saedeleer, T. Castermans, A. Bengoetxea, M.
Petieau, K. Seetharaman, T. Hoellinger, B. Dan, T. Dutoit, F. Sylos Labini, F.
Lacquaniti, Y. Ivanenko, From spinal central pattern generators to cortical
network: integrated BCI for walking rehabilitation, Neural Plast. 2012
(375148) (2012).
[161] K. Kiguchi, Y. Hayashi, A study of EMG and EEG during perception-assist with
an upper-limb power-assist robot, in: 2012 IEEE International Conference on
Robotics and Automation, 2012, pp. 2711–2716.
[162] M. Tombini, J. Rigosa, F. Zappasodi, C. Porcaro, L. Citi, J. Carpaneto, P.M.
Rossini, S. Micera, Combined analysis of cortical (EEG) and nerve stump
signals improves robotic hand control, Neurorehabil. Neural Repair 26 (3)
(2012) 275–281.
[163] J. Gallego, J. Ibanez, J.L. Dideriksen, J.I. Serrano, M.D. Del Castillo, D. Farina, E.
Rocon, A multimodal human–robot interface to drive a neuroprosthesis for
tremor management, IEEE Trans. Syst. Man Cybern. Part C Appl. Rev. 42 (6)
(2012) 1159–1168.
[164] A.M. Simon, L.J. Hargrove, B.A. Lock, T.A. Kuiken, Target achievement
control test: evaluating real-time myoelectric pattern-recognition control of
multifunctional upper-limb prostheses, J. Rehabil. Res. Dev. 48 (6) (2011)
619–628.
[165] A.J. Young, L.H. Smith, E.J. Rouse, L.J. Hargrove, A comparison of the real-time
controllability of pattern recognition to conventional myoelectric control for
discrete and simultaneous movements, J. Neuroeng. Rehabil. 11 (5) (2014).
[166] L.J. Hargrove, A.M. Simon, R. Lipschutz, S.B. Finucane, T.A. Kuiken, Nonweight-bearing neural control of a powered transfemoral prosthesis,
J. Neuroeng. Rehabil. 10 (62) (2013).
[167] Z.O. Khokhar, Z.G. Xiao, C. Menon, Surface EMG pattern recognition for realtime control of a wrist exoskeleton, Biomed. Eng. Online 9 (41) (2010).
[168] M. Chandrapal, X. Chen, W. Wang, B. Stanke, N. Le, Preliminary evaluation
of intelligent intention estimation algorithms for an actuated lower-limb
exoskeleton, Int. J. Adv. Robot. Syst. 10 (2013) 1–10.
[169] R. Ronsse, T. Lenzi, N. Vitiello, B. Koopman, E. van Asseldonk, S.M.M. De
Rossi, J. van den Kieboom, H. van der Kooij, M.C. Carrozza, A.J. Ijspeert,
Oscillator-based assistance of cyclical movements: model-based and modelfree approaches, Med. Biol. Eng. Comput. 49 (10) (2011) 1173–1185.
[170] G.S. Sawicki, D.P. Ferris, Mechanics and energetics of level walking with
powered ankle exoskeletons, J. Exp. Biol. 211 (2008) 1402–1413.
[171] F.V.G. Tenore, A. Ramos, A. Fahmy, S. Acharya, R. Etienne-Cummings,
N.V. Thakor, Decoding of individuated finger movements using surface
electromyography, IEEE Trans. Biomed. Eng. 56 (5) (2009) 1427–1434.
[172] H. Daley, K. Englehart, L. Hargrove, U. Kuruganti, High density electromyography data of normally limbed and transradial amputee subjects for multifunction prosthetic control, J. Electromyography Kinesiol. 22 (3) (2012) 478–484.

170

D. Novak, R. Riener / Robotics and Autonomous Systems 73 (2015) 155–170
Domen Novak received his diploma and Ph.D. in electrical engineering from the University of Ljubljana in 2008
and 2012, respectively. From 2008 to 2012 he was a researcher with the Laboratory of Robotics at the University of Ljubljana. He is now a postdoctoral researcher at
the Sensory–Motor Systems Lab of ETH Zurich. His research interests include rehabilitation robotics, applied
psychophysiology, machine learning and enhancing motivation in virtual environments.

Robert Riener is full professor for Sensory–Motor Systems
at the Department of Health Sciences and Technology,
ETH Zurich, and professor of medicine at the University
Clinic Balgrist, University of Zurich. He has published more
than 400 peer-reviewed journal and conference articles,
20 books and book chapters and filed 21 patents. He
has also received more than 15 personal distinctions and
awards including the Swiss Technology Award in 2006, the
IEEE TNSRE Best Paper Award 2010, and the euRobotics
Technology Transfer Awards 2011 and 2012. His research
focuses on the investigation of the sensory–motor actions
in and interactions between humans and machines, with the main application areas
being rehabilitation and sports.

