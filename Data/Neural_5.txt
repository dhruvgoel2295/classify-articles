Neural Networks 71 (2015) 150–158

Contents lists available at ScienceDirect

Neural Networks
journal homepage: www.elsevier.com/locate/neunet

Reinforcement learning solution for HJB equation arising in
constrained optimal control problem
Biao Luo a , Huai-Ning Wu b , Tingwen Huang c , Derong Liu d,∗
a

The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China

b

Science and Technology on Aircraft Control Laboratory, Beihang University (Beijing University of Aeronautics and Astronautics), Beijing 100191, China

c

Texas A&M University at Qatar, PO Box 23874, Doha, Qatar

d

School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing 100083, China

article

info

Article history:
Received 4 January 2015
Received in revised form 15 August 2015
Accepted 16 August 2015
Available online 24 August 2015
Keywords:
Constrained optimal control
Data-based
Off-policy reinforcement learning
Hamilton–Jacobi–Bellman equation
The method of weighted residuals

abstract
The constrained optimal control problem depends on the solution of the complicated Hamilton–Jacobi–
Bellman equation (HJBE). In this paper, a data-based off-policy reinforcement learning (RL) method is
proposed, which learns the solution of the HJBE and the optimal control policy from real system data.
One important feature of the off-policy RL is that its policy evaluation can be realized with data generated
by other behavior policies, not necessarily the target policy, which solves the insufficient exploration
problem. The convergence of the off-policy RL is proved by demonstrating its equivalence to the successive
approximation approach. Its implementation procedure is based on the actor–critic neural networks
structure, where the function approximation is conducted with linearly independent basis functions.
Subsequently, the convergence of the implementation procedure with function approximation is also
proved. Finally, its effectiveness is verified through computer simulations.
© 2015 Elsevier Ltd. All rights reserved.

1. Introduction
Optimal control is an important part of control theory, which
has been widely investigated over the past several decades. The
bottleneck of its applications to nonlinear systems is that it depends on the solution of the Hamilton–Jacobi–Bellman equation
(HJBE) (Bertsekas, 2005; Hull, 2003; Lewis, Vrabie, & Syrmos,
2013), which is extremely difficult to obtain analytically. Over the
past few years, reinforcement learning (RL) (Lendaris, 2009; Powell, 2007; Precup, Sutton, & Dasgupta, 2001; Sutton & Barto, 1998),
has appeared as an efficient tool to solve the HJBE and many meaningful results (Faust, Ruymgaart, Salman, Fierro, & Tapia, 2014;
Jiang & Jiang, 2012; Lee, Park, & Choi, 2012; Liu, Wang, & Li, 2014;
Liu & Wei, 2014; Luo, Wu, Huang, & Liu, 2014; Modares & Lewis,
2014; Murray, Cox, Lendaris, & Saeks, 2002; Vamvoudakis & Lewis,
2010; Vrabie & Lewis, 2009; Vrabie, Pastravanu, Abu-Khalaf, &
Lewis, 2009; Wang, Liu, & Li, 2014; Wei & Liu, 2012; Yang, Liu,

∗

Corresponding author.
E-mail addresses: biao.luo@hotmail.com (B. Luo), whn@buaa.edu.cn
(H.-N. Wu), tingwen.huang@qatar.tamu.edu (T. Huang), derong@ustb.edu.cn
(D. Liu).
http://dx.doi.org/10.1016/j.neunet.2015.08.007
0893-6080/© 2015 Elsevier Ltd. All rights reserved.

& Wang, 2014; Yang, Liu, Wang, & Wei, 2014; Zhao, Xu, & Jagannathan, 2014) have been reported. For example, appropriate estimators were employed for approximating value function such
that the temporal difference error is minimized (Doya, 2000). Murray et al. (2002) suggested two policy iteration algorithms that
avoid the necessity of knowing the internal system dynamics. Vrabie et al. (2009) extended their result and proposed a new policy
iteration algorithm to solve the linear quadratic regulation problem online along a single state trajectory. A nonlinear version of
this algorithm was presented in Vrabie and Lewis (2009) by using neural network (NN) approximator. Vamvoudakis and Lewis
(2010) also gave a so-called synchronous policy iteration algorithm
which tunes synchronously the weight parameters of both NNs in
the actor–critic structure. An integral reinforcement learning (IRL)
method (Modares & Lewis, 2014) was introduced to solve the linear quadratic tracking problem of partially-unknown continuoustime systems. Online adaptive optimal control (Jiang & Jiang, 2012)
and Q-learning (Lee et al., 2012) algorithms were developed for
linear quadratic regulator problem. Off-policy RL approaches were
proposed to solve the nonlinear data-based optimal control problem (Luo et al., 2014) and partially model-free H∞ control problem
(Luo, Wu, & Huang, 2015). However, it is noted that control constraints are not involved in these results.
In practice, constraints widely exist in real control systems and
have damaging effects on the system performance, and thus should

B. Luo et al. / Neural Networks 71 (2015) 150–158

be accounted for during the controller design process. For the constrained optimal control problem, several results (Abu-Khalaf &
Lewis, 2005; He & Jagannathan, 2005, 2007; Heydari & Balakrishnan, 2013; Liu, Wang, & Yang, 2013; Lyshevski, 1998; Modares,
Lewis, & Naghibi-Sistani, 2013; Zhang, Luo, & Liu, 2009) have been
reported recently. A nonquadratic cost functional was introduced
by Lyshevski (1998) to confront input constraints, and then the associated HJBE was reformulated accordingly. As the extensions of
the method in Saridis and Lee (1979) and Beard, Saridis, and Wen
(1997) to handle constrained optimal control problem, modelbased successive approximation method was used for solving
the HJBE of continuous-time systems (Abu-Khalaf & Lewis, 2005)
and discrete-time systems (Chen & Jagannathan, 2008). Modares,
Lewis, and Naghibi-Sistani (2014) developed an experience-replay
based IRL algorithm for nonlinear partially unknown constrainedinput systems. A heuristic dynamic programming was used to solve
the constrained optimal control problem of nonlinear discretetime systems (Zhang et al., 2009). The single network based adaptive critics method was proposed for finite-horizon nonlinear
constrained optimal control design (Heydari & Balakrishnan,
2013). However, the data-based constrained nonlinear optimal
control problem is rarely studied with off-policy RL and still remains an open issue.
In this paper, a data-based off-policy RL method is proposed for
learning the constrained optimal control policy form real system
data instead of using mathematical model. The rest of the paper
is arranged as follows. Section 2 gives the problem description
and Section 3 presents a model-based successive approximation
method. The data-based off-policy RL method is developed in
Section 4. Section 5 shows the simulation results and Section 6
gives the conclusions.
Notation: R and Rn are the set of real numbers and the ndimensional Euclidean space, respectively. ∥ · ∥ denotes the
vector norm or matrix norm in Rn . The superscript T is used for
the transpose and I denotes the identify matrix of appropriate
dimension. ▽
∂/∂ x denotes a gradient operator. C 1 (X) is a
function space on X with continuous first derivatives. Let X and U
be compact sets, denote D
{(x, u)|x ∈ X, u ∈ U}. For column
vector functions s1 (x, u) and s2 (x, u), where (x, u) ∈ D , define
inner product ⟨s1 (x, u), s2 (x, u)⟩D
sT (x, u)s2 (x, u)d(x, u) and
D 1
norm ∥s1 (x, u)∥D

when x = 0, u = 0. Then, the optimal control problem is briefly
presented as
u( t ) = u∗ ( x )

Definition 1 (Admissible Control). For the given system (1), x ∈ X,
a control policy u(x) is defined to be admissible with respect to
the cost function (2) on X, denoted by u(x) ∈ U(X), if, (1) u is
continuous on X, (2) u(0) = 0, (3) u(x) stabilizes the system, and
(4) V (x) < ∞, ∀x ∈ X.
For ∀u(x) ∈ U(X), its value function V (x) of (2) satisfies the
following linear partial differential equation (Abu-Khalaf & Lewis,
2005):

[∇ V (x)]T (f (x) + g (x)u(x)) + Q (x) + W (u) = 0,

where V (x) ∈ C (X), V (x) ≥ 0 and V (0) = 0. From the optimal
control theory (Anderson & Moore, 2007; Bertsekas, 2005; Lewis
et al., 2013), if using the optimal control u∗ (x), the Eq. (4) results in
the HJBE

[∇ V ∗ (x)]T (f (x) + g (x)u∗ (x)) + Q (x) + W (u∗ ) = 0.

x(0) = x0 ,

(1)

where x = [x1 , . . . , xn ] ∈ R is the state, x0 is the initial state and
u = [u1 , . . . , um ]T ∈ Rm is the control input constrained by |ui |
β , β > 0. Assume that f (x) + g (x)u(x) is Lipschitz continuous on
X that contains the origin, f (0) = 0, and the system is stabilizable
on X, i.e., there exists a continuous control function u(x) such that
the system is asymptotically stable. f (x) and g (x) are continuous
unknown vector or matrix functions of appropriate dimensions.
The optimal control problem under consideration is to learn
a state feedback control law u(t ) = u(x(t )) from real system
data, such that the system (1) is closed-loop asymptotically stable, and minimize the following generalized infinite horizon cost
functional:
V (x0 )

n

+∞



(Q (x(t )) + W (u(t )))dt ,

(2)

0

where Q (x) and W (u) are positive definite functions, i.e., for ∀x ̸=
0, u ̸= 0, Q (x) > 0, W (u) > 0, and Q (x) = 0, W (u) = 0 only

(5)

For the system (1) with input constraints |ui | β , the following
nonquadratic form W (u) for the cost functional (2) can be used
(Abu-Khalaf & Lewis, 2005; Lyashevskiy, 1996; Lyshevski, 1998;
Modares et al., 2013):


m


ul

rl

ϕ −1 (µl )dµl ,

(6)

0

l=1

x˙ (t ) = f (x(t )) + g (x(t ))u(t ),

(4)

1

W ( u) = 2

Let us consider the following continuous-time nonlinear
system:

(3)

u

For the model-based optimal control problem (3), i.e., the
mathematical models of f (x) and g (x) are completely known, it can
be converted to solving the HJBE. In Abu-Khalaf and Lewis (2005),
a model-based successive approximation method was given for
solving the HJBE, where the HJBE is successively approximated by
a sequence of linear partial differential equations. Before we start,
the definition of admissible control (Abu-Khalaf & Lewis, 2005;
Beard et al., 1997) is given.

2. Problem description

T

arg min V (x0 ).

3. Model-based successive approximation method

1/2

⟨s1 (x, u), s2 (x, u)⟩D .

151

where µ ∈ Rm , rl > 0 and ϕ(·) is a continuous one-to-one
bounded function satisfying |ϕ(·)|
β with ϕ(0) = 0. Moreover,
ϕ(·) is a monotonic odd function and its derivative is bounded.
An example of ϕ(·) is the hyperbolic tangent tanh(·). Denoting
R = diag(r1 , . . . , rm ), it follows from Abu-Khalaf and Lewis (2005)
and Lyshevski (1998) that the HJBE (5) of the constrained optimal
control problem is given by
∗ T

[∇ V ]



f − gϕ



1 −1 T
R g ∇V ∗
2



+ Q (x)




1 −1 T
+ W −ϕ
R g ∇V ∗
= 0.
2

(7)

By solving the HJBE for V ∗ (x), the optimal control policy is obtained
as:
u∗ (x) = −ϕ





1 −1 T
R g (x)∇ V ∗ (x) .
2

(8)

For simplicity of description, define

ν ∗ (x)

1

− R−1 g T (x)∇ V ∗ (x).
2

(9)

152

B. Luo et al. / Neural Networks 71 (2015) 150–158

Then, the HJBE (7) and optimal control (8) can briefly be rewritten
as:



(∇ V ∗ )T f + g ϕ(ν ∗ ) + Q + W (ϕ(ν ∗ )) = 0
u = ϕ(ν ).
∗

∗

(10)
(11)

In Abu-Khalaf and Lewis (2005), the HJBE (10) is successively
approximated with a sequence of iterative equations

[∇ V (i) ]T (f + gu(i) ) + Q + W (u(i) ) = 0;

i = 0, 1, . . . ,

(12)

with policy improvement
u(i+1) = ϕ(ν (i+1) ),

(13)

where

ν (i+1)

1

− R−1 g T ∇ V (i) .

(14)

2

Remark 1. From the famous RL books (Bertsekas, 2005; Sutton
& Barto, 1998), policy iteration is a basic framework of RL. Over
the past few years, policy iteration has already been employed to
solve the unconstrained optimal control problems of linear systems (Jiang & Jiang, 2012; Modares & Lewis, 2014; Vrabie et al.,
2009) and nonlinear systems (Vamvoudakis & Lewis, 2010; Vrabie
& Lewis, 2009). In fact, the successive approximation between the
iterative equations (12) and (13) is essentially a model-based policy iteration method, which involves two basic steps: policy evaluation and policy improvement. The Eq. (12) is policy evaluation
for evaluating the control policy u(i) for its value function V (i) , and
Eq. (13) is policy improvement for obtaining an improved control
policy u(i+1) based on the current value function V (i) . By implementing the two steps alternatively, it has been proven in AbuKhalaf and Lewis (2005) that the value function V (i) will converge
to the solution of the HJBE (10), i.e., limi→∞ V (i) = V ∗ and thus
limi→∞ u(i) = u∗ . Note that the iterative equation (12) involves the
full mathematical system models of f (x) and g (x). In Abu-Khalaf
and Lewis (2005), a model-based approach was developed to solve
the iterative equation (12) by using NN for approximating the value
function V (i) .

Integrating both sides of (16) on the interval [t , t +
rearranging terms yields,
t+ t


2

For the data-based constrained optimal control problem
under consideration, the explicit expression of the HJBE (10) is
unavailable since the mathematical system models f (x) and g (x)
are unknown, which prevents using model-based approaches for
control design. To overcome this difficulty, a data-based off-policy
RL is developed to learn the optimal control policy.

+ V (i) (x(t )) − V (i) (x(t + t ))
 t+ t


Q (x(τ )) + W (ϕ(ν (i) (x(τ )))) dτ ,
=

where V (i) (x) is an unknown function and ν (i+1) (x) is an unknown
function vector to be solved. The main idea of the off-policy RL is
solving the iterative equation (17) instead of the iterative equation
(12). Compared with the iterative equation (12), the iterative
equation (17) does not require the explicit mathematical model
of the system (1), i.e., f (x) and g (x). According to the definition of
off-policy RL (Maei, Szepesvári, Bhatnagar, & Sutton, 2010; Precup
et al., 2001; Sutton & Barto, 1998), the value function V (i) of the
target control policy u(i) can be evaluated by using system data
generated with other behavior policies u and not restricted to the
target policy. This implies that the proposed off-policy RL method
has an advantage that it can learn the value function and control
policy from system data that are generated according to more
exploratory or even random policies.
For the proposed off-policy RL, its aim is to learn the constrained
optimal control policy (8) by iteratively solving Eq. (17) for the
unknown function V (i) (x) and function vector ν (i+1) (x). Thus, it is
necessary to prove that the generated sequences {V (i) } and {ν (i) }
will converge to V ∗ and ν ∗ , respectively.
Theorem 1. Let V (i) (x) ∈ C 1 (X), V (i) (x) ≥ 0, V (i) (0) = 0 and
ϕ(ν (i+1) (x)) ∈ U(X). (V (i) (x), ν (i+1) (x)) is the solution of Eq. (17) if
and only if it is the solution of the iterative equations (12)–(14),
i.e., Eq. (17) is equivalent to the iterative equations (12)–(14).
Proof. From the derivation of Eq. (17), we have that if (V (i) , ν (i+1) )
is the solution of the iterative equations (12)–(14), it also satisfies
Eq. (17). To complete the proof, we have to show that (V (i) , ν (i+1) )
is the unique solution of Eq. (17). The proof is by contradiction.
Before start, we derive a simple fact. Consider
lim

(i)

+ g [u − u ]

(15)
(i)

for ∀u ∈ U. Let us consider the case when V (x) be the solution of
the iterative equation (12). By using (12)–(14), we take derivative
of V (i) (x) with respect to time along the state of system (15)
dV (i) (x)
dt

t

= −Q − W (u ) + 2(ν

h¯ (τ )dτ
t

 t +
1
= lim
t →0
t
0
 t
d
=
h¯ (τ )dτ
dt

t

h¯ (τ )dτ −

t



h¯ (τ )dτ



0

0

= h¯ (t ).

(18)

From (17), we have

(i+1) T

t →0

1  (i)
V (x(t +
t



t+ t

= 2 lim

t →0

t )) − V (i) (x(t ))



[ν (i+1) (x(τ ))]T

t
(i)

× R[ϕ(ν (x(τ ))) − u(τ )]dτ
 t+ t


1
Q (x(τ )) + W (ϕ(ν (i) (x(τ )))) dτ .
− lim
t →0

dV (i) (x)

(i)

) R [ u − u]

= −Q − W (ϕ(ν (i) )) + 2(ν (i+1) )T R[ϕ(ν (i) ) − u].

= lim

t

(19)

t

By using the fact (18), Eq. (19) is rewritten as

= [∇ V (i) ]T (f + gu(i) ) − [∇ V (i) ]T g [u(i) − u]
(i)

t+ t



1

dt

In this subsection, the off-policy RL approach is derived based
on (12) and (13). Inspired by Jiang and Jiang (2014) and Luo et al.
(2014), the system (1) can be rewritten as
x˙ = f + gu

(17)

t

dV (i) (x)

4.1. Off-policy reinforcement learning

(i)

[ν (i+1) (x(τ ))]T R[ϕ(ν (i) (x(τ ))) − u(τ )]dτ

t

t →0

4. Data-based constrained optimal control

t ] and

dt
(16)

= 2[ν (i+1) (x(t ))]T R[ϕ(ν (i) (x(t ))) − u(t )]
− Q (x(t )) − W (ϕ(ν (i) (x(t )))).

(20)

B. Luo et al. / Neural Networks 71 (2015) 150–158

Suppose that (W (x), υ(x)) is another solution of Eq. (17), where
W (x) ∈ C 1 (X) with boundary condition W (0) = 0 and ϕ(υ(x)) ∈
U(X). Thus, (W , υ) also satisfies Eq. (20), i.e.,
dW (x)
dt

= 2υ T (x(t ))R[ϕ(ν (i) (x(t ))) − u(t )]

Due to the truncation error of the trail solutions (24) and (25),
the replacement of V (i) and ν (i+1) in the iterative equation (17) with
Vˆ (i) and νˆ (i+1) respectively, yields the following residual error:

(22)

This means that Eq. (22) holds for ∀u ∈ U. If letting u = ϕ(ν (i) ),
we have


d  (i)
V (x) − W (x) = 0.

(23)

dt

This implies that V (i) (x) − W (x) = c holds for ∀x ∈ X, where
c is a real constant. For x = 0, c = V (i) (0) − W (0) = 0. Then,
V (i) (x) − W (x) = 0, i.e., W (x) = V (i) (x) for ∀i, x ∈ X. From (22),
we have

[ν (i+1) (x) − υ(x)]T R[ϕ(ν (i) (x)) − u] = 0
for ∀u ∈ U. Thus, ν (i+1) (x) − υ(x) = 0, i.e., υ(x) = ν (i+1) (x) for ∀i,
x ∈ X. This completes the proof.
Theorem 1 shows that the off-policy RL with iterative equation
(17) is theoretically equivalent to the model-based successive
approximation method with the iterative equations (12)–(14),
which is convergent as proved in Abu-Khalaf and Lewis (2005).
Thus, the convergence of the off-policy RL can be guaranteed.
4.2. The method of weighted residuals
At each step of the off-policy RL, it requires to solve the iterative
(i+1)
equation (17) for V (i) (x) and ν (i+1) (x) = [ν1
(x), . . . , νm(i+1) (x)]T .
By using real system data, the method of weighted residuals
(MWR) is derived based on the actor–critic NN structure. Although
similar MWR can also be found in Luo et al. (2014), for the sake of
clearness and completeness, the MWR will be developed for (17)
which is much more complicated and different to some extent.
l
Let Ψ (x)
{ψj (x)}∞
{φkl (x)}∞
j=1 and Φ (x)
k=1 be complete sets
of any linearly independent basis functions, such that ψj (0) = 0
and φkl (0) = 0 for ∀l, j, k. Then, the solution (V (i) (x), ν (i+1) (x))
of the iterative equation (17) can be expressed as linear combination of basis function sets Ψ (x) and Φ l (x), i.e., V (i) (x) =
∞ (i)

(i+1)
(i+1) l
(x) = ∞
j=1 θV ,j ψj (x) and νl
k=1 θνl ,k φk (x), (l = 1, . . . , m),
which are assumed to converge pointwise in X. By using finitedimensional sets ΨN (x)
[ψ1 (x), . . . , ψLV (x)]T and ΦNl (x)
[φ1l (x), . . . , φLl u (x)]T as neuron activation functions, the real output of critic and actor NNs can be, respectively, given by
(i)

(i)
where θˆV

By using (24) and (26), we have

σ (i) (x(t ), u(t ))
= [ΨN (x(t )) − ΨN (x(t + t ))]T θˆV(i)
 t + t 
m



T
ϕ ΦNl (x(t )) θˆν(li)
+2
rl
t
l =1

 l
T
× ΦN (x(τ )) dτ θˆν(li+1)
 t + t

m


T
−2
rl
ul (τ ) ΦNl (x(τ )) dτ θˆν(li+1)
t
 l=t +1 t
−
Q (x(τ ))dτ
t

 


 t + t  ϕ Φ l (x(t )) T θˆν(i)
m

N
l

−2
rl
ϕ −1 (µl )dµl  dτ .
t

l =1

(28)

0

For simplicity of notation, define

ρ

Ψ (x(t ))

]T
[ΨtN+(xt(t )) − ΨN (x(t + t ))


T

T
ρΦ(i)l (x(t ))
ϕ ΦNl (x(t )) θˆν(li) ΦNl (x(τ )) dτ
t
 t+ t

T
l
ρuΦ (x(t ), u(t ))
ul (τ ) ΦNl (x(τ )) dτ
 t+ t t
ρQ (x(t ))
Q (x(τ ))dτ
t

 


 t + t  ϕ Ψ l (x(t )) T θˆν(i)
N
l

ρ1(i)l (x(t ))
ϕ −1 (µl )dµl  dτ .
t

(29)

0

Then, Eq. (28) is rewritten as

σ (i) (x(t ), u(t )) = ρ

ˆ (i)
Ψ (x(t ))θV

+2

m


(i)l

rl ρΦ (x(t ))θˆν(li+1)

l =1

−2

m


rl ρul Φ (x(t ), u(t ))θˆν(li+1)

l =1

− ρQ (x(t )) − 2

m


(i),l

rl ρ1 (x(t )).

(30)

l =1

Vˆ (i) (x) = ΨNT (x)θˆV

(x))

(27)

t

× R[ϕ(ν (i) (x(t ))) − u(t )].

(x) = (

[ˆν (i+1) (x(τ ))]T

t


d  (i)
V (x) − W (x) = 2[ν (i+1) (x(t )) − υ(x(t ))]T
dt

νˆ l

2

× R[ϕ(ˆν (i) (x(τ ))) − u(τ )]dτ
+ Vˆ (i) (x(t )) − Vˆ (i) (x(t + t ))
 t+ t


Q (x(τ )) + W (ϕ(ˆν (i) (x(t )))) dτ .
−

(21)

Substituting Eq. (21) from (20) yields,

ΦNl

t+ t



σ (i) (x(t ), u(t ))

− Q (x(t )) − W (ϕ(ν (i) (x(t )))).

(i+1)

153

(24)
T

θˆν(li+1) ,

(25)

[θˆV(i,)1 , . . . , θˆV(i,)LV ]T and θˆν(li+1)

[θˆν(li,+11) , . . . , θˆν(li,+Lu1) ]T
(i)

are the estimations of the unknown ideal weight vectors θV

[θV(i,)1 , . . . , θV(i,)LV ]T and θν(li+1)

[θν(li,+11) , . . . , θν(li,+Lu1) ]T . The expression

(25) can be rewritten as a compact form


T
νˆ (i+1) (x) = νˆ 1(i+1) (x), . . . , νˆ m(i+1) (x)
T

= (ΦN1 (x))T θˆu(1i+1) , . . . , (ΦNm (x))T θˆu(mi+1) .

(26)

To write Eq. (30) in a compact form, define

  
T

T 
T
θˆV(i) , θˆu(1i+1) , . . . , θˆu(mi+1)


ρ (uiΦ)l (x(t ), u(t )) rl ρΦ(i)l (x(t )) − ρul Φ (x(t ), u(t ))

ρ (i) (x(t ), u(t ))
ρ T Ψ (x(t )), 2ρ (uiΦ)1 (x(t ), u(t )), . . . ,

(i)m
2ρ uΦ (x(t ), u(t ))

θˆ (i+1)

π (i) (x(t ))

ρQ (x(t )) + 2

m

l=1

(i)l

rl ρ1 (x(t ))

(31)

154

B. Luo et al. / Neural Networks 71 (2015) 150–158

where θˆ (i+1) is the unknown constant vector of size L = LV + mLu .
Then, the Eq. (30) is rewritten as

where η(i)
π (i) (x1 ), . . . , π (i) (xM ) . Then, the substitution of
(35) and (36) into (34) yields,

σ (i) (x(t ), u(t )) = ρ (i) (x(t ), u(t ))θˆ (i+1) − π (i) (x(t )),


−1
θˆ (i+1) = (W (i) )T Z (i)
(W (i) )T η(i) .

(i)

where we denote ρ (i)

(32)

(i) T

= [ρ 1 , . . . , ρ L ] . In the MWR, the
unknown constant vector θˆ (i+1) can be solved in such a way that
the residual error σ (i) (x, u) (for ∀t 0) of (32) is forced to be zero
in some average sense. The weighted integrals of the residual are
set to zero:



(i)

WL (x, u), σ (i) (x, u)


D

(i)

= 0,

(33)

(i)

(i)

where WL (x, u)
[ω1 (x, u), . . . , ωL (x, u)] is named the
weighted function vector. Then, the substitution of (32) into (33)
yields,
T



θˆ (i+1) − WL(i) (x, u), π (i) (x) = 0,
D
D




(i)
(i)
(i)
(i)
where the notations WL , ρ
and WL , π
are given by
D
D






ω1(i) , ρ 1(i)
· · · ω1(i) , ρ (Li)
D
D





(i)
..
..
..
WL , ρ (i)


.
.
.
D



 
(i)
(i)
(i)
(i)
· · · ωL , ρ L
ωL , ρ 1
(i)



WL (x, u), ρ (i) (x, u)



D

(i)

WL , π (i)


D

(34)
(i)



(i)

and WL , π (i)

D



(i)

Monte-Carlo integration for computing WL (x, u), ρ (i) (x, u)

.

D

Let ID
d(x, u), and SM
{(xk , uk )|(xk , uk ) ∈ D , k =
D
1, 2, . . . , M } be the set that are sampled on domain D , where M is
the size of sample set SM . Generally, it is desired to collect data set
SM as rich as possible to cover
the domain D as

 much as possible.



(i)

computed with



(i)

WL (x, u), ρ (i) (x, u)





=

D

(i)

D

=
=
where W (i)

M k=1
ID
M

(i)

WL (xk , uk )ρ (i) (xk , uk )

(W (i) )T Z (i) ,

(i)



is approximately

D

WL (x, u)ρ (i) (x, u)d(x, u)

M
ID 

(35)

(i)

WL (x1 , u1 ), . . . , WL (xM , uM )

T

and Z (i)

T
 (i)T
ρ (x1 , u1 ), . . . , ρ (i)T (xM , uM ) . Similarly,


(i)

WL (x, u), π (i) (x)



=

D

=

M
ID  

M k=1
ID
M

(i)

WL (xk , uk )

(W (i) )T η(i) ,

T

Step 2: Give initial parameter vectors θˆu such that φ(ˆν (0) ) ∈
U(X). Let i = 0;
Step 3: Compute W (i) , Z (i) and η(i) , and update θˆ (i+1) with (37);
Step 4: Let i = i + 1. If ∥θˆ (i) − θˆ (i−1) ∥ ≤ ξ (ξ is a small positive
(i)
number), stop iteration and θˆu is employed to obtain the final
(i)
control policy φ(ˆν ); else go back to Step 3 and continue.

in-

D

volve many numerical integrals on domain D , which are computationally expensive. Thus, the Monte-Carlo integration method (Luo
et al., 2014; Peter Lepage, 1978) is introduced, which is especially
competitive on multi-dimensional domain.

 We now illustrate the

With the sample set SM , WL (x, u), ρ (i) (x, u)

Remark 2. It is assumed that the system state x is measurable and
sufficient system data can be collected for implementing the offpolicy RL. For the function approximation of V (i) (x) and ν (i+1) (x),
linearly independent basis functions are required, which may bring
limitation to some extent. Further studies will be conducted to
reduce the deficiencies of this limitation.

(0)

Thus, θˆ (i+1) can be obtained with

Note that the computations of WL , ρ (i)

then the unknown parameter vector θˆ (i+1) is obtained with the
expression (37) accordingly. With θˆ (i+1) , the unknown function
V (i) (x) and function vector ν (i+1) (x) can be approximately obtained
by expressions (24) and (26), respectively.

Step 1: Collect real system data (xk , uk ) for sample set SM , and
then compute ρ Ψ (xk ), ρul Φ (xk , uk ) and ρQ (xk );

D



It is observed that the sample set SM is generated on domain
D , based on which W (i) , Z (i) and η(i) can be computed and

Algorithm 1. Off-policy RL for data-based constrained optimal
control design.



 T

ω1(i) , π (i) , . . . , ωL(i) , π (i)
.



−1 
(i)
WL , π (i) .
θˆ (i+1) = WL(i) , ρ (i)
D
D


(37)

Based on the parameter update strategy (37), we give the
implementation procedure of the off-policy RL for data-based
constrained optimal control design.

D

D

T

4.3. Implementation of off-policy reinforcement learning

and





π (i) (xk )
(36)

Remark 3. Note that the off-policy RL method (i.e., Algorithm 1) is
also suitable for solving the unconstrained optimal control problem. By using a sufficiently large control bound β , the constrained
optimal control problem is relaxed to be an unconstrained one.
Remark 4. On-policy RL is one of the popular methods used for
control design (Modares & Lewis, 2014; Vrabie & Lewis, 2009;
Vrabie et al., 2009). To evaluate the value function of a general
target control policy µ in the on-policy RL methods, it needs
to generate system data using the policy µ. This biases the
learning process by under-representing states that are unlikely
to occur under µ. As a result, the estimated value function of
these underrepresented states may be highly inaccurate, and
seriously impact the improved policy. This is known as inadequate
exploration—a particularly acute difficult issue in RL methods,
which is rarely discussed in the existing works using RL techniques
for control design. On the other hand, for real implementation of
the on-policy learning methods, the approximate target control
policy µ
ˆ (rather than the actual target policy µ) is usually used to
generate data for learning its value function. In other words, the
on-policy learning methods use the ‘‘inaccurate’’ data to learn its
value function, which will increase the accumulated error. Note
that these mentioned problems are solved in the developed offpolicy RL method (i.e., Algorithm 1). The policy evaluation in the
off-policy RL method can be realized with data generated by other
behavior policies while not necessarily the target policy, which
increases the ‘‘exploration’’ ability during the learning process.
Moreover, in the off-policy RL algorithm, the control u and state
x can be arbitrary on U and X, where no error occurs during the
process of generating data, and thus the accumulated error can be
reduced.

B. Luo et al. / Neural Networks 71 (2015) 150–158
t+ t



4.4. Convergence analysis with approximation



−

155

Q (xl (τ )) + W (ϕ(ˆν (i) (xl (t )))) dτ





t

Under NN approximation structure (24) and (25), the convergence of the developed off-policy RL method is proved in the following Theorem 2.

+ [
θ (i+1) ]T

WL (xl , ul )el

l =1

Theorem 2. Let the parameter vector θˆk be computed with (37).
Assume that there exist constants M 1 > 0 and δ1 > 0, such that
for ∀M M 1 and i 0,

M


= [
θ (i+1) ]T

WL (xl , ul )el ,

(46)

l =1

where

1

(i) T (i)

M

(W ) Z

δ1 IL .

el = −

Proof. (1) Let V
(i) T

(i)

−2

(i)

[∇ V ] (f + g uˆ ) + Q + W (ˆu ) = 0,
where V
(i+1)

(i)

j =1

(39)

(i)
j=1 θ V ,j ψj (x)

∞

(i+1)
θˆ (i+1) − θ
.

(41)

Then, it follows from (37) and (41) that

(W (i) )T Z (i) θˆ (i+1) = (W (i) )T η(i) ,

M δ1 ∥
θ (i+1) ∥2

(W (i) )T Z (i)
θ (i+1) = (W (i) )T η(i) − (W (i) )T Z (i) θ

(i+1)

W (i)
θ (i+1)



(i+1)
].
[
θ (i+1) ]T (W (i) )T Z (i)
θ (i+1) = [W (i)
θ (i+1) ]T [η(i) − Z (i) θ

M δ1 ∥
θ (i+1) ∥2 .

(42)

(43)

(i)

Based on definitions of η , Z
given by
(i)(i+1) T

[W θ

(i)

and θ

(i+1)

].

, the right side of (45) is

1

∥
θ (i+1) ∥

δ1

] [η − Z θ
]
M

= [
θ (i+1) ]T
WL (xl , ul )π (i) (xl )
− [
θ (i+1) ]T

M


t+ t

WL (xl , ul ) 2

× R[ϕ(ˆν (i) (xl (τ ))) − ul (τ )]dτ
(i)

+ V (xl (t )) − V (xl (t + t ))


θV(i,)j ψj (x) +

∞


(i)

θ V ,j ψj (x),

j=LV +1
Lu


∞



θν(li,+k1) φkl (x) +

(i+1)

θ νl ,k φkl (x),

k=Lu +1

(i)

Vˆ (i) (x) = V

lim

νˆ l(i+1) (x) = ν (l i+1) (x).

(x),

(48)
(49)

Next, we will use mathematical induction to prove that
(i)

(i+1)

limLV ,Lu →∞ V (x) = V (i) (x) and limLV ,Lu →∞ ν l
for i = 0, 1, 2 . . . .
(0)

(x) = V (0) (x) and ν (l 1) (x) = νl(1) (x).

(b) For some i, assume that limLV ,Lu →∞ V
(i)

(i)

(i)

(x) = νl(i+1) (x)

(x) and ν (l i+1) (x) that

(i−1)

(x) = V (i−1) (x) and

limLV ,Lu →∞ ν l (x) = νl (x). According to (16) and (39),

T

θ

(i+1)

dV

[ν (i+1) (xl (τ ))]T

(i)

(x)

= −Q − W (ϕ(ˆν (i) )) + 2(ν (i+1) )T R[ϕ(ˆν (i) ) − u]. (50)

From (16) and (50),

t

l =1

LV


lim

LV ,Lu →∞

dt

 

(x) =

we have

l =1

= [
θ (i+1) ]T

(i)

νˆ l(i+1) (x) − ν (l i+1) (x) =

V

WL (xl , ul ) ρ (i) (xl , ul )



(47)

where ε1
max |el | and ε2
max ∥WL (x, u)∥. Note that
limLV ,Lu →∞ el = 0. Then, limLV ,Lu →∞ ε1 = 0, i.e., limLV ,Lu →∞
∥
θ (i+1) ∥ = 0. Considering

l =1
M


ε1 ε2 ,

(a) For i = 0, it follows from definitions of V

(i) (i+1)

(i)

(45)



i.e.,

LV ,Lu →∞

Combining (43) and (44) yields

(i+1)

∥WL (xl , ul )∥ |el |

k=1

(44)

(i+1)

M


η(i) − Z (i) θ

= M ∥
θ (i+1) ∥ε1 ε2

By using (38), the left side of (43) satisfies

[W (i)
θ (i+1) ]T [η(i) − Z (i) θ

T 

j =1

.

Multiplying [
θ (i+1) ]T on both sides of (42) yields

M δ1 ∥
θ (i+1) ∥2

φkj (xl (τ ))
t

l=1

Vˆ (i) (x) − V

i.e.,

t+ t



From (45) and (46), we have

(i)

[
θ (i+1) ]T (W (i) )T Z (i)
θ (i+1)

(i+1)

θ ν l ,k

k=Lu +1

(40)

(x) and ν (i+1) (x) can be represented as
∞ (i+1) l
(i+1)
V ( x) =
and ν l
( x) =
k=1 θ νl ,k φk (x),
respectively. Define the error weight vector 
θ (i+1) as
(i)

∞


∥
θ (i+1) ∥

1
(i)
− R−1 g T ∇ V .
2

(i)

rj

(0) = 0 and uˆ (i) = ϕ(ˆν (i) ). Define

From Section 4.2, V


θ (i+1)

m


× [ϕj (ˆν (i) (xl (τ ))) − uj,l (τ )]dτ .

(x) be the solution of the following equation

(i)

(i)

θ V ,k [ψj (xl (t )) − ψj (xl (t + t ))]

k=LV +1

|Vˆ (i) (x) − V (i) (x)| ε and ∥νˆ (i) (x) − ν (i) (x)∥ ε .
|Vˆ (i) (x) − V ∗ (x)| ε and ∥ˆν (i) (x) − ν ∗ (x)∥ ε .

(1)
(2)

∞


(38)

For ∀x ∈ X, ε > 0, there exist integers L1 , L2 , I1 > 0 such that if
LV
L1 , Lu L2 and i I1 , then

ν

M


V

(i)

(x(t )) − V (i) (x(t ))
 ∞
=
[W (ϕ(ˆν (i) )) − W (ϕ(ν (i) ))]dτ
t

156

B. Luo et al. / Neural Networks 71 (2015) 150–158

∞


+2

(ν (i+1) )T R[ϕ(ˆν (i) ) − ϕ(ν (i) )]dτ

t

∞


+2

[ν (i+1) − ν (i+1) ]T R[ϕ(ν (i) ) − u]dτ .

(51)

t

(i)

(i)

According to (49) and limLV ,Lu →∞ ν l (x) = νl (x), we have that
(i)
(i)
limLV ,Lu →∞ νˆ l (x) = νl (x). Then,
(i)

lim

V

(x) = V (i) (x),

(52)

lim

ν l(i+1) (x) = νl(i+1) (x).

(53)

LV ,Lu →∞

LV ,Lu →∞

Based on Eqs. (48), (49), (52) and (53), for ∀x ∈ X, ε > 0, there
exist integers L1 , L2 > 0 such that if LV
L1 and Lu L2 . Then,

ˆ (i)

(i)

(i)

ˆ (i)

(i)

|V (x) − V (x)| + |V (x) − V (x)|
ε
ε
+ = ε,

|V (x) − V (x)|

2

functional (2) is

2

∥ˆν (i) (x) − ν (i) (x)∥ + ∥ν (i) (x) − ν (i) (x)∥
ε
ε
+ = ε.

∥ˆν (i) (x) − ν (i) (x)∥

2

W (u) = 2

2

.

(54)

According to the part (1) of Theorem 1, there exist LV
Lu L2 such that

ε

|Vˆ (i) (x) − V (i) (x)|

2

.

L1 and

(55)

From (54) and (55), we have

|Vˆ (i) (x) − V ∗ (x)|

|Vˆ (i) (x) − V (i) (x)| + |V (i) (x) − V ∗ (x)|
ε
ε
+ = ε.
2

(i)

2

Similarly, ∥ˆν (x) − ν (x)∥
∗

ε . The proof is completed.

5. Simulation studies
In this section, we study the effectiveness of the developed
off-policy RL approach on a complex rotational/translational
actuator (RTAC) nonlinear benchmark problem (Abu-Khalaf, Lewis,
& Huang, 2008). The dynamics of the nonlinear plant poses
challenges as the rotational and translational motions are coupled.
In the simulation studies, select the weighted function vector as
(i)
WL (x, u) = ρ (i) (x, u). Then, W (i) = Z (i) and the parameter vector
update strategy (37) becomes a least-square scheme. The RTAC
system is given as follows:





x˙ = 




x2

−x1 + ζ sin x3
1 − ζ 2 cos2 x3
x24

x4

ζ cos x3 (x1 − ζ x24 sin x3 )
1 − ζ 2 cos2 x3
x0 = [0.4, 0.0, 0.4, 0.0]T ,



0





  −ζ cos x3 
 

  1 − ζ 2 cos2 x3 
+
 u,
 

0
 

 

1
1−ζ

2

cos2

β tanh−1 (µ/β)Rdµ

= 2β Ru tanh−1 (u/β) + β 2 R ln(1 − u2 /β 2 ).

2

(2) From Ref. Abu-Khalaf and Lewis (2005) and Theorem 1, for
I1 ,

ε

u


0

∀ε > 0, there exists integer I1 such that for ∀i
|V (i) (x) − V ∗ (x)|

(i)

Fig. 1. The norm ∥θˆV ∥ at each iteration.

(i)

x3

where ζ = 0.2. The input is constrained by |u|
β , where
β = 0.2. Let ϕ(µ) = β tanh(µ/β) and R = 1, then W (u) in cost

Let Q (x) = xT Sx with S = diag(0.5, 0.05, 0.05, 0.05). To learn
the constrained optimal control policy with the off-policy RL
method (Algorithm 1), select the basis function vectors as ΨN (x) =
[x21 , x1 x2 , x1 x3 , x1 x4 , x22 , x2 x3 , x2 x4 , x23 , x3 x4 x21 x24 , x1 x32 , x1 x22 x3 , x1 x22
x4 , x1 x2 x23 , x1 x2 x3 x4 , x1 x22 x4 , x1 x33 , x1 x23 x4 , x1 x3 x24 , x1 x34 , x42 , x32 x3 , x22
x23 , x2 x3 x4 , x22 x24 , x2 x33 , x2 x23 x4 , x2 x34 , x43 , x33 x4 , x23 x24 , x3 x34 , x44 ]T
with
the size of LV = 42, and ΦN (x) = [x1 , x2 , x3 , x4 , ϕ T (x)]T with
(0)
(0)
the size of Lu = 46, and initial θˆu as θˆu = [1.0, 1.0, −0.7,
T
−2.0, 0, . . . , 0] .
Collect sample set SM with size M = 1001, and compute
ρ Ψ (xk ), ρQ (xk ), ρul Φ (xk , uk ). Setting ξ = 10−5 , the simulation results show that at the 20th iteration (i.e., i = 20), the weight vec(20)
tors converge, respectively, to θˆV = [4.2970, 0.1216, −0.2196,
−0.8742, 4.0075, 0.2672, 0.7472, 0.1643, 0.4953, 0.7819,
0.8625, 1.9686, 2.1268, 0.5542, −1.6487, −0.3671, −0.6932,
−2.4891, −0.9257, 0.5616, 1.4230, 0.4928, 0.6333, 0.4141,
0.4928, 0.0537, 0.1653, 0.9117, 1.4092, 0.4641, −1.0433,
−0.1432, 0.0280, 1.0835, 0.1701, −0.5234, −0.5237, −0.0486,
−0.0376, 0.2384, −0.0521, −0.5238]T and θˆu(20) = [0.4421,
0.4591, −0.2291, −0.7333, 0.1905, −0.1791, −0.0575,
−0.2978, −0.0232, −0.0739, 0.1672, −0.0035, −0.0009,
0.0519, 0.4666, −4.6554, −3.1500, −0.9666, 1.2378, 0.2776,
0.1788, 8.9946, 3.1199, 3.2885, 2.5886, −3.9747, 3.9289,
−18.3913, −3.9747, 4.6712, −5.3350, −6.1230, 14.1708,
−1.4422, 0.7945, 3.5809, 0.7768, 2.1957, 1.9014, −1.5518,
−7.0940, 0.1421, −0.4144, −0.1659, 0.3712 −10.5140]T . The
(i)
norm of parameter vectors are shown in Figs. 1 and 2, where ∥θˆV ∥
(i)

and ∥θˆu ∥ converge to 8.1462 and 31.7143 respectively. By using
(20)
the convergent parameter vector θˆu , closed-loop simulation is
conducted with the final control policy uˆ (20) , and Figs. 3 and 5 give
the control and state trajectories, respectively. It is indicated from
Fig. 3 that the control constraint |u| 0.2 is satisfied. To show the
real cost generated by a control policy u, define
J (t )

t



Q (x(τ )) + W (u(τ ))dτ .
0

Fig. 4 gives the trajectory of J (t ) under the final control policy uˆ (20) ,
from which it is observed that J (t ) approaches to 0.6781 as time
increases.

B. Luo et al. / Neural Networks 71 (2015) 150–158

(i)

Fig. 2. The norm ∥θˆu ∥ at each iteration.

157

Fig. 3. The final control policy uˆ (20) .

6. Conclusions
An off-policy RL method has been developed for solving the
data-based constrained optimal control problem of nonlinear systems, which learns the optimal control policy from real system data
rather than mathematical model, and thus avoids the solution of
the complicated HJBE. Theoretically, it is found that the off-policy
RL method is equivalent to the model-based successive approximation approach for solving the HJBE, and thus its convergence
has been proved. To solve the iterative equation in the off-policy
RL method, the MWR and the numerically efficient Monte-Carlo
integration approaches have been introduced for its implementation. The off-policy RL algorithm is an offline control design procedure, which learns the constrained optimal control policy offline
and then is used for real online control purpose after the convergence of the algorithm. Finally, the effectiveness of the developed
off-policy RL method has been demonstrated through simulation
studies on a rotational/translational actuator system.
Acknowledgments
This work was supported in part by the National Natural
Science Foundation of China under Grants 61233001, 61273140,

Fig. 4. The cost J (t ) with the final control policy uˆ (20) .

61304086, and 61374105, in part by Beijing Natural Science
Foundation under Grant 4132078, in part by the Early Career
Development Award of SKLMCCS and in part by the NPRP grant
#NPRP 4-1162-1-181 from the Qatar National Research Fund (a
member of Qatar Foundation). The authors would like to thank
anonymous reviewers for their valuable comments.

Fig. 5. System state trajectories with the final control policy uˆ (20) .

158

B. Luo et al. / Neural Networks 71 (2015) 150–158

References
Abu-Khalaf, M., & Lewis, F. L. (2005). Nearly optimal control laws for nonlinear
systems with saturating actuators using a neural network HJB approach.
Automatica, 41(5), 779–791.
Abu-Khalaf, M., Lewis, F. L., & Huang, J. (2008). Neurodynamic programming and
zero-sum games for constrained control systems. IEEE Transactions on Neural
Networks, 19(7), 1243–1252.
Anderson, B. D., & Moore, J. B. (2007). Optimal control: linear quadratic methods.
Mineola, NY: Dover Publications.
Beard, R. W., Saridis, G. N., & Wen, J. T. (1997). Galerkin approximations
of the generalized Hamilton–Jacobi–Bellman equation. Automatica, 33(12),
2159–2177.
Bertsekas, D. P. (2005). Dynamic programming and optimal control. Vol. 1. Nashua:
Athena Scientific.
Chen, Z., & Jagannathan, S. (2008). Generalized Hamilton–Jacobi–Bellman
formulation-based neural network control of affine nonlinear discrete-time
systems. IEEE Transactions on Neural Networks, 19(1), 90–106.
Doya, K. (2000). Reinforcement learning in continuous time and space. Neural
Computation, 12(1), 219–245.
Faust, A., Ruymgaart, P., Salman, M., Fierro, R., & Tapia, L. (2014). Continuous action
reinforcement learning for control-affine systems with unknown dynamics.
IEEE/CAA Journal of Automatica Sinica, 1(3), 323–336.
He, P., & Jagannathan, S. (2005). Reinforcement learning-based output feedback
control of nonlinear systems with input constraints. IEEE Transactions on
Systems, Man, and Cybernetics, Part B: Cybernetics, 35(1), 150–154.
He, P., & Jagannathan, S. (2007). Reinforcement learning neural-network-based
controller for nonlinear discrete-time systems with input constraints. IEEE
Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 37(2),
425–436.
Heydari, A., & Balakrishnan, S. N. (2013). Finite-horizon control-constrained nonlinear optimal control using single network adaptive critics. IEEE Transactions
on Neural Networks and Learning Systems, 24(1), 147–157.
Hull, D. G. (2003). Optimal control theory for applications. Troy, NY: Springer.
Jiang, Y., & Jiang, Z.-P. (2012). Computational adaptive optimal control for
continuous-time linear systems with completely unknown dynamics. Automatica, 48(10), 2699–2704.
Jiang, Y., & Jiang, Z.-P. (2014). Robust adaptive dynamic programming and feedback
stabilization of nonlinear systems. IEEE Transactions on Neural Networks and
Learning Systems, 25(5), 882–893.
Lee, J. Y., Park, J. B., & Choi, Y. H. (2012). Integral Q-learning and explorized
policy iteration for adaptive optimal control of continuous-time linear systems.
Automatica, 48(11), 2850–2859.
Lendaris, G. G. (2009). Adaptive dynamic programming approach to experiencebased systems identification and control. Neural Networks, 22(5–6), 822–832.
Lewis, F. L., Vrabie, D., & Syrmos, V. L. (2013). Optimal control. Hoboken. NJ: John
Wiley & Sons.
Liu, D., Wang, D., & Li, H. (2014). Decentralized stabilization for a class of
continuous-time nonlinear interconnected systems using online learning
optimal control approach. IEEE Transactions on Neural Networks and Learning
Systems, 25(2), 418–428.
Liu, D., Wang, D., & Yang, X. (2013). An iterative adaptive dynamic programming
algorithm for optimal control of unknown discrete-time nonlinear systems
with constrained inputs. Information Sciences, 220, 331–342.
Liu, D., & Wei, Q. (2014). Policy iteration adaptive dynamic programming algorithm
for discrete-time nonlinear systems. IEEE Transactions on Neural Networks and
Learning Systems, 25(3), 621–634.
Luo, B., Wu, H.-N., & Huang, T. (2015). Off-policy reinforcement learning for H∞
control design. IEEE Transactions on Cybernetics, 45(1), 65–76.
Luo, B., Wu, H.-N., Huang, T., & Liu, D. (2014). Data-based approximate
policy iteration for affine nonlinear continuous-time optimal control design.
Automatica, 50(12), 3281–3290.

Lyashevskiy, S. (1996). Constrained optimization and control of nonlinear systems:
new results in optimal control. In Proceedings of the 35th IEEE decision and control
(pp. 541–546).
Lyshevski, S. E. (1998). Optimal control of nonlinear continuous-time systems:
design of bounded controllers via generalized nonquadratic functionals.
In Proceedings of the 1998 American control conference. Vol. 1 (pp. 205–209). IEEE.
Maei, H. R., Szepesvári, C., Bhatnagar, S., & Sutton, R. S. (2010). Toward offpolicy learning control with function approximation. In Proceedings of the 27th
international conference on machine learning (pp. 719–726).
Modares, H., & Lewis, F. L. (2014). Linear quadratic tracking control of partiallyunknown continuous-time systems using reinforcement learning. IEEE Transactions on Automatic Control, 59(11), 3051–3056.
Modares, H., Lewis, F. L., & Naghibi-Sistani, M.-B. (2013). Adaptive optimal control
of unknown constrained-input systems using policy iteration and neural
networks. IEEE Transactions on Neural Networks and Learning Systems, 24(10),
1513–1525.
Modares, H., Lewis, F. L., & Naghibi-Sistani, M.-B. (2014). Integral reinforcement
learning and experience replay for adaptive optimal control of partiallyunknown constrained-input continuous-time systems. Automatica, 50(1),
193–202.
Murray, J. J., Cox, C. J., Lendaris, G. G., & Saeks, R. (2002). Adaptive dynamic
programming. IEEE Transactions on Systems, Man, and Cybernetics, Part C:
Applications and Reviews, 32(2), 140–153.
Peter Lepage, G. (1978). A new algorithm for adaptive multidimensional
integration. Journal of Computational Physics, 27(2), 192–203.
Powell, W. B. (2007). Approximate dynamic programming: solving the curses of
dimensionality. Hoboken, NJ: John Wiley & Sons.
Precup, D., Sutton, R. S., & Dasgupta, S. (2001). Off-policy temporal-difference
learning with function approximation. In Proceedings of the 18th international
conference on machine learning (pp. 417–424).
Saridis, G. N., & Lee, C.-S. G. (1979). An approximation theory of optimal control for
trainable manipulators. IEEE Transactions on Systems, Man and Cybernetics, 9(3),
152–159.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: an introduction.
Cambridge: The MIT Press.
Vamvoudakis, K. G., & Lewis, F. L. (2010). Online actor–critic algorithm to solve the
continuous-time infinite horizon optimal control problem. Automatica, 46(5),
878–888.
Vrabie, D., & Lewis, F. L. (2009). Neural network approach to continuous-time direct
adaptive optimal control for partially unknown nonlinear systems. Neural
Networks, 22(3), 237–246.
Vrabie, D., Pastravanu, O., Abu-Khalaf, M., & Lewis, F. L. (2009). Adaptive
optimal control for continuous-time linear systems based on policy iteration.
Automatica, 45(2), 477–484.
Wang, D., Liu, D., & Li, H. (2014). Policy iteration algorithm for online design
of robust control for a class of continuous-time nonlinear systems. IEEE
Transactions on Automation Science and Engineering, 11(2), 627–632.
Wei, Q., & Liu, D. (2012). An iterative ϵ -optimal control scheme for a class of
discrete-time nonlinear systems with unfixed initial state. Neural Networks, 32,
236–244.
Yang, X., Liu, D., & Wang, D. (2014). Reinforcement learning for adaptive optimal
control of unknown continuous-time nonlinear systems with input constraints.
International Journal of Control, 87(3), 553–566.
Yang, X., Liu, D., Wang, D., & Wei, Q. (2014). Discrete-time online learning control for
a class of unknown nonaffine nonlinear systems using reinforcement learning.
Neural Networks, 55, 30–41.
Zhang, H., Luo, Y., & Liu, D. (2009). Neural-network-based near-optimal control for
a class of discrete-time affine nonlinear systems with control constraints. IEEE
Transactions on Neural Networks, 20(9), 1490–1503.
Zhao, Q., Xu, H., & Jagannathan, S. (2014). Near optimal output feedback control
of nonlinear discrete-time systems based on reinforcement neural network
learning. IEEE/CAA Journal of Automatica Sinica, 1(4), 372–384.

