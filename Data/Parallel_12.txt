Parallel Computing 49 (2015) 50–65

Contents lists available at ScienceDirect

Parallel Computing
journal homepage: www.elsevier.com/locate/parco

Reducing the memory footprint in Large Eddy Simulations of
reactive ﬂows
S. Weise∗, C. Hasse
Chair of Numerical Thermo-Fluid Dynamics, TU Bergakademie Freiberg, ZIK Virtuhcon, Fuchsmühlenweg 9, Freiberg 09599, Germany

a r t i c l e

i n f o

Article history:
Received 8 July 2014
Revised 1 June 2015
Accepted 14 July 2015
Available online 29 July 2015
Keywords:
Memory manager
Database
Parallel simulation

a b s t r a c t
CFD simulations of reactive ﬂows couple the domains of ﬂame chemistry and computational
ﬂuid dynamics. Solving the chemistry domain in-situ is extremely demanding. It is therefore
calculated beforehand and stored into a Look-up table (LUT), which is loaded in each MPIbased CFD application process in a parallel simulation. These chemistry database ﬁles can
become very large when including many variables and solution values, putting a limit on the
number of CFD mesh points as well as the solver instances which can be kept in RAM at the
same time. In the paper we approach this problem using dynamic memory managing techniques for single core and parallel applications effectively reducing the memory requirements
per core, while keeping the same database resolution. Different implementation options for
shared memory on compute nodes provided by the MPI-3 standard and MMAP as a POSIXcompliant system call are analyzed and evaluated using two test cases.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
In CFD simulations of reactive ﬂows an appropriate description of the chemical reactions is necessary in addition to the modeling of the ﬂuid ﬂow. Detailed chemical reaction mechanisms usually involve a signiﬁcant number of species and reactions
among them and both increase with fuel complexity. For each chemical species, a partial differential equation must be solved
with a source term, which is the sum of all reaction contributions described by the mechanism. In addition, an energy or enthalpy equation must be solved. The solution of the complete reactive system by itself requires optimized solvers due to its stiff
character. The associated smallest chemical time scales are usually smaller than the laminar/turbulent ﬂow time scales. Direct
numerical simulation of non-reactive systems at high Reynolds numbers is currently not feasible except for special applications. Thus, a reactive direct numerical simulation fully resolving all chemical and physical time and length scales is extremely
demanding (with HPC requirements signiﬁcantly higher than for the non-reactive case) already for small scale systems when
considering complex chemistry. For larger combustion systems, the application of DNS is currently restricted to simple chemical
mechanisms.
Instead of trying to perform a DNS, the use of suitable turbulence-chemistry interaction models such as ﬂamelet, FGM for FPI
[1–5] in combination with a RANS or LES modeling approach for the ﬂuid dynamics is very popular. These methods use pretabulated solutions of non-premixed or premixed ﬂames. These solutions are accessed during runtime and these retrieved solutions
replace the direct solution of the reactive subsystem. The parameter for these tables depend on the system and the number of
physical processes to be simulated, e.g. when considering non-adiabatic ﬂows, the enthalpy defect describing heat losses due to
∗

Corresponding author. Tel.: +49 3731394497.
E-mail address: steffen.weise@iec.tu-freiberg.de (S. Weise).

http://dx.doi.org/10.1016/j.parco.2015.07.004
0167-8191/© 2015 Elsevier B.V. All rights reserved.

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

51

radiation or at wall, becomes such a parameter [6]. In addition to the multi-dimensionality of the table, each retrieved solution is
a vector, which contains e.g. the species mass fractions and other variables such as density, temperature or heat capacities. These
two factors, the multi-dimensional table and the size of the solution vector, respectively, lead to table sizes which can easily
go beyond several 100 MB or more depending on the required accuracy. The database may even become larger than the RAM
available on the system, after subtracting the memory required for the CFD solver, the grid instances and the operating system
components. Also, other physical processes relevant for reacting and non-reacting ﬂows such as thermal radiation might require
additional databases of signiﬁcant size, see e.g. [7]. In a parallel application, the multidimensional table must be available for
each process, which in multi-core applications leads to a large overall memory footprint. This paper investigates techniques to
reduce this memory footprint and is structured as follows. First, in Section 2 an introduction to current trends in HPC systems
and software is given, which outline the targets and limitations for the memory management techniques presented in this paper.
It is followed in Section 3 by a description of approaches speciﬁc to reactive ﬂow simulations, showing the tool-chain used for
look-up-table generation and access of selected values in a CFD application. Section 4 gives an introduction to memory management terminology and techniques, which are then used to describe the methods that are applied later in this paper. These
tools are then applied in Section 5 to two combustion test cases. The effects and beneﬁts of the presented memory management
approaches are studied in Section 6. Finally the paper concludes with a discussion of the results in Section 7.

2. Development of HPC systems and software
High Performance Computers (HPC) today are large installations of commonly available hardware that follow computer industry trends in processor and system design closely. For years increased circuit density, known as Moore’s Law, translated into
higher clock frequencies. This increased the speed of existing single core applications without code modiﬁcation. Due to design
limits in power dissipation, the so called power wall [8], an increased number of processor cores is put on a die instead of increasing the clock rate. Programs have to be modiﬁed in order to make use of multi-threaded and multiprocessor programming
paradigms.
Computational Fluid Dynamics (CFD) has a long history of using multiple processors employing techniques such as domain
decomposition to decrease simulation time. The average amount of RAM per processor in specialized Symmetric Multiprocessor
Systems (SMP) has traditionally been high since there was a practical limit for the total number of processors in a machine that
shared the main memory. Now the number of cores translates into number of execution units, and the available RAM per core
decreases drastically [9,10]. The available RAM in HPC systems increases slowly and does not match the increase in number of
cores. This can become a serious issue [11] in highly-parallel MPI-based applications of combustion systems.
Modern RAM is still much slower than the CPU and this speed difference still increases [12,13]. This requires detailed attention
to any memory transfers inside modern computer systems. Modern highly parallel applications are mostly MPI-based [10] and
thus not aware that different instances on the same machine may use other communication and data exchange primitives than
message passing. Hybrid OpenMP/MPI applications [14–17] can solve this issue eﬃciently. MPI is in charge of machine to machine communication hosting one OpenMP application per machine. The OpenMP application has a shared memory layout since
OpenMP is a thread-based parallelization scheme. This shared memory is used to allow for communication and data exchange
between threads.
There have been similar application-speciﬁc approaches to reduce replicated data in MPI parallelized programs using shared
memory instead of OpenMP [18–20]. The MPI-3.0 standard [21] has recently added programming options, that makes MPI applications aware of the machine context, enabling communication groups that collect other processes which are using the same
shared memory. This kind of programming is called MPI+MPI programming [15,22].
It is clearly not feasible to rewrite a large existing MPI-based CFD application such as OpenFOAM, which is used in this work,
to make use of hybrid OpenMP/MPI. This paper explores two general implementation options using the MPI-3 standard and
a MMAP-based implementation, respectively. Both techniques are applied and analyzed for the speciﬁc problem of database
memory management in combustion simulations. This investigation builds upon a previous publication [23], where online compression algorithms were investigated as a ﬁrst step to reduce the memory footprint.

3. Reactive simulations using pretabulated chemistry
3.1. Flamelet progress variable approach
A widely used method for reactive ﬂow simulations with tabulated chemistry is the ﬂamelet progress variable (FPV) approach
[24–26]. The basic idea of the FPV approach is that the chemical state can be uniquely determined by the mixture fraction and
the progress variable. It requires the solution for the non-reactive mixture fraction Z and the reactive scalar progress variable YC .
The mixture fraction describes the mixing of fuel and oxidizer and is bounded between 0 (oxidizer) and 1 (fuel). It is obtained
by solving a suitable transport equation. These are given in the numerical setup of the test cases described in Sections 5.1.2
and 5.2.2.
The chemical state is determined by solving the ﬂamelet equations (with Z as the independent coordinate and varying scalar
dissipation rate), see Eqs. 1 and 2 for the temperature and the species mass fractions for a non-premixed system assuming unity

52

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Fig. 1. Flamelet look-up table generation and coupling to CFD applications. All ﬂamelet look-up generator (FLUG) processes are done a-priori. The ﬂameletConﬁg
and the appropriate FLUT are accessed during CFD simulation.

Le-numbers. The in-house ULF (Universal Laminar Flame) solver is used here.

ρ
ρ

∂T
χ ∂ 2T
χ ∂ T ∂ cp
−ρ
−ρ
−
∂t
2 ∂ Z2
2c p ∂ Z ∂ Z

N

ρ

i

∂ Yi
χ ∂ 2Yi
−ρ
− m˙ i = 0
∂t
2 ∂ Z2

1
χ c pi ∂ Yi ∂ T
+
2 cp ∂ Z ∂ Z
cp

N

m˙ i hi −
i

∂p
∂t

=0

(1)

(2)

where ρ is the density, T is the temperature, p is the pressure, cp is the speciﬁc heat capacity at constant pressure, hi is the
speciﬁc enthalpy, Yi is the species mass fraction and χ is the scalar dissipation rate. Based on the solution, the progress variable
is constructed as independent variable and replaces the scalar dissipation rate as look-up parameter. The progress variable is
used to represent the essential features of the chemical process. It is typically deﬁned by the sum of mass fractions of selected
product species, YP .

YC =

YP

(3)

The progress variable is normalized during the look-up table creation using Eq. (4) to allow for easier access in a look-up table.

C=

YC (Z ) − YC,min (Z )
YC,max (Z ) − YC,min (Z )

(4)

Further details concerning the transport of Z and YC in the CFD application, as well as deﬁnition of YC are speciﬁed in the
numerical description of each combustion test case. The progress variable is deﬁned following the procedure described in [27].
3.2. Flamelet look-up generation and CFD coupling
The Flamelet Look-Up Generator (FLUG) is a program speciﬁcally developed for reactive ﬂow simulations realized in OOP. It
provides a unique interface to simple ﬂame conﬁguration solvers. These either operate in ﬂamelet space or physical space for
premixed, non-premixed and partially premixed ﬂames. This allows for detailed ﬂamelet analysis for speciﬁc conditions. This
tool-chain allows for an eﬃcient implementation of new features and models. Its coupling with CFD applications is show in
Fig. 1.
The results of the FLUG runs are used to construct Flamelet Look-up Tables (FLUT), they are stored in a binary ﬁle embedded
in a HDF5 [28] container. An overview of the ﬁle format and its contents is given in the supplementary material.
3.3. FLUT access using ﬂameletConﬁg
The ﬂameletConﬁg library is a tool-set, which allows for eﬃcient access to the FLUT while reading its discretization as well
as default values from the HDF5 container. It is built as a library, which is linked to the CFD application during runtime. It also
performs check-sum tests and sets the appropriate access parameters, providing the user with a simpliﬁed access to the data.
A multivariate interpolation is performed inside the ﬂameletConﬁg, since the data stored in the FLUT is discretized with a
certain number of points. The interpolation weights are determined using a recursive n-dimensional multi-linear interpolation

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

53

MPI_Send & MPI_Recv

timer_t

of boundary cells
continuity equation

for each look-up
start timer_fc
MPI_Win_Fence**

FLUT lookup
Shared Memory Access *
stop timer_fc
continuity equation

pressure
correction

pressure velocity coupling

time step

momentum equation

pressure equation

turbulence

Fig. 2. Overview of CFD application message exchange in simulation runs. The processed highlighted in gray are running in turbulent instationary simulations
only. ∗ Depending on whether a shared memory manager is used. ∗∗ Only applicable when PMAL is used. A detailed description of the number of MPI_Win_Fence
calls and the syncing mechanism is given in Fig. 8.

method optimized for memory placement and computational eﬃciency. Some FLUTs require normalizing YC to C when requesting a FLUT solution vector, using tabulated boundary values for YC . The function required for this is selected when parsing the
FLUT. It is then connected to the interpolation function using function pointers inside the ﬂameletConﬁg.
To measure the time spent inside the ﬂameletConﬁg and inside the CFD application, a set of timers was implemented, which
run during a parallel application. One timer, later denoted using “_t”, is created inside the CFD application, only to be stopped
and evaluated at each boundary cell exchange, shown in Fig. 2.
The second timer is set up inside the ﬂameletConﬁg (denoted using “_fc”) and it is reset at every CFD boundary cell exchange.
It is resumed before each call to the ﬂameletConﬁg and stopped as soon as the call returns. Thus, it effectively measures the time
spent inside the ﬂameletConﬁg. The time measured by the second timer is therefore included inside the time measured by the
ﬁrst timer.
4. Memory management
One of the main tasks of an operating system is to manage the physical memory installed in a system. This is usually done
using a technique called virtual memory [29], in which physical addresses are mapped to virtual addresses in a continuous
virtual address space. The operating system manages the assignment of virtual and physical memory segments. Every program
on a system is provided with its own virtual address space (VAS). The program is placed into the VAS, as well as requested
memory pages and parts of shared libraries. The contents however are actually stored in physical memory. When running a
parallel reactive CFD simulation (see Fig. 3), several copies of the CFD application are put into physical memory, each gets their
own VAS. Each process then reads the appropriate mesh, solver settings and a copy of the FLUT from the ﬁlesystem. It is important
to note that truly shared resources, like shared libraries are stored in physical memory only once but may be mapped into several
VAS.
4.1. Memory manager
Memory management techniques are usually platform and operating system dependent. The basic techniques however have
been around for a long time and were ﬁrst described comprehensively by Denning [29] and Aho et al. [30]. They provide each
process with enough memory as long as there is enough free RAM available. Physical memory is usually split into pages, which
represent the smallest logical unit of memory that can be handled with reasonable effort by a memory manager. Memory pages
are assigned as long as there is enough memory available to satisfy every process request. If there are more process requests
for memory pages than free pages, the system is said to be under memory pressure [31]. Sleeping and running processes are

54

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Standard Simulation
FLUT-File
System
Memory

FLUT
CFD

OS

Process 0 CFD

0

Process 1 CFD

1

CFD

0

FLUT

1

FLUT

0

FLUT

1

FLUT

0

1

Fig. 3. Simpliﬁed memory usage layout on a cluster node during a standard parallel simulation. Several copies of a FLUT are read into memory by different
processes. OS – Operating System, CFD – CFD Solver & Grid and FLUT – ﬂamelet look-up table.

Standard Simulation
System
Memory

CFD

OS

FLUT

CFD

FLUT

1

FLUT

0

1

Simulation using MAL

FLUT

0

FLUT-File
System
Memory

0

1

FLUT

OS

Process 0

CFD

0

Process 1

CFD

1

CFD

FLUT

CFD

0

1

FLUT

0

FLUT

1

0

FLUT

1

Fig. 4. Simpliﬁed memory usage layout on a cluster node during a parallel CFD simulation, when using the MAL in comparison to a standard parallel CFD
simulation. Each process loads only a part of the FLUT. The total memory usage is therefore reduced. OS – Operating System, CFD – CFD Solver & Grid and FLUT –
ﬂamelet look-up table.

then stripped off their memory pages by pulling the content of used pages to lower entities in the cache hierarchy. There are
several algorithms to decide which pages are selected for this, the most famous one being the LRU-Algorithm [32]. It selects
those pages for replacement, which have been used least recently by the processes. This is an extension of the temporal data
locality assumption.
These ideas are used in the ﬂameletConﬁg to provide access to a FLUT. The main goal is to reduce the memory footprint of a
running application. The FLUT is virtually split into several blocks. Each block size should be a multiple of the smallest element
stored in the FLUT, e.g. a solution vector. Ideally the block should match the size of a memory page. An equally sized virtual
representation of the FLUT is created in memory and all ﬁle access towards the FLUT from the application is directed towards it.
The memory manager’s task is to translate the request to the virtual representation into actions, which allow access to the real
representation. Each block not yet loaded into real memory is usually fetched upon access request and stored for as long as it is
needed while there is no memory pressure, the latter leading to an early discard of the block. The overall memory footprint is
thus reduced since only active blocks are stored in memory.
Different memory managers were designed to perform these tasks. They are described in more detail in the following sections
and their performance is evaluated (see Section 6) using two test cases.

4.2. Memory Abstraction Layer
A low level memory manager called Memory Abstraction Layer (MAL) [23] was implemented in ANSI C using solely libc
library calls. Since this library is available on every modern operating system, the MAL can be compiled and run on almost every
system. The techniques used in the MAL are less complex than operating system tasks associated with memory handling, since
the content of a block can always be found in the database ﬁle. Every access to the database (on disk) however is signiﬁcantly
slower than any operation inside the main memory. The MAL therefore extends the LRU model to compress and decompress
operations on memory pages. This effectively introduces an additional layer in the cache hierarchy between main memory and
disk. The effect on memory usage in a parallel simulation is displayed in Fig. 4. Since only the parts of the FLUT, which were
actively requested by the application are loaded in each process, the memory usage for each process is smaller than in a standard
application (see Fig. 3 for reference). The total system memory usage also decreases, since the sum of all loaded memory pages
decreases.

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

55

Fig. 5. Simpliﬁed memory usage layout on a cluster node during a parallel CFD simulation using shared memory. Since all processes share access to the FLUT
representation, it contains all contents required by both processes 0 and 1 in the example. The merged contents (FLUTm ) are mapped in both processes, the
content however is stored only once in system memory. OS – Operating System, CFD – CFD Solver & Grid and FLUT – ﬂamelet look-up table.

Shared Memory Window Layout
Update Request
Access Plan
Access History
Block Management Table
Low Level Memory Manager
Managed Block
Storage Memory

Fig. 6. Layout of PMAL components in a shared memory window.

4.3. Parallel Memory Abstraction Layer (PMAL)
While the MAL reduces the load effort necessary for each individual process in a parallel simulation, it does not make any
use of the fact that several parallel processes may use the same part of the database ﬁle. This is especially an option when these
processes run on the same system, where the contents are stored next to each other in the main memory. The MAL also requires
each process to open a ﬁle pointer to the FLUT on the (network) ﬁlesystem. Parallel ﬁle operations may interfere with each other
depending on the implementation of the ﬁle locking mechanism on the ﬁlesystem. Due to these limitations a parallel version of
the MAL (PMAL) was developed which is presented here.
The PMAL is based on the MPI standard and uses a set of features, which have been added in the approved MPI-3.0 standard
[21]. A shared memory window is used to exchange memory contents between different processes running the same code. This
window is also used for communication between the clients concerning memory operations. The MPI-3.0 standard introduced
a communicator splitting option MPI_COMM_TYPE_SHARED, which allows the application programmer to locally aggregate all
processes on a machine accessing a shared memory. It thus provides an abstraction, which allows for communication, exchange
and grouping of process by shared memory without knowing the underlying hardware implementation. A simpliﬁed overview
of the memory usage in such a parallel application is given in Fig. 5. The implementation of the MPI library itself deﬁnes the
underlying function calls towards the operating system.
The implementation of a low level shared memory manager is placed into the shared memory window and allows for concurrent parallel access to all blocks stored into the memory window. The contents of this memory window are several object
oriented components, such as update request information and a low level memory manager including its managed memory
region. The general layout is displayed in Fig. 6.
Each process 0 in a shared communicator handles the ﬁle operations on the FLUT and is allowed to store and delete blocks.
This is shown in Fig. 7. All other processes in this communication group, called clients, have to communicate with this process
0 which is referred to as master process from now on. All master processes are independent from other master processes on

56

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Fig. 7. Overview of parallel memory abstraction access model. Every access on the FLUT is directed towards a virtual representation, which is ﬁlled with parts
of the FLUT only when required. All FLUT ﬁle operations are handled by the master process. This is a machine perspective only, where the shared FLUT segments
are stored in system main memory.

master

client

sync

sync

- access to virtual FLUT
- get from FLUT if required,
store in virtual FLUT

- access to virtual FLUT
- mark request if not found

- check for marked requests from clients
- get from FLUT if required,
store in virtual FLUT
sync
- access pulled contents
in virtual FLUT
sync
Fig. 8. Overview of syncing mechanism used inside the PMAL implementation. Every sync mentioned here is a call to MPI_Win_Fence, which is a barrier
synchronization for master and client processes.

different machines, in terms of memory management. They are however coupled by the MPI communications required to run
the CFD simulation. One of these couplings, boundary cell exchange, is displayed in Fig. 2.
To access the contents of the memory window, a simple syncing mechanism was used, that relies on the use of
MPI_Win_Fence. Each call to the MPI_Win_Fence should start and close a memory access epoch according to the MPI-3.0 standard. It implies a barrier synchronization for all participating processes. A call to MPI_Win_Fence thus completes only after all
other processes in the group also entered the matching call. It is therefore guaranteed that all operations required to synchronize
the window contents have ﬁnished.
The syncing mechanism (see Fig. 8) starts by allowing each process to access the required contents of the FLUT representation
inside the memory window. Missing contents are marked in the update request part of the memory window by the clients, see
Fig. 6, while the master process is allowed to fetch the contents it needs from ﬁle and stores them in the memory window. This
access epoch is then ended by syncing. In the next step, the master fetches all the memory contents required by the clients and
stores them in the memory window, ﬁnishing with a sync, while the clients simply wait for the next sync. The master process is
waiting in the next epoch while the client processes pull the memory contents from the shared window they needed in the ﬁrst
epoch. This epoch as well as the entire syncing mechanism is ﬁnished by a last MPI_Win_Fence. The selected syncing mechanism
is reliable but slower than others due to strong use of barrier synchronization. The merits in terms of memory usage however
are optimal, as this approach does not infer any internal caching or prediction, nor does it create any access tracking structures.
The current PMAL implementation is primarily focused on code- and state-correctness to examine the principles, gains and
drawbacks of this implementation option of a dynamic memory manager. Allowing more processes to load contents from the
disk and storing these in the shared memory window can allow for a different synchronization mechanism, that may increase
performance. Changing the implementation details may also aid in tuning it for other database driven applications.
There are 3 major ways to implement shared memory in Linux/UNIX systems, System V shared memory, memory mappings
and POSIX shared memory. Depending on the implementation chosen by the MPI library, operating systems limits have to be
considered when creating this shared memory window. In case of POSIX shared memory, the memory window has to be smaller
than the space made available in /dev/shm. The MPI libraries add their own syncing objects, depending on the number of clients,
inside the memory window. The limits for /dev/shm however can only be set by an administrator. Limits such as minimum
shared memory size, maximum size or number of shared memory segments depend on the kernel conﬁguration.

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

57

The overall runtime impact of shared memory window synchronization using the PMAL can be eliminated in a special case.
This is possible when the free memory available in a shared memory during a parallel simulation is larger than the look-up
table. A read-only copy of the look-up table can be stored inside the shared memory window. The PMAL also allows for eﬃcient
adaptive compression of database parts in memory, extending the storage capabilities in system main memory.
4.4. Memory map
Memory Map, a technique which is usually abbreviated MMAP, is a POSIX system call that allows for ﬁle input and output. It is
available in the Linux operating system, Mac OS X and the Berkeley System Distribution and its derivatives. The general concept
of a memory mapped ﬁle is also available in the Microsoft Windows API [33]. It maps a ﬁle’s content into the VAS of a process,
allowing for eﬃcient ﬁle access. This mapping can be accessed by multiple processes without the need for further memory
to be allocated. In general it provides the same functionality as the PMAL (see Fig. 5), however the details of implementation
are left to the operating system/kernel. The content of the ﬁle is not modiﬁed and therefore represented in memory as seen
on disk. Any calls to syncing of ﬁle changes, which would usually require an explicit call to msync(), can therefore be avoided.
Accessing mapped parts of the VAS will result in ﬁle reads containing the requested part of the ﬁle. File contents will be held in
memory as long as there is enough memory available. Since it is hard to optimize the mechanism for every conceivable behavior
several options can be given to the system call. The mechanism can be optimized for e.g. sequential ﬁle operations or random
ﬁle operations. Most of these options are highly system-speciﬁc. The kernel has to allocate internal structures in order to manage
the page association. Address resolutions of memory maps, between physical memory addresses and virtual memory addresses,
however are computationally cheap because the operating system uses the Transaction Look-aside Buffer (TLB).
Due to memory saving techniques inside the operating system, it is often hard to determine the effective memory usage in
a parallel simulation. Many shared libraries are shared between processes, and the speciﬁcs are only visible to the operating
system. Memory usage sizes are therefore best taken from the /proc ﬁlesystem in Linux, which is a pseudo-ﬁlesystem that is
an interface to internal kernel data structures. The main memory usage factor we will focus on is PSS (proportional set size),
which can be read for any ﬁle mapping using /proc/ < PID > /smaps (PID is a unique process id associated individually to each
running process). It is a proportional share of a ﬁle opened using MMAP. The sum of all PSS of all processes however gives the
total memory usage of all processes on a ﬁle.
Working with compression when using the MMAP implementation is based on storing a precompressed database in RAM using MMAP. The appropriate memory page has to be decompressed before it can be accessed, a buffer of uncompressed pages has
to be kept in memory to make this an eﬃcient technique. The Linux Kernel supports this since version 3.14 using a feature called
zram [34]. The compression block size is then based on the page size, which may be not as eﬃcient as a PMAL implementation
where the block size is tuned to data characteristics.
5. Combustion test case description
In the following sections, the two reactive test-cases are described in detail.
5.1. POX-ﬂame
5.1.1. Physical setup
The POX ﬂame [35–37] was designed as a laminar reference ﬂame for partial oxidation conditions. The setup for this inverse
diffusion ﬂame consists of two concentric tubes, an inner oxidizer tube and an outer fuel tube. Both inlets are laminar. The
oxidizer tube has an inner diameter of 15 mm and a wall thickness of 0.5 mm. The fuel tube has an inner diameter of 30 mm it
is extended by a 500 mm quartz glass tube. This conﬁnement shields the ﬂame from the surrounding air. Table 1 gives the inlet
gas compositions as well as temperature and velocity (Fig. 9).
5.1.2. Numerical setup
The OpenFOAM-2.1.0-based solver used is ﬂameletFoam with in-house developed combustion submodels. It uses the
ﬂameletConﬁg library and allows the ﬂexible selection of the three memory managers.

Table 1
Mixture fraction, progress variable, temperature and velocity (component in ﬂow direction) inlet boundary conditions for the POX ﬂame.

Fuel
Oxidizer

Z

YC

T (K)

Uz (m/s)

1
0

0.733 (C = 0)
0.0

300
300

0.262
0.3

58

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Ø 30 mm

endothermic zone
500 mm

Zst
flame zone

0.5 mm

Ø 15 mm
Z=1

Fuel

Z=0

Oxidizer

Z=1

Fuel

Fig. 9. The laminar POX ﬂame setup, displayed in a cross-section.

Fig. 10. Overview of ﬂamelet coupling in the laminar POX ﬂame.

The transport equations for the mixture fraction Z and the progress variable YC are given in Eqs. (5) and (6), where ρ is the
density, u the velocity vector and D the molecular diffusivity.

∇ · (ρ uZ ) = ∇ · (ρ D∇ Z )

(5)

∇ · (ρ uYc ) = ∇ · (ρ D∇Yc ) + ω˙ Yc ,

(6)

The progress variable is deﬁned as:

YC = YCO + YCO2 + YH2 O

(7)

Based on the solutions of Eqs. (1) and (2), the chemical source term ω˙YC is computed and stored in the look-up table. The
values exchanged in each iteration between the ﬂameletConﬁg and the solver are displayed in Fig. 10. The FLUT generated for
this case has 2 input parameters, Z and C, both are deﬁned between 0 and 1. They are discretized using 1001 points for Z and 500
points for C. Every solution vector has 65 solution values, which contain 53 species mass fractions as well as other properties
like density, enthalpy, heat capacity, viscosity, YC -max, YC -min, temperature, mean molar mass, thermal conductivity, thermal
diffusivity, scalar dissipation rate at stoichiometry and the progress variable source term ω˙YC , respectively. This results in a total
size of 249 MB.
The numerical grid used has 3,187,800 cells with an O-grid structure. It has a variable grading in main ﬂow direction to
increase the cell density and maintain a good aspect ratio in the inlet zone. The grid was decomposed into 60 domains, grouped
for minimal communication across the network interconnect. The domains are displayed in Appendix A.

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

59

Fig. 11. The turbulent Sandia Flame D setup. The numerical 3D grid is an O-grid, with 1,469,055 cells. The physical dimensions of the grid are: length = 300 mm,
diameter = 147 mm, Dref = 7.2 mm. Also shown are the instantaneous temperature ﬁeld (left) and the averaged temperature (right).
Table 2
Mixture fraction and progress variable inlet boundary conditions for the Sandia
ﬂame.

Jet
Pilot
Co-ﬂow

Z

YC

1
0.271
0

0
0.20831
0.00626 (C = 0)

5.2. Sydney/Sandia Flame D
5.2.1. Physical setup
The Sydney/Sandia jet burner ﬂames are a collection of piloted jet ﬂames originally designed to provide high quality data for
model validation. They are described in detail in the proceedings of the TNF workshop [38]. These ﬂames were ﬁrst measured
by Barlow et al. [39,40]. Later velocity data were provided by Schneider et al. [41]. Here, we look speciﬁcally at Flame D. The
simulation geometry is displayed in Fig. 11, it is positioned above the jet exit plane. The inner jet is a partially premixed gas
composed of 25% methane and 75% air by volume. The pilot consists of a burnt gas. The corresponding values for Z and YC are
given in Table 2. The ﬂames are shielded from lab air by an air co-ﬂow and the whole experiment is performed at atmospheric
pressure, fuel and air temperatures are 291 K.
5.2.2. Numerical setup
The LES transport equations for the mean mixture fraction Z and mean progress variable YC are given as:

∂ρ Z
+ ∇ · (ρ uZ ) = ∇ · (ρ(D + Dt )∇ Z )
∂t

(8)

∂ρYC
+ ∇ · ρ uYC = ∇ · ρ(D + Dt )∇ YC + ω˙ YC
∂t

(9)

Z

2

= KZ

2
2

2

∇Z ,

(10)

60

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Fig. 12. Overview of the ﬂamelet coupling in the turbulent Sandia ﬂame.
Table 3
Hardware and software speciﬁcations of a cluster node used
in both setups. A shared version of the MPI libraries was
compiled and used. No additional parameters were set during compilation. Running the parallel commands was issued
without additional tuning parameters.
Hardware
CPU
RAM
Interconnect
Interconnect
Software
Operating system
Kernel
MPI
Compiler
OpenFOAM

2× Xeon E5-2680v2
128 GB
Inﬁniband FDR 4×
Gigabit Ethernet
Debian 6.0
3.12.4 custom
MPICH 3.0.4
gcc-4.5.1
2.1.0

where an algebraic closure for the mean mixture fraction variance Z 2 was chosen according to Forkel et al. [42]. The LES ﬁlter
width is represented by and K 2 = 0.13. The progress variable is deﬁned as:
Z

YC = YCO + YCO2 + YH2 O + YH2

(11)

The simulation was run using an in-house OpenFOAM-based solver. The values exchanged in each iteration between the
ﬂameletConﬁg and the solver are displayed in Fig. 12. The σ -model from Nicoud et al. [43] was used to model the turbulent
viscosity.
The FLUT used during this simulation has a size of 185MB. It has 3 addressing parameters Z, YC and Z 2 , discretized using
101, 114 and 81 points (each ranging from 0 to 1 in normalized form). Each solution vector contains 26 values composed of 7
selected species mass fractions, 7 selected species mole fractions, density, enthalpy, heat capacity, YC -min, YC -max, temperature,
thermal diffusivity, viscosity, Bilger mixture fraction, scalar dissipation rate at stoichiometry, error function complement ratio
(comparing to stoichiometric error function) and mean progress variable source term ω˙YC .
The numerical grid is described in Fig. 11. The grid has a variable grading in main ﬂow direction and is decomposed into 40
domains. The domains are displayed in Appendix A.
6. Results and discussion
6.1. POX ﬂame
The POX ﬂame analysis was started from a fully developed ﬂame (see Fig. 13) in a converged state, running on 60 cores on
3 cluster nodes (see Table 3). Since the look-up parameters are not changing during the analysis, the memory usage in Fig. 14
is shown as a bar graph. All memory managers show a signiﬁcant improvement over the standard simulation where the FLUT
would have been stored 60 times inside the system main memory. Using the MAL the memory usage can be reduced by a
factor of approximately 6. Both shared memory based memory managers (PMAL+MMAP) show a signiﬁcantly smaller memory
usage than the MAL. The MMAP uses slightly more memory than the PMAL. Only runtimes spent inside the ﬂameletConﬁg
are evaluated here, the runtime spent inside the CFD solver is approx. 12 s per time-step. A simulation that does not use a
memory manager would require the same time in the CFD domain. Neglecting the initial load time of the database, this allows
an approximation of the runtime increase when using a dynamic memory manager. The MAL (1.7% increase) and MMAP (1.6%
increase) are comparable in terms of runtime, while the PMAL (64% increase) implementation is much slower.

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

61

Fig. 13. Center slice of the 3D POX ﬂame simulation in converged state. The temperature is in Kelvin.

8000
memory usage
runtime

800

7000

700
6000
600
5000
500
4000
400
3000
300

run-time in msec

memory usage in percent of database size

900

2000
200
2.04%
100

0

FLUT

1000

MAL

MMAP

PMAL

0

Fig. 14. Memory usage and runtime behavior of different memory manager implementations in the POX ﬂame simulation. The memory usage is displayed in
percent of the database size (100% is given as reference). A standard simulation without any additional memory management would require 60 ∗ 100% = 6000%.

62

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

memory usage in percent of database size

1200

1000

800
MAL
PMAL
MMAP
FLUT

600

400

200
100
0

0

5000

10000 15000 20000 25000 30000 35000 40000 45000 50000

number of time steps
Fig. 15. Sandia ﬂame D memory usage shown over 50,000 time-steps. The memory usage is displayed in percent of the database size (100% is given as reference).
A standard simulation without any additional memory management would require 40 ∗ 100% = 4000%.

Fig. 16. Timing of different memory management techniques during the LES. Timings with appended “_t” are total runtimes including CFD. Timing marked with
“_fc” is runtime spent inside the ﬂameletConﬁg library, which includes memory management and interpolation. The runtimes are evaluated in each time step.

6.2. Sandia Flame D
The Sandia Flame D simulation was run on 40 cores distributed on 2 cluster nodes (see Table 3). For the subsequent analysis of
the memory footprint it was restarted from a state where the ﬂame was fully developed and it already had a proﬁle as displayed
in Fig. 11. This procedure avoids any bias due to initial unphysical perturbations. The temperature plot shows an instantaneous
proﬁle on the left. The right side shows the mean temperature, averaged over 3 ﬂow-through times.
The instationary simulations were continued for 50,000 time steps, in order to monitor the dynamic memory requirements
in the turbulent simulations. The selected time-span equals 20 ms which is 3 ﬂow-through times. The memory requirements for
each of the memory manager implementations are shown in Fig. 15. The shared memory implementations have a clear beneﬁt
over the MAL implementation in terms of memory usage. The difference in memory requirement between MMAP and PMAL

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

63

Fig. 17. Timing in the ﬂameletConﬁg library for different memory management techniques during the CFD simulation.

is very small. The PMAL uses more memory than the MMAP. The MAL as well as the PMAL show a fast increase in memory
usage during the ﬁrst 2000 time-steps. It further progresses slowly towards a steady value. Fluctuations in the velocity ﬁeld and
consequently also in Z and YC lead to variations in the FLUT look-up. This instationary effect has a larger impact on the MAL
adaptivity, which is limited since processes on different domains are bound to the locality of their domain. The MMAP therefore
adapts much faster to these changes.
The total runtime of the simulation is determined by the sum of time spent inside the ﬂameletConﬁg and in the CFD solver.
All times given in Figs 16 and 17 are summed up over the entire cell update loop as described in Section 3.3. The interpolation
effort however varies since the interpolation algorithm determines several weights, which requires less computational effort
when the values of a left and right neighbor of an interpolation are identical. The time spent interpolating is approximately
10 ms per time-step. The retrieval time also differs, as the contents of memory blocks are stored and retrieved in the cache
hierarchy. The MAL and MMAP are much faster than the PMAL, which is a result of the explicit syncing mechanism used in the
PMAL. The MAL is even faster than the MMAP, as seen in Fig. 17 since it does not require any kind of synchronization between
the processes based on memory transfers of FLUT contents. The PMAL exceeds the CFD runtime and thus signiﬁcantly increases
the total runtime of the simulation. The relative runtime increase when compared to a simulation without a memory manager
and using the same assumptions as mentioned in Section 6.1, is 5.9% for the MAL, 9.9% for the MMAP an 448% for the PMAL,
respectively.

7. Summary and conclusions
Parallel reactive ﬂow simulations with tabulated chemistry usually require an individual copy of the table per running process. Due to new developments in hardware design, the RAM per core available on modern computer systems decreases. This
limits the RAM available for large tables. This issue is addressed in this paper. Different memory management techniques were
developed and their performance in parallel 3D simulations was evaluated. Two combustion test cases, a laminar POX ﬂame and
the turbulent Sandia ﬂame D were selected, the latter being one of the most widely used benchmark test cases for turbulent
combustion modeling.
The application of memory managers in parallel reactive ﬂow simulations decreases the total memory usage signiﬁcantly.
Memory managers that employ shared memory programming techniques (MMAP and PMAL) are more eﬃcient than single
process memory managers (MAL). On the other hand MMAP and PMAL require synchronization, which increases the runtime
compared to a single process memory manager. Depending on the level of ﬁne-grained control on memory operations and
interaction options, two options for implementing such a memory manager were presented. The MMAP allows for ﬁne-grained
control while using system calls directly, handing over memory contents to the operating system. The PMAL builds on top of MPI
library topology, allowing for fast adaptation to a problem set, but requires an implementation of a synchronization mechanism
on a relatively high abstraction level. It also strongly depends on the implementation quality of the MPI library.

64

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65

Acknowledgments
The authors kindly acknowledge the ﬁnancial support by the Federal Ministry of Education and Research of Germany in the
framework of Virtuhcon (Project number 03Z2FN11).
Appendix A. Domain decomposition

The MPI ids have been pinned to the machines in ascending order. Node 1 has ids from 0 to 19, node 2 has ids from 20 to 39.
Node 3 has ids from 40 to 59. This layout allows for different ﬂame zones with similar conditions and thus similar requests, to be
grouped on the same machine. Different decompositions and MPI id associations will lead to different memory usage proﬁles.
Supplementary material
Supplementary material associated with this article can be found, in the online version, at 10.1016/j.parco.2015.07.004.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]

J.v. Oijen, L.D. Goey, Combust. Sci. Technol. 161 (1) (2000) 113.
O. Gicquel, N. Darabiha, D. Thévenin, Proc. Combust. Inst. 28 (2) (2000) 1901.
J.V. Oijen, F. Lammers, L. De Goey, Combust. Flame 127 (3) (2001) 2124.
P.D. Nguyen, L. Vervisch, V. Subramanian, P. Domingo, Combust. Flame 157 (1) (2010) 43.
B. Fiorina, R. Baron, O. Gicquel, D. Thévenin, S. Carpentier, N. Darabiha, Combust. Theory Model. 7 (3) (2003) 449.
M. Ihme, H. Pitsch, Phys. Fluids 20 (055110) (2008) 1–20.
A. Wang, M.F. Modest, Int. J. Heat Mass Transf. 50 (19) (2007) 3877–3889.
H. Sutter, J. Larus, Dr. Dobb’s J. 30 (3) (2005) 202–210.
B. Raayman, http://basraayman.com/2009/12/29/the-ram-per-cpu-wall/, 2009 (accessed 27.04.14).

S. Weise, C. Hasse / Parallel Computing 49 (2015) 50–65
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]

65

J. Dinan, P. Balaji, E. Lusk, P. Sadayappani, R. Thakur, Proceedings of the 7th ACM International Conference on Computing Frontiers, 2010.
B. Franzelli, E. Riber, M. Sanjosé, T. Poinsot, Combust. Flame 157 (7) (2010) 1364.
Techcenter-Dell, http://en.community.dell.com/techcenter/high-performance-computing/w/wiki/2284.aspx, 2011 (accessed 10.05.14).
W.A. Wulf, S.A. McKee, ACM SIGARCH Comput. Architect. News 23 (1) (1995) 20.
R. Rabenseifner, G. Hager, G. Jost, Proceedings of the Cray Users Group Conference, 2009.
R. Rabenseifner, G. Hager, G. Jost, Supercomputing Conference on Hybrid MPI and OpenMP Parallel Programming, 2013.
L. Smith, M. Bull, Sci. Program. 9 (2-3) (2001) 83–98.
E. Ayguade, M. Gonzalez, X. Martorell, G. Jost, J. Parallel Distrib. Comput. 66 (5) (2006) 686–697.
S. Laizet, N. Li., Int. J. Numer. Methods Fluids 67 (11) (2011) 1735–1757.
M.R. de Jonge, H.M. Vinkers, J.H. van Lenthe, F.F.D. Daeyaert, I.J. Bush, H.J.J. van Dam, P. Sherwood, M.F. Guest, AIP Conf. Proc. 940 (2007) 168–178.
I.J. Bush, HPCx Technical Report, STFC Daresbury Laboratory, 2007.
MPI Forum, MPI: A Message-Passing Interface Standard. Version 3.0, http://www.mpi-forum.org, 2012 (accessed 1.5.14).
T. Hoeﬂer, J. Dinan, D. Buntinas, P. Balaji, B. Barrett, R. Brightwell, W. Gropp, V. Kale, R. Thakur, Computing 95 (12) (2013) 1121–1136.
S. Weise, D. Messig, B. Meyer, C. Hasse, Combust. Theory Model. 17 (3) (2013) 411.
C. Pierce, P. Moin, J. Fluid Mech. 504 (2004) 73.
M. Ihme, C.M. Cha, H. Pitsch, Proc. Combust. Inst. 30 (1) (2005) 793.
E. Knudsen, H. Pitsch, Combust. Flame 156 (2009) 678.
U. Prüfert, S. Hartl, F. Hunger, D. Messig, M. Eiermann, C. Hasse, Flow Turbul. Combust. 94 (3) (2015) 593–617.
HDF-Group, http://www.hdfgroup.org, 2014 (accessed 3.05.14).
P.J. Denning, ACM Comput. Surv. 2 (3) (1970) 153.
A.V. Aho, P.J. Denning, J.D. Ullman, J. ACM 18 (1) (1971) 80.
R. Mills, A. Stathopoulos, D. Nikolopoulos, Proceedings of the 18th International Parallel and Distributed Processing Symposium, 2004.
P.J. Denning, Commun. ACM 11 (5) (1968) 323.
Microsoft Windows File Mapping API, https://msdn.microsoft.com/en-us/library/aa366556(v=vs.85).aspx, 2015 (accessed 26.03.15).
Linux 3.14, http://kernelnewbies.org/Linux_3.14, 2014 (accessed 26.03.15).
B. Stelzner, F. Hunger, A. Laugwitz, M. Gräbner, S. Voss, K. Uebel, M. Schurz, R. Schimpke, S. Weise, S. Krzack, D. Trimis, C. Hasse, B. Meyer, Fuel Process.
Technol. 110 (0) (2013) 33.
F. Hunger, B. Stelzner, D. Trimis, C. Hasse, Flow Turbul. Combust. 90 (4) (2013) 833.
B. Stelzner, F. Hunger, S. Voss, J. Keller, C. Hasse, D. Trimis, Proc. Combust. Instit. 34 (1) (2013) 1045.
R. Barlow (Ed.), TNF3 Proceedings, Sandia National Laboratories (1998) http://www.ca.sandia.gov/tdf.
R. Barlow, J. Frank, Proc. Combust. Inst. 27 (1) (1998) 1087.
R. Barlow, J. Frank, A. Karpetis, J.Y. Chen, Combust. Flame 143 (4) (2005) 433.
C. Schneider, A. Dreizler, J. Janicka, E. Hassel, Combust. Flame 135 (1–2) (2003) 185.
H. Forkel, J. Janicka, Flow Turbul. Combust. 65 (2000) 163.
F. Nicoud, H.B. Toda, O. Cabrit, S. Bose, J. Lee, Phys. Fluids 23 (8) (2011) 085106.

