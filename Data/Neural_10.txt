Neural Networks 71 (2015) 11–26

Contents lists available at ScienceDirect

Neural Networks
journal homepage: www.elsevier.com/locate/neunet

Complex Rotation Quantum Dynamic Neural Networks (CRQDNN)
using Complex Quantum Neuron (CQN): Applications to time
series prediction
Yiqian Cui ∗ , Junyou Shi, Zili Wang
School of Reliability and Systems Engineering, Beihang University, Beijing, China
Science and Technology Key Laboratory on Reliability and Environmental Engineering, Beihang University, Beijing, China

highlights
•
•
•
•

A novel hybrid networks model Complex Rotation Quantum Dynamic Neural Networks (CRQDNN) is proposed.
Deep quantum entanglement is realized by incorporating Complex Quantum Neurons (QRN).
The embedded IIR filter structure enables the dynamic properties to treat with time series input.
The application studies of chaotic time series prediction and electronic prognostics are investigated.

article

info

Article history:
Received 3 December 2014
Received in revised form 2 July 2015
Accepted 23 July 2015
Available online 31 July 2015
Keywords:
Quantum entanglement
Complex Quantum Neuron (CQN)
Infinite Impulse Response (IIR)
Chaotic time series prediction
Remaining Useful Life (RUL) prediction

abstract
Quantum Neural Networks (QNN) models have attracted great attention since it innovates a new neural
computing manner based on quantum entanglement. However, the existing QNN models are mainly
based on the real quantum operations, and the potential of quantum entanglement is not fully exploited.
In this paper, we proposes a novel quantum neuron model called Complex Quantum Neuron (CQN)
that realizes a deep quantum entanglement. Also, a novel hybrid networks model Complex Rotation
Quantum Dynamic Neural Networks (CRQDNN) is proposed based on Complex Quantum Neuron (CQN).
CRQDNN is a three layer model with both CQN and classical neurons. An infinite impulse response (IIR)
filter is embedded in the Networks model to enable the memory function to process time series inputs.
The Levenberg–Marquardt (LM) algorithm is used for fast parameter learning. The networks model is
developed to conduct time series predictions. Two application studies are done in this paper, including
the chaotic time series prediction and electronic remaining useful life (RUL) prediction.
© 2015 Elsevier Ltd. All rights reserved.

1. Introduction
Time series prediction refers to the study of the present and past
behavior of the system for the prediction of the future (Widiputra,
Pears, & Kasabov, 2011). Artificial Neural Networks (ANN) are
universally employed in time series prediction since they have
the properties such as self-organizing, data-driven, self-study, selfadaptive and associated memory (Zhang, Eddy Patuwo, & Y Hu,
1998).
Traditional static ANN models do not incorporate the temporal cumulative effect because a single input sample is either irrelative to time or relative to a moment instead of a period of time.

∗ Corresponding author at: School of Reliability and Systems Engineering,
Beihang University, Beijing, China.
E-mail addresses: yiqiancui@163.com (Y. Cui), shisjy@hotmail.com (J. Shi),
wangzili2014@yahoo.com (Z. Wang).
http://dx.doi.org/10.1016/j.neunet.2015.07.013
0893-6080/© 2015 Elsevier Ltd. All rights reserved.

The time-varying effect is supposed to be incorporated in neural
networks models (Kim, 1997). To better deal with the time series
inputs, from about 1990s, great attention has been paid to the development of Dynamic Neural Networks (DNN) due to their capabilities in modeling nonlinear dynamical systems (Lou & Cui, 2007;
Meyer-Bäse, Ohl, & Scheich, 1996).
Recently, Quantum Neural Networks (QNN) models have attracted great attention worldwide because of its novel computational characteristics. Quantum entanglement is involved in the
quantum networks modeling, which is responsible for the associations between input and output patterns in the proposed architecture (Svitek, 2008). The quantum entanglement mechanism can
be well brought in neural networks modeling that provides more
adaptability for parameter learning of the networks. Many researchers confirm that the entanglement mechanism is necessary
to realize high-level quantum computing (Jozsa & Linden, 2003;
Menneer, 1999), including quantum neural computing (QNC).

12

Y. Cui et al. / Neural Networks 71 (2015) 11–26

In existing QNN models, QNC is generally based on the real
assumption, i.e. the quantum probability amplitudes are all real.
Notwithstanding, the degree of quantum entanglement is limited if
QNC is only defined in the real number domains. An extension from
the real number domain to the complex number domain not only
complies with more of the real physical world, but also can lead to
a higher level of quantum entanglement by combined interaction
of the real parts and the imaginary parts. In this way, the data
information is encoded into an enhanced level of uncertainty, and
the neural networks have a higher flexibility in parameter learning.
To improve the approximation and generalization ability of
ANN by utilizing the mechanism of deep quantum entanglement
within complex number domain, in this paper, a novel hybrid
network model Complex Rotation Quantum Dynamic Neural Networks (CRQDNN) is proposed. The CRQDNN model is established
on the definition of a new quantum neuron model based on the
newly defined Quantum Complex Rotation Gate (QCRG). Since
there are two freedom degrees considering the combination of the
real and imaginary parts, a CQN has two output ports with each
representing an individual portion of the deep quantum entanglement.
In this paper, the structure of the CRQDNN model is proposed,
and the input/output mathematical relationships are derived. We
select the Levenberg–Marquardt (LM) networks learning algorithm to ensure a fast convergence and a high probability of finding the global minimum. Two application studies are done in this
paper, including the chaotic time series prediction and electronic
remaining useful life (RUL) prediction. The experimental results
show that CRQDNN is superior to traditional QNN model in the
metrics of prediction accuracy.
The remainder of this paper is organized as follows. Section 2
sorts out the related works in previous researches. Section 3 introduces the background knowledge of qubits and quantum gates.
Section 4 proposes the novel quantum neuron model CQR. Section 5 proposes the networks structure of CRQDNN with its input/output relationships. Section 6 conducts the application study
in chaotic time series prediction. Section 7 conducts the application
study in electronic RUL prediction. Finally, Section 8 concludes the
whole paper.
2. Related works
2.1. DNN
DNN with different time-scales can model the dynamics of the
short-term memory of neural activity levels (Lou & Cui, 2007;
Meyer-Bäse et al., 1996). Recurrent Neural Networks (RNN) are a
basic kind of DNN with feedback paths introducing dynamics into
the model. Recurrent Neural Networks (RNN) are a closed loop system, with feedback paths introducing dynamics into the model.
Unlike feed-forward neural networks, RNN can use their internal
memory to process arbitrary sequences of inputs (Mandic & Chambers, 2001). Typical RNN models include fully recurrent network,
Hopfield neural networks (HNN) (Hopfield, 1982), Elman neural networks (ENN) (Elman, 1990), Bi-directional RNN (Graves &
Schmidhuber, 2005; Schuster & Paliwal, 1997), etc. However, these
recurrent models merely utilize only the last one step of the feedback data. As a result, the memory capability does not always meet
the requirement to treat time series with time accumulation effect.
To better deal with the problem, new kinds of dynamic
neural networks are proposed that use the embedded filters as
the memory units. Ayoubi (1994) presents a class of dynamic
neuron model in which a filter is employed to generate the
dynamics between the input and output of the system. A dynamic
neuron model is constructed by adding an IIR filter within the
standard static neuron structure. Compared with the one-way

DNN, the DNN with recurrent properties have a higher stability
and robustness towards time series variances or noise. In this
way, Back and Tsoi (1991) propose the dynamic neuron model
with both IIR and finite impulse response (FIR) synapses, where
the embedded FIR filter serves as the multi-step recurrent model.
This kind of networks model maintains a short-term memory
based on the mechanism of internal local feedback provided by
the FIR filter. Yazdizadeh and Khorasani (2002); Frasconi, Gori, and
Soda (1992) present another two similar forms of dynamic neuron
model where the filter was placed after the activation function of
the neuron.
A common feature of such kinds of DNN is, the interconnected
dynamic neurons or the embedded filtering units contribute to
the memory function. The memory parameters are adjustable according to specific requirements, which facilitate sequence or time
series processing and prediction. However, a limitation of the existing DNN models is that they only process the time series data in
a linear manner. There are some other kinds of DNN with their own
dynamic designs, including Dynamic Wave Expansion Neural Networks (DWENN) (Lebedev, Steil, & Ritter, 2005), Dynamic Evolving
Spiking Neural Networks (DESNN) (Kasabov, Dhoble, Nuntalid, &
Indiveri, 2013), etc.
2.2. QNN
In recent years, quantum computing (QC) has attracted great
attention worldwide because of its novel computational characteristics and potential computational efficiency. The concept of
quantum computing is first stated based on the quantum mechanical nature of physical reality (Benioff, 1982; Feynman, 1982). Considerable interest has been generated in QC since Shor shows that
large numbers can be factored in polynomial time (Shor, 1997)
and Grover invents the searching algorithm that achieves quadratic
speedup over classical searching algorithms (Grover, 1997). The
quantum inspired computational intelligence (QCI) applications
are enhanced based on QC principles. QCI has been shown to offer
good performance in solving a wide range of problems and many
efficient strategies have been proposed as improvement (Manju &
Nigam, 2014). The most established research areas of QCI would
include quantum evolutionary algorithms (Han & Kim, 2002; KukHyun & Jong-Hwan, 2004; Platel, Schliebs, & Kasabov, 2009), quantum fuzzy systems (Rigatos & Tzafestas, 2002; Seising, 2006), and
QNN.
QNN is seen as the quantum analog of ANN, which is firstly
proposed by Kak (1995). Lagaris, Likas, and Fotiadis (1997) innovatively propose the artificial quantum neuron model based on
the quantum dynamics. Purushothaman and Karayiannis (1997)
propose the model of quantum neural networks with multilevel
hidden neurons based on the superposition of quantum states in
the quantum theory. Kouda, Matsui, and Nishimura (2002); Matsui, Nishimura, and Peper (2005) propose the single layer quantum perceptron model for networks formulation. Zak and Williams
(1998) realize the reconciliation of the linear reversible structure of quantum evolution with nonlinear irreversible dynamics
of neural networks. Liu, Chen, Chang, and Shih (2013) propose a
single-hidden-layer feed-forward quantum neural networks structure based on Grover quantum learning method.
The quantum neural networks can be modified by using different kinds of quantum gates or base state identifiers. To name
a few, Matsui, Takai, and Nishimura (2000) invent a quantum neural networks model using the single bit rotation gate and two-bit
controlled-not gate. Li, Li, Xiong, Chai, and Zhang (2014) propose the quantized neural networks model with the application of
short-term electrical load forecasting, in which the quantum CNOT
gate is embedded in the input layer quantum neurons. Further,
Shafee (2007) proposes a completely different kind of networks

Y. Cui et al. / Neural Networks 71 (2015) 11–26

from the mainstream works, where the neurons are states connected by quantum gates.
Quantum entanglement is involved in the quantum networks modeling, which is responsible for the associations between input and output patterns in the proposed architecture (Svitek, 2008). In quantum physics, quantum entanglement refers to the situation where the quantum states of two
or more objects have to be described with reference to each
other, even though the individual objects may be spatially separated. In information sciences, quantum entanglement refers
to the non-factorizable superposition of the basic quantum
states (Ezhov & Ventura, 2000). It encompasses the intermediate states with interactions of basic states, and provides more
flexibility than traditional computational manners in the training
phase of the neural networks. It is demonstrated that the QNN
models with quantum entanglement have at least no worse information processing capability than traditional networks (Kouda,
Matsui, Nishimura, & Peper, 2005; Menneer & Narayanan, 1995).
The neural networks models above are all static models and
they do not consider the time accumulation effect. To better model
the temporal features, the quantum spiking NN model and a
quantum computational neurogenetic model are discussed by N.
Kasabov (2010); N.K. Kasabov (2007) by utilizing spatial–temporal
information. Also, in an attempt to incorporate the time accumulation effect to model and predict time series behaviors, Li and Xiao
(2014), Li et al. (2013)) propose two novel quantum behaved neural networks: Controlled-Hadamard Quantum Neural Networks
(CHQNN) and Quantum-inspired Neural Networks with Sequence
Input (QNNSI), which use temporal sequences or time series as the
inputs.
In most of the existing QNN models, the quantum neural computing (QNC) is generally based on the real assumption, i.e. the
quantum probability amplitudes are all real. This is also true for
the existing quantum rotation gate only helps to shift the phase of
the real angles. However, as is well known in quantum mechanics or quantum computing, a qubit is a two-level quantum system,
described by a two-dimensional complex Hilbert space. The probability amplitudes of a qubit can necessarily be complex instead of
real, and the quantum rotation can also be worked on the imaginary part of the qubits (as long as it is a unitary operation).
We address the issue above mainly because of the consideration
that the real and imaginary parts can convey different information,
and in this way a deeper quantum entanglement can be realized. In
quantum neuron models, the quantum entanglement is only confined in the real number domain. Notwithstanding, a combined interaction of the real parts and the imaginary parts enables a deeper
quantum entanglement. The data information is asymmetrically
distributed to both the real parts and the imaginary parts, and the
complex quantum rotation operation enhances the intrinsic uncertainty (or fuzziness) level. In this way, the data information is thus
encoded into a higher degree of entanglement, and by combining
such intrinsic fuzziness, the neural networks have a higher flexibility in parameter learning.

Unlike the classical bit, which can only be set equal to 0 or 1, the
qubit resides in a vector space parameterized by the continuous
variables θ and φ . In Eq. (1), cos θ and eiφ sin θ are termed as the
probability amplitudes (PA) of basis states |0⟩ and |1⟩. It is clearly

2

seen that |cos θ |2 + eiφ sin θ  = cos2 θ + eiφ e−iφ sin θ = 1, which
satisfies the normalization condition called the probability axiom.
The Bloch sphere representation is useful to describe qubits,
which provides a geometric picture of the qubit and of the
transformations that one can operate on the state of a qubit. The
Bloch sphere is shown in Fig. 1(a). Owing to the normalization
condition, the qubit’s state can be represented by a point on a
sphere of unit radius, called the Bloch Sphere. This sphere can be
embedded in a three-dimensional space of Cartesian coordinates.
For convenience, many researches confine their scope within
the real domain. Their qubit states are represented by a point on a
circle of unit radius as shown in Fig. 1(b), where the amplitudes of
the basis states take real values. It can be written in the form as



|ζ ⟩ = cos γ |0⟩ + sin γ |1⟩.
(2)
In this setting, cos γ and sin γ are termed as the probability
amplitudes of the basis states |0⟩ and |1⟩, respectively. According to
quantum theory, the probability measure of state |0⟩ equals cos2 γ ,
while the probability measure of state |1⟩ equals sin2 γ ; we have
cos2 γ + sin2 γ = 1, which satisfies the probability axiom.
3.2. Quantum Rotation Gate (QRG)
In quantum theory, the transformation of states is performed
by the quantum transformation matrix. Such matrix is called a
quantum gate, which forms the basic unit of quantum algorithms.
The difference between the classical and quantum context is that a
quantum gate has to be implemented reversibly and, in particular,
must be a unitary operation. The definition QRG is given by
cos θ
R(θ ) =
sin θ




− sin θ
.
cos θ

(3)

The QRG R(θ ) shifts the phase of a qubit by θ . For the initial
quantum state |ζ ⟩ = [cos θ0 , sin θ0 ]T , the function of the quantum
rotation gate is given by
R(θ )|ζ ⟩ =



cos θ
sin θ

− sin θ
cos θ

cos θ0
sin θ0






=

cos (θ0 + θ)
.
sin (θ0 + θ )



(4)

3.3. Quantum Complex Rotation Gate (QCRG)
Since the quantum probability amplitude can necessarily be
complex, here we propose a new quantum gate QCRG. Compared
with QRG, the QCRG operation can exert an effect on the imaginary
part of a qubit. The definition of QCRG is given by
eiφ cos θ
CR(θ , φ) = iφ
e sin θ




−eiφ sin θ
= eφ R(θ ).
eiφ cos θ

(5)

QCRG is a unitary operation, which is given by
CR(θ , φ) · CR(θ , φ)∗

3. Qubits and quantum gates



eiφ cos θ
eiφ sin θ



1
0

=
In this section, we introduce the basic knowledge of QC that
lays the foundation of the subsequent discussions, including the
definition of the qubits and the quantum gates.

=

−eiφ sin θ
eiφ cos θ

e−iφ cos θ
−e−iφ sin θ



e−iφ sin θ
e−iφ cos θ





0
= I2×2 .
1

Functioning QCRG on the initial quantum state |ϕ⟩ = cos θ0 |0⟩ +
eiφ0 sin θ0 |1⟩, we have

3.1. Qubits
In quantum computing, a qubit is a two-level quantum system,
described by a two-dimensional complex Hilbert space. From the
superposition principles, any state of the qubit may be written as

|ϕ⟩ = cos θ|0⟩ + eiφ sin θ|1⟩.

13

(1)



cos θ0
−eiφ sin θ
eiφ0 sin θ0
eiφ cos θ
 iφ

e cos (θ0 + θ)
= i(φ0 +φ)
.
e
sin (θ0 + θ )

CR(θ , φ)|ϕ⟩ =



eiφ cos θ
eiφ sin θ

(6)

14

Y. Cui et al. / Neural Networks 71 (2015) 11–26

Fig. 1. Graphical qubit description. (a) Bloch sphere. (b) Qubit on unit circle.

With CR(θ , φ), both the exponential part and the sinusoidal parts
have the phase shifts by φ and θ , respectively.
4. Quantum neuron model
The quantum neuron model would be the most important part
of the quantum neural networks model. Kouda et al. (2002) and
Matsui et al. (2005) propose the single layer quantum perceptron
model for the networks formulation, which is widely used as
the quantum neuron model in the literature. The basic form of
quantum neuron model proposed by Kouda et al. is shown in
Fig. 2. This quantum neuron model only processes the real input
quantized data |xh ⟩ (h = 1, . . . , n), and utilizes the real quantum
rotation gate as mentioned in Section 3.
Kouda’s quantum neuron model only processes the real input
quantized data |xh ⟩ (h = 1, . . . , n), and utilizes the real quantum
rotation gate as mentioned in Section 3. According to Fig. 2, the
quantum neuron model involves the operations of phase shift,
summation, reversal. The phase shift operation is done by using
the quantum rotation gate, which is given by
R(θh ) =



cos θh
sin θh

− sin θh
.
cos θh


phase shift operation. Since a complex qubit has two 
freedom den
gree, in CQN, the treatment of the summated qubit
h=1 CR(θh ,
φh )|xh ⟩ is diverged in two branches. The first
none treats with the
real portion, which is given by α = arg(Re( h=1 CR(θh , φh )|xh ⟩));
the second one treats
n with the imaginary portion, which is given
by ρ = arg(Im( h=1 CR(θh , φh )|xh ⟩)). The final outputs are the
result of the reversal operations acted on both the real branch and
the imaginary branch.
The output of the quantum neuron is defined by the probability
amplitude corresponding to the quantum basis |1⟩. We have


u1 = sin

π
2


f (β) − arg Re



n



CR(θh , φh )|xh ⟩

(9)

h =1

and


u2 = sin

π
2


f (σ ) − arg Im


n



CR(θh , φh )|xh ⟩

.

(10)

h =1

5. Networks structure of CRQDNN
In this section we propose the CRQDNN structure combining
both CQNs and classical neurons. The mathematical relationships
in the CRQDNN models are also exploited.

The summation operation is done by
h=
1 R(θh )|xh ⟩. Let α be

n
the angle of the summation, i.e. α = arg
h=1 R(θh )|xh ⟩ . The
quantum reversal operation is done by the quantized CNOT gate
U (α, β). The operational logic of U (α, β) is given by

n



π



cos
f (β) − α

2
U (αi , β) = 
π

sin
f (β) − α
2

π



− sin
f (β) − α

2
π
 
cos
f (β) − α

(7)

2

where f (·) is the activation function. The output of the quantum
neuron is defined by the probability amplitude corresponding to
the quantum basis |1⟩, i.e.


u = sin

π
2

f (β) − arg


n



R(θh )|xh ⟩

.

(8)

h =1

As is discussed in Section 2, a deeper entanglement can be realized when the data are artificially quantized into a complex form
than into a real form. Considering this, we propose a novel quantum neuron model CQN based on Kouda’s neuron model, which is
shown in Fig. 3.
In CQN, all the quantized inputs are complex qubits. We use
QCRG instead of traditional quantum rotation gate to realize the

5.1. CRQDNN structure
The CRQDNN model is a 3 layer neural networks structure, as is
shown in Fig. 4. From the input to the output, the layers are termed
as the input layer (also termed as the quantum layer), the hidden
layer, and the output layer, etc. The input layer consists of quantum
neurons, while the hidden and output layers consist of common
neurons.
For CRQDNN, the inputs are quantized sequences. Let n1 , n2 ,
and n3 be the number of outputs concerning the quantum, hidden,
and output layers, respectively; in this way, the number of CQNs is
n0 = n1 /2. In Fig. 4, w(output ) ∈ Rn3 ×(n2 +1) denotes the connection
weights in the output layer, and w(hidden) ∈ Rn2 ×(n1 +1) denotes
the connection weights in the hidden layer. f (·) is the
 activation

function. In our study, the sigmoid function f (x) = 1/ 1 + e−x is
used as the activation function.
We embed an IIR filter to fulfill the sequential computation
tasks, which endow the networks with the memory function. The
IIR filter possesses a feedforward status that is attached to the
output ports of the quantum neurons. The internal structures of
the IIR filter structure is depicted in Fig. 4, where N and M are
the lengths of the filters, aT (T = 1, . . . , N + 1) is the IIR filter
parameters (see Fig. 5).

Y. Cui et al. / Neural Networks 71 (2015) 11–26

15

Fig. 2. Basic quantum neuron model.

Fig. 3. CQN model.

Fig. 4. CRQDNN structure.



|x1 [t ]⟩


×  ... 
|xn [t ]⟩

= |q1 [t ]⟩ · · ·

5.2. Mathematical relationships
For the input layer, the laws of quantum physics give a description of the interactions between the qubit neurons and the classic
neurons. The mathematical quantum descriptions of the real input
vector x[t ] ∈ Rn×1 is given in the quantized form as
2π


|xh [t ]⟩ = cos



|0⟩
1 + exp(−xh [t ])

+ exp (i arctan (xh [t ])) sin

2π
1 + exp(−xh [t ])

(11)

where [t ] represents the present time point, ϕh and ψh are the
quantum phases of the hth element of the quantized input. When
the quantum inputs are presented to quantized neural model, the
quantum phase shift operation successively functions in qubit neurons. The quantum phase shift operation is given by

CR(θ, f)|x[t ]⟩ = 



CR(θ1,1 , φ1,1 )

..
.

CR(θn0 ,1 , φn0 ,1 )

···
..
.
···

CR(θ1,n , φ1,n )

..
.

(12)



cos ϕh [t ] + θj,h

h =1

exp (iψh [t ]) · sin ϕh [t ] + θj,h









(13)

h =1

= cos (ϕh [t ]) |0⟩ + exp (iψh [t ]) · sin (ϕh [t ]) |1⟩



n




|qj [t ]⟩ = 
n


|1⟩

T

with




|qn0 [t ]⟩





CR(θn0 ,n , φn0 ,n )

where θ ∈ Rn0 ×n and f ∈ Rn0 ×n represent the quantum rotation parameter matrices, and CR(θ, f) represents QCN, |qj [t ]⟩(j =
1, . . . , n0 ) represents the quantum activation potential at time
point t. In Fig. 4, αj [t ] and ρj [t ] represent the real and imaginary
angles of |qj [t ]⟩(j = 1, . . . , n0 ). From Eqs. (12) and (13), we have
 
n
 


 
exp i ψh [t ] + φj,h · sin ϕh [t ] + θj,h

  h=1

 .
αj [t ] = arctan 
n

Re 





exp iφj,h · cos ϕi [t ] + θj,h
h=1

(14)

16

Y. Cui et al. / Neural Networks 71 (2015) 11–26

Fig. 5. Internal structures of the embedded IIR filter.

(2)

And


n



  h=1

ρj [t ] = arctan 
Im 

exp i ψh [t ] + φj,h

 

n





· sin ϕh [t ] + θj,h

exp iφj,h · cos ϕi [t ] + θj,h







 

 .




h=1

(IIR)

Let vk denote the kth output of the hidden layer. Let un1 +1 [t ] ≡
1, which serves as an external bias term of the hidden layer. The
hidden layer outputs are thus calculated by
(2)

v k [t ] = f

n +1
1



U (α[t ], β) = 

sin

π
 π2
2



,

k = 1 , . . . , n2 .

(20)

T

(16)
(3)

y l [t ] =

n 2 +1



) (2)
wl(,output
vk [t ],
k

l = 1, . . . , n3 .

(21)

k =1



π

U (ρ[t ], σ ) = 

 π2

cos


sin

2

f (σ ) − ρ[t ]
f (σ ) − ρ[t ]


(17)




where β ∈ R
represents the quantum reversal parameter vector corresponding to the real parts, and σ ∈ Rn0 ×1 represents the
quantum reversal parameter vector corresponding to the imaginary parts. In this study, we use the probability of the base state
|1⟩ as the output, instead of the probability amplitude. Thus, the
jth output of the quantum layer is given by
n0 ×1


π


sin2
f (βj ) − αj [t ] ,
(1)
2
π

uj [ t ] =

sin2
f (σj ) − ρj [t ] ,
2

j = 1, 3, . . . , n1 − 1
(18)

j = 2, 4, . . . , n1 .

(IIR)

(1)

Let uj
denote the output of the IIR filter connected to uj ; thus,
it is calculated by
(IIR)

[t ]

y1 [t ], . . . , yn3 [t ] . The output values are thus calculated by






and

uj

uj




(2)

f (β) − α[t ]
f (β) − α[t ]

(IIR)

Let vn2 +1 [t ] ≡ 1, which serves as an external bias term of
the output layer. The output layer vector is defined by y[t ] =

The quantum reversal operation is given by
cos



j =1

(15)



)
f
wk(hidden
,j

[t ] =

N +1

T =1

(1)

aT uj [t − T + 1],

j = 1, . . . , n1 .

(19)

5.3. Networks learning
In CRQDNN, the adjustable parameters include the rotation angles of the quantum rotation parameters θ and f, the quantum reversal vector β and σ , the IIR filter parameter vector a, the weights
in the hidden layer w(hidden) , and the weights in the output layer
w(output ) . To guarantee a fast convergence speed, we employ the
Levenberg–Marquardt (LM) algorithm (Marquardt, 1963) to adjust
the CRQDNN parameters. LM algorithm is a nonlinear optimization approach that lies between the Gauss–Newton method and
the gradient descent method, for which the performance index is
the mean squared error. It greatly reduces the chances that the
evaluation function is trapped in local minima. Also, it is insensitive to overfitting, and it is effective to tackle with the problem of
redundant parameters. Using LM algorithm, the networks would
converge very fast in a high probability with the evaluation approximates the global minimum.
The fulfillment of LM algorithm depends on the gradient calculations based on the CRQDNN structures. From the output layer to
the quantum layer, we obtain the gradient calculations in a back
propagation scheme, which is discussed in detail in the Appendix.

Y. Cui et al. / Neural Networks 71 (2015) 11–26

17

6.2. Experimental configurations
For CRDQNN, the input time series is scaled in the range of

[−1, 1] to make sure that the initial phases of the quantum neurons
are confined within the interval of [0, 2π ]. Since we do not require

Fig. 6. Histogram of monthly mean total Sunspot number from Nov 1834 to Jun
2001.

6. Sunspot time series prediction application
In this section, we present the first application study towards
chaotic time series prediction using CRDQNN. The famous Sunspot
time series (SIDC, 2015) as the real-world experimental data is
used as the experimental data.
6.1. Dataset description and preliminary statistical analysis
The Sunspot time series is an excellent indication of the solar
activities for solar cycles, which impact the climate and weather
patterns of the earth, as well as the satellite and space missions. It is
well known that the approximate 11 year solar cycle actually consists of a 22 year magnetic cycle that flips polarity every 11 years.
The prediction of solar cycles is difficult due to its complexity.
The yearly, monthly, as well as daily Sunspot time series can
be obtained from the World Data Center for the Sunspot Index
(SIDC). The Sunspot time series can be seen as chaotic systems
with noise, which have the characteristics of aperiodic, bounded,
deterministic, and sensitive to initial conditions (Abarbanel, 1996).
Many prediction methods have been exploited to the related time
series, and in most cases the frequently used predictors are ANN
models (Marra & Morabito, 2006).
The truncated sub-series from November 1834 to June 2001 is
selected that consists of 2,000 months. This interval is also selected
to compare the performance of the proposed methods from the
researches, e.g. (Ardalani-Farsa & Zolfaghari, 2010; Gholipour,
Araabi, & Lucas, 2006). With respect to the data of the monthly
mean total Sunspot number within this period, its mean and
standard deviation values are calculated as 90.1824 and 70.5796,
respectively. Fig. 6 provides the histogram of the monthly mean
total Sunspot number, which shows a clear left-skewed pattern.
The histogram can be represented using a two-parameter Gamma
distribution given by
f (x) =

xα−1


exp −

x


(22)

β α Γ (α)
β
where α > 0 is the shape parameter, β > 0 is the scale parameter,
and Γ (·) represents the Gamma function. The shape of the Gamma

distribution corresponding to the Sunspot data is also shown in
Fig. 6. From a statistical analysis, the shape parameter and scale
parameter are estimated as αˆ = 1.6326 and βˆ = 55.2378.
In this experiment, the Sunspot time series of 13-month
smoothed monthly total Sunspot number is chosen as the data
under investigation. The first 1000 samples are used for training,
while the remaining 1000 samples are used for testing and validation.

the output time series to be normalized, we do not use any
activation function for the output layer.
In our study, we use the Levenberg–Marquardt algorithm to
facilitate a fast training of the CRDQNN model. The initial velocity
parameter is selected as µ0 = 0.001, while the multiplying
parameter is selected as v = 10. The maximum iteration number
is 50, while the termination condition is that the weight position
increment is lower than δ = 1e−5.
The performance under different networks structures or
configurations are different. In our study, we conduct 100
independent experiments for each networks configuration, and in
each experiment the initial parameters of the networks models are
randomly chosen. The performance results are shown in the sense
of average values, best values, and standard deviations. Further,
the CRDQNN performance is also compared with (1) Kouda’s Qubit
Neural Networks (Kouda et al., 2002) with IIR filter (QNN-IIR) and
(2) traditional Elman networks.
6.3. Experimental results
To evaluate the prediction performance, the root mean squared
error (RMSE) and normalized mean squared error (NMSE) are used
to measure the prediction performance of the recurrent neural
networks. These are given in the following equations:



tmax
 1 

T 

y˜ [t ] − y [t ]
y˜ [t ] − y [t ]
RMSE = 
tmax t =1

tmax



y˜ [t ] − y [t ]

NMSE =

T 

y˜ [t ] − y [t ]

T 

y˜ [t ] − y¯

t =1

(23)


(24)

tmax



y˜ [t ] − y¯



t =1

where y¯ is the average value of the training data during the whole
time span.
At the beginning, the number of quantum layer and the hidden
layer is empirically evaluated. At this time, we fixed the length of
IIR filter as N = 5, and the length of FIR filter as M = 5.
The experimental results calculated from 100 independent experiment runs are listed in Table 1 for the Sunspot datasets, including mean RMSE, best RMSE, mean NMSE, as well as the standard
variation values.
In Table 1, the best mean RMSE results are highlighted in bold.
As is seen in Table 1, the experimental results using CRQDNN are
quite outstanding among all the 3 methods employed in this study.
We can see that for each group divided by the hidden neuron
number, the mean RMSE values for CRQDNN are largely about or
lower than 2.0E–2, and almost all of the best RMSE values are
lower than 8.0E–3. The experimental values under QNN-IIR are
already satisfactory to our expectation, with the mean RMSE values
are lower than 3.0E–2, and the best RMSE values are lower than
1.0E–2. However, the results under QNN-IIR are still inferior to the
results under CRQDNN in most of the cases. Also, the RMSE results
under CRQDNN are almost about 1/5 ∼1/8 of the values using the
traditional Elman recurrent networks.
The similar conclusions can also be drawn considering the
NMSE results. As is shown in Table 1, the NMSE values under
CRQDNN are generally 5 ∼10% lower than those under QNN-IIR
for almost every line. Together with the RMSE results, it is demonstrated that the CRQDNN model has a strong capability of time series prediction with sequential inputs.

16

12

8

4

Hidden

4
8
12
16
4
8
12
16
4
8
12
16
4
8
12
16

Quantum

RMSE best

7.87E−03
5.99E−03
5.66E−03
5.32E−03
7.01E−03
6.55E−03
5.65E−03
4.76E−03
7.01E−03
5.18E−03
5.22E−03
4.89E−03
6.23E−03
4.98E−03
4.64E−03
3.94E−03

RMSE mean

2.14E−02
1.72E−02
1.70E−02
1.65E−02
1.84E−02
1.54E−02
1.34E−02
1.31E−02
1.56E−02
1.47E−02
1.16E−02
1.11E−02
1.56E−02
1.44E−02
1.21E−02
1.14E−02

CCRQDNN

3.07E−03
1.53E−03
1.74E−03
1.85E−03
1.60E−03
8.38E−04
9.20E−04
9.73E−04
1.44E−03
1.23E−03
7.05E−04
8.40E−04
1.11E−03
9.36E−04
9.21E−04
8.67E−04

NMSE mean
7.61E−03
7.19E−03
4.82E−03
5.59E−03
4.73E−03
3.87E−03
5.45E−03
4.46E−03
3.26E−03
5.61E−03
3.60E−03
3.30E−03
4.05E−03
3.96E−03
2.63E−03
4.84E−03

Std

Table 1
Prediction performance of CRQDNN, QNN-IIR, Elman for the Sunspot time series.

2.33E−02
1.98E−02
1.66E−02
1.55E−02
2.13E−02
1.80E−02
1.87E−02
1.29E−02
1.93E−02
1.60E−02
1.43E−02
1.38E−02
1.88E−02
1.55E−02
1.43E−02
1.29E−02

RMSE mean

QNN-IIR

8.87E−03
6.91E−03
5.75E−03
5.05E−03
6.43E−03
7.01E−03
5.69E−03
5.32E−03
7.22E−03
6.76E−03
5.45E−03
4.77E−03
4.90E−03
5.75E−03
5.03E−03
4.68E−03

RMSE best
3.85E−03
2.24E−03
1.96E−03
1.71E−03
2.04E−03
1.38E−03
1.05E−03
9.88E−04
1.54E−03
1.42E−03
8.97E−04
8.56E−04
1.74E−03
1.25E−03
9.70E−04
8.00E−04

NMSE mean
9.74E−03
6.99E−03
6.69E−03
6.75E−03
8.32E−03
6.48E−03
5.07E−03
4.69E−03
7.68E−03
5.36E−03
4.66E−03
4.71E−03
6.92E−03
3.86E−03
4.44E−03
4.94E−03

Std

Elman

4.84E−02

3.80E−02

2.55E−02

1.86E−02

7.31E−02

6.30E−02

5.67E−02

RMSE best

9.61E−02

RMSE mean

5.38E−03

5.23E−03

6.76E−03

9.40E−03

NMSE mean

Std

1.61E−02

1.65E−02

1.78E−02

1.98E−02

18
Y. Cui et al. / Neural Networks 71 (2015) 11–26

Y. Cui et al. / Neural Networks 71 (2015) 11–26

(a) Performance on the training data.

19

Fig. 8. RMSE performances comparison under different temporal structures.

6.4. Results under different temporal structures

(b) Performance on the test data.

Aside from the number of quantum layer or hidden layer
neurons, the temporal structures in the CRQDNN recurrent
networks model are also very important for the networks
performances. The total delay timers of the IIR filter, also exert
an effect on the time series prediction performances. In this part,
we investigate the effect under different temporal structures. We
obtain the RMSE values of the Sunspot time series prediction
results corresponding to varying N, which are shown in Fig. 8. The
mean and best RMSE curves all have a decreasing tendency when
N increases, meaning that a high value of N would generally bring
down the prediction errors. When N is small (e.g. N = 1), the RMSE
value is very high, and the prediction accuracy is correspondingly
low. At the same time, when N exceeds 5, the decrease of RMSE
values becomes quite insignificant. Since a higher N means a more
complex networks structure and higher computational efforts,
N = 5 might be the best choice to facilitate an accurate prediction.

7. Electronic prognostics application

(c) Error on the test data.
Fig. 7. Typical prediction by CRQDNN for Sunspot time series.

For CRQDNN, the more the number of the quantum neurons,
the better the prediction performance. This is consistent with our
heuristic knowledge about quantum neural networks model that
the more the number of quantum neurons, the higher the level of
quantum entanglement. For CRQDNN, it utilizes not only the real
portion quantum entanglement, but also the corresponding imaginary portion. The entanglement is more complicated than QNNIIR, which provides more redundancy and flexibility in networks
parameter learning. It is demonstrated that CRQDNN takes more
of the advantage of quantum entanglement in the training process
compared with QNN-IIR.
Fig. 7 shows that the CRQDNN model is trustworthy to give
good prediction performance with low prediction error. It is able
to tackle the noise in the Sunspot time series.
We also make a more detailed comparison of the chaotic time
series prediction results with other methods published in the
literature, see Table 2. From the perspective of NMSE, with respect
to the Sunspot dataset, the prediction results of the proposed
CRQDNN model are generally better than (or can be compared to)
other methods apart from Hybrid NARX-Elman RNN with residual
analysis (Ardalani-Farsa & Zolfaghari, 2010).

Addressing on the requirement of early fault prediction with
high accuracy in EPHM, we focus on the issue of electronic
prognostics, and show that the CRQDNN model can fulfill the RUL
estimation task. In this section, we will show that CRQDNN has the
ability to track the major deterioration trends of the developing
fault, and it is also robust towards noise compared with methods
based on traditional networks models.
7.1. Fundamentals of prognostics and RUL estimation
Electronic Prognostics and Health Management (Electronic
PHM) is gaining more and more attention since it is an important
and integral part of reliable and safe performances of electronic
systems. It is an enabling discipline of technologies and methods
with the potential of solving reliability and testability problems
due to complexities in design, manufacturing, environmental and
operational use conditions, and maintenance (Gu, Lau, & Pecht,
2009; Pecht & Jaai, 2010). The prevention of electronic faults during
operation requires the prediction of the remaining useful life (RUL)
based on deterioration or declination monitoring, especially for the
critical circuit units or catastrophic faults (Sarathi Vasan, Long, &
Pecht, 2013).
The RUL prediction relies on the current status identification of
the monitoring electronic product. As is shown in Fig. 9, a product gradually turns from the healthy operational state to the faulty
state, and the identified level of status deterioration (or fault development) serves as the prerequisite to make RUL prediction. The
deterioration paths do not always follow the same pattern, and the
effect of noise or inaccurate measurement even worsens the situation. The task in this section is to abstract the useful information
hidden in the data and facilitate a reliable RUL prediction.

20

Y. Cui et al. / Neural Networks 71 (2015) 11–26
Table 2
A comparison with the results from the literature on the Sunspot time series.
Prediction method
McNish–Lincoln (Sello, 2001)
Multi-layer perceptron/Elman RNN (Koskela et al., 1996)
Wavelet packet multilayer perceptron (Teo et al., 2001)
Radial basis networks with orthogonal least squares (RBF-OLS) (Gholipour et al., 2006)
Locally linear model tree (LLNF-LoLiMot) (Gholipour et al., 2006)
Hybrid NARX-Elman RNN with RA (Ardalani-Farsa & Zolfaghari, 2010)
Synapse Level Cooperative Coevolution RNN (SL-CCRNN) (Chandra & Zhang, 2012)
Neuron Level Cooperative Coevolution RNN (NL-CCRNN) (Chandra & Zhang, 2012)
Proposed CRQDNN

Fig. 9. Illustration of status deterioration.

The data-driven strategy is universally used in prognostics and
RUL prediction since it can detect and utilize the useful information
‘‘hidden’’ in the monitoring or training data (Kabir, Bailey, Lu, &
Stoyanov, 2012; Pecht & Jaai, 2010). The information of the past
history to infer the future by updating the fault time prediction and
provide RUL estimations. For data-driven methods, the prognostic
effectiveness of the electronic products is highly dependent on the
prediction accuracy of the prognostic model or method.
7.2. Experimental setup
The electronic circuit under test is the secondary power conversion board in the XD-3539A airborne radio frequency analyzing instrument. The power conversion board is expected to convert the input DC voltage to the standard +12 V DC voltage output
which can be utilized by the downstream modules. A low dropout
(LDO) regulator chip realizes the power conversion function. There

RMSE

NMSE

1.19E−02
1.66E−02
2.60E−02
1.16E−02

8.00E−02
9.79E−02
1.25E−01
4.60E−02
3.20E−02
5.90E−04
1.47E−03
3.62E−03
7.05E−04

are also some small-scaled peripheral circuits responsible for the
power control.
The power conversion board is very important for the whole
instrument, therefore it is required to learn the state declination
pattern for the purpose of fault identification as well as prognostics. Accelerated aging methodologies are integral to the induction
of degradation and component faults into the electronic products.
In an accelerated aging scheme, we conduct the disruptive experiment for a total of 4 identical power conversion board, and exert continuous thermal cycling on them on a high temperature
level. We choose thermal cycling with vibration as the aging condition because thermal cycling is strongly associated with die solder
degradation and wire lift, and vibration is closely associated with
the wire or pin fractures. The experimental system schematic of
the thermal cycling with vibration is illustrated in Fig. 10.
Accelerated aging methodologies are integral to the induction
of degradation and component faults into the electronic products.
In an accelerated aging scheme, we conduct the disruptive
experiment for a total of 4 identical power conversion board at the
same time, and exert continuous thermal cycling on them on a high
temperature level. We choose thermal cycling because it is strongly
associated with die solder degradation and wire lift.
In this study, the converted voltage output serves as the
prognostic indicator revealing the declination level of the circuit
board, which can be used by prognostics algorithms to estimate
the RUL. The HL4005T temperature chamber is used to provide the
high and low temperature environment. The voltage outputs for
the circuits are induced and collected by the voltage sensors placed
outside the chamber. The data are sampled and sent through a data
acquisition device to a computer running a LabView interface. The
default sampling time is 10 s.
There is a relatively long period of perfect operation during
the accelerated aging experiment, after which the fault declination
begins to be observed. In this study, we only take the last thousands
of samplings representing the declination stage to make the
following analyses.

Fig. 10. Overview of the accelerated aging experimental system.

Y. Cui et al. / Neural Networks 71 (2015) 11–26

Fig. 11. Monitoring output voltage values of the training data.

7.3. Data analysis and training process
We obtain 4 datasets of output voltage corresponding to the 4
power conversion circuit boards in the period of accelerated aging
experiment. The data are all bounded within the interval of [−1, 1].
All the 4 datasets reveal the deterioration process from the normal
operational state to the faulty state. The deterioration process is
not following a monotonous pattern; instead, it involves significant
fluctuations and repetitions. Additionally, there is heavy sensor
noise on the monitoring signals. Of course, we do not know exactly
what internal changes lead to such fault deterioration pattern, and
the data-driven prognostic method would be the reasonable choice
to solve the problem.
For the 4 experimental power conversion boards, the degrees
of fluctuation and repetition vary from each other, as well as the
actual time to fault (TTF) values. The observed TTF values for them
are 2059, 2032, 2294, 2107 time units, respectively. We select
Dataset 1 as the training data, and the rest datasets are used for
model testing. Fig. 11 depicts the output voltage values over time.
From Fig. 11 we can see that the output voltage signals are not
continuously decreasing; instead, there are several ‘‘horizontal’’
phases with the voltage values maintaining on the same level,
which are marked with the dash lines in Fig. 11.
We use the CRQDNN model to make the RUL prediction with
the voltage value bounded in [−1, 1]. The bounded values are also
quantized to fulfill the requirement of the quantum neurons. We
take the training data (i.e. Dataset 1) as an example to make the

(a) Amplitude of basis state |0⟩.

21

Fig. 13. Training curves for CRQDNN and Elman networks.

quantization. The real and imaginary parts of the quantum amplitude values in the basis state of |0⟩ and |1⟩ are calculated, which is
shown in Fig. 12. Fig. 12 depicts the deep entanglement with the
complex quantum rotation gate. The qubits after quantization using complex quantum rotation gate convey more information than
the qubits using traditional quantum rotation gate.
Note that we utilize the filtering function of CRQDNN, and do
not denoise the data when using CRQDNN to predict RUL. Based
on the LM networks learning algorithm, we obtain the trained
CRQDNN model. The number of quantum and hidden neurons are
both 10. Besides, the lengths of IIR and FIR filters are also chosen to
be 10.
In order to make a comparison, we also use the QNN-IIR method
and the traditional Elman neural networks model in the RUL
prediction. In this setting, since the traditional Elman recurrent
neural networks lack the capability to deal with data with heavy
noise, we use the Daubechies wavelet (DB8) to denoise the training
voltage signal with 8 levels of the decomposition components. We
also use the Levenberg–Marquardt algorithm to train the Elman
recurrent networks. The number of hidden neurons is 10.
Fig. 13 shows the training error curves of the methods. We use
the RMSE as the training error metric, and the error goal is set to
be 0.05. From Fig. 13, for CRQDNN training, only 21 iterations are
required to make sure that the RMSE value turns below the error
goal line. If we use QNN-IIR, 25 iterations are required to satisfy the
convergent condition. By contrast, the Elman recurrent networks
have not met the requirement in the first 50 iterations. Also, the

(b) Real amplitude of base state |1⟩.

(c) Imaginary amplitude of base |1⟩.
Fig. 12. Quantized amplification portions of basis states for the training data.

22

Y. Cui et al. / Neural Networks 71 (2015) 11–26

Fig. 14. Comparison with predicted and observed RUL. (a) Dataset 1 (training data), (b) Dataset 2, (c) Dataset 3, (d) Dataset 4.

final error value for Elman networks is 0.0834, while the value
for CRQDNN is 0.0495. It manifests that the CRQDNN model has
a comparatively more robust convergence capability.

Dataset 1
Dataset 2
Dataset 3
Dataset 4

7.4. Experimental results
We conduct 100 independent experiments for each networks
configuration, and in each experiment, the initial parameters of
the networks models are randomly chosen. We use the trained
CRQDNN model to function on the training data as well as the 3
test datasets. The typical predicted system deterioration curves are
shown in Fig. 14. The prediction curves track the real observed
deterioration process well on a large scale, and the lifetime
estimation results are quite reliable. This is especially true for the
quasi linear declining stage (about 1050 ∼ 1600 time points)
where the fitness level is the highest during the whole period. By
contrast, the prediction deviation degree is high when entering or
leaving the horizontal phases.
We select RMSE, NMSE, as well as Mean absolute percentage
error (MAPE) as the characterizing metrics to make a quantitative
evaluation of the prediction accuracy. The mathematical forms of
RMSE and NMSE have been presented in Section 6, and MAPE is
given by the following equation:
MAPE =

tmax
 y˜ [t ] − y [t ] 
1 

tmax t =1





y [t ]

Table 3
Prediction error under CRQDNN for the datasets.


 .

2

(25)

MAPE

RMSE

NMSE

0.1558
0.2291
0.2308
0.1877

0.0655
0.0935
0.0903
0.0652

0.0455
0.0633
0.0758
0.0320

The mean MAPE, RMSE, and NMSE values of the 100 experimental runs are calculated and listed in Table 3. It is seen that with respect to the predicted RUL results (bounded to [0, 1]), the MAPE,
RMSE, and NMSE values are all lower than 0.3, 0.1, and 0.1, respectively. Considering the fact that there are more than 2000 data
points, the prediction effectiveness and accuracy are quite optimistic. The prediction accuracy for Dataset 3 is the lowest. This is
because that the TTF value for Dataset 3 is about 2300 time units.
Its changing pattern is also the most distinct one from the training
data. By contrast, Dataset 4 is the best one among the three test
datasets considering the prediction accuracy metrics.
Next, we make a comparison with other methods. We use both
the original and denoised training data and test data as the input
for CRQDNN, QNN-IIR, and traditional Elman recurrent networks.
The final results are listed in Table 4.
Heuristically, both CRQDNN and QNN-IIR have the ability to
process data with noise, as a result of the dynamic properties
provided by its embedded IIR filters. With respect to CRQDNN,
the differences of the prediction errors using the original data
and the denoised data are not significant, which demonstrates
that CRQDNN has a comparatively strong ability to ‘‘learn from

Y. Cui et al. / Neural Networks 71 (2015) 11–26
Table 4
Comparison of RUL prediction results using different methods.
Dataset

Method

MAPE

RMSE

NMSE

Dataset 3

CRQDNN with original data
QNN-IIR with original data
CRQDNN with denoised data
QNN-IIR with denoised data
Elman with denoised data

0.2308
0.3182
0.2192
0.1961
0.6161

0.0903
0.1044
0.0754
0.0632
0.1882

0.0758
0.0887
0.0631
0.0544
0.1448

CRQDNN with original data
QNN-IIR with original data
CRQDNN with denoised data
QNN-IIR with denoised data
Elman with denoised data

0.1877
0.2776
0.2465
0.2541
0.4891

0.0652
0.0871
0.0910
0.1109
0.1450

0.0320
0.0408
0.0621
0.0530
0.0916

Dataset 4

23

of China. This work is supported by Science and Technology on Reliability and Environmental Engineering Laboratory, Beihang University. This work is also sponsored by fund projects 61316705,
51319040301 and Z132014B002.
Appendix. Gradient calculations of CRQDNN
For the output layer of CRQDNN, the increment of w (output ) is
calculated by
(output )

∆wl,k

=

∂ e [t ]

∂ y(l 3) [t ]

)
∂ y(l 3) [t ] ∂wk(output
,l

= y(k2) [t ]

(26)

(l = 1, . . . , n3 , k = 1, . . . , n2 + 1) .
noise’’. It can also be seen in Table 4 that the prediction accuracy
under CRQDNN is generally higher than QNN-IIR if we use the
denoised data. This can be explained by the fact that CRQDNN takes
advantage of a deeper quantum entanglement compared with
QNN-IIR, and correspondingly the former can bring the filtering
function into better play. It is interesting that the prediction results
under CRQDNN are a little inferior to the results under QNN-IIR for
the group of denoised data.
It is also seen in Table 4 that the prediction results using
CRQDNN (with original data) are better than the Elman recurrent
networks (with denoised data). The MAPE, RMSE, and NMSE values
for CRQDNN are about 1/2 ∼ 2/3 less than the Elman networks.
8. Conclusions
To enhance the approximation and generalization ability of
ANN by utilizing the mechanism of deep quantum entanglement
within complex number domain, a novel hybrid networks model
Complex Rotation Quantum Dynamic Neural Networks (CRQDNN)
is proposed in this paper. CRQDNN is a three layer model with
a quantum layer which employs the Quantum Complex Rotation
Gate (QCRG) and quantum CNOT gate, and a hidden layer which
employs the IIR filter as the memory units. Compared with previous real QNN model, the proposed complex QNN model enhances
the level of quantum entanglement, i.e. the intrinsic uncertainty
level. The data information is thus encoded into a higher degree of
entanglement, and by combining such intrinsic fuzziness, the neural networks have a higher flexibility in parameter learning. Also,
to handle time series input, an IIR filter is embedded in the hidden
layer to endow the networks model with the memory capability
and dynamic features.
Two application studies are done in this paper, including the
chaotic time series prediction and RUL prediction under the task
of electronic prognostics. In the first application study, the average RMSE and NMSE values under CRQDNN are generally 5% ∼10%
lower than the values using traditional real QNN models. This identifies that the former has taken advantage of deep quantum entanglement created by the complex rotation method. The second
application study confirms the prediction accuracy of CRQDNN.
The experimental results demonstrate an outstanding prediction
accuracy of the proposed CRQDNN model compared with traditional real QNN models. At the same time, the RUL prediction effectiveness using the original data can be compared to the cases
using the denoised data, which shows that the CRQDNN model has
a higher capability to deal with noise.
Acknowledgments

For the hidden layer, the increment of w (hidden) is calculated by
(hidden)

∆wk,j

=

∂vk(2) [t ]

)
∂vk [t ] ∂wk(hidden
,j


n3

∂ e[t ] ∂ y(l 3) [t ] ∂vk(2) [t ]
− (3)
=
)
∂ yl [t ] ∂vk(2) [t ] ∂wk(hidden
l =1
,j
n3




)
wl(,output
· f u(j IIR) [t ]
= g vk(2) [t ]
k

(2)

l =1


(k = 1, . . . , n2 )
(27)
= δk(2) [t ] · f u(j IIR) [t ]


n3
(output )
(2)
(2)
where δk [t ] = g vk [t ]
, and g (·) is the first orl=1 wl,k


der derivation of the activation function given by g (x) =

(

e−x
2
1+e−x

1−f (x)
.
f (x)2

=

)

d
f
dx

(x) =

The increment of the IIR filter parameter vector a is calculated

by

∆ aT =

(IIR)
n1

∂ e[t ] ∂ uj [t ]
(IIR)
[ t ] ∂ aT
j = 1 ∂ uj

  ∂ e[t ] ∂v (2) [t ] ∂ u(j IIR) [t ]
k
(2)
(IIR)
[ t ] ∂ aT
j=1 k=1 ∂vk [t ] ∂ uj


n
+
1
n
+
1


1
2
(IIR)
(2)
(hidden)
=
g uj [ t ]
δk [t ]wk,j
· u(j 1) [t − T + 1]
n1 +1 n2

=

j =1

k=1

n 1 +1

=



δj(IIR) [t ] · u(j 2) [t − T + 1] (T = 1, . . . , N + 1)

(28)

j =1

(IIR)

where δj



n2
(2)
(hidden)
[t ] = g u(j IIR) [t ]
.
k=1 δk [t ]wk,j

For the quantum layer, the increment of the rotation angles θ is
given by

∆θj,h =

∂ e [t ]
∂αj [t − T + 1]
∂α
[
t
−
T
+
1
]
∂θj,h
j
T =1

∂ e [t ]
∂ρj [t − T + 1]
+
∂ρj [t − T + 1]
∂θj,h

N +1 


(j = 1, . . . , n0 , h = 1, . . . , n).

(29)

We divide the calculation of ∆θj,h into two steps. First, we
∂ e[t ]
∂ e[t ]
calculate ∂α
and ∂ρ
, which are given by
[t ]
[t ]
j 0

We express sincere appreciation to the editor and reviewers for
their efforts to improve this paper. We are very thankful for the
help from No. 10 Research Institute, Electronics Technology Group

∂ e[t ]

j 0

(IIR)
(1)
∂ e [t ]
∂ e[t ] ∂ u2j−1 [t ] ∂ u2j−1 [t0 ]
=
∂αj [t0 ]
∂ u(2jIIR−)1 [t ] ∂ u(2j1−) 1 [t0 ] ∂αj [t0 ]

24

Y. Cui et al. / Neural Networks 71 (2015) 11–26

=

(1)
(IIR) ∂ u2j−1 [t0 ]
aT δ2j−1

∂αj [t0 ]


n

π

= −aT δ2j(IIR−)1 · 2 sin
f (βj ) − αj [t0 ]
2

π
f (βj ) − αj [t0 ]
× cos

 h=1
Γ¯ j [t0 ] = Re 




=



· sin π f (βj ) − 2αj [t0 ]

(30)

and
(IIR)
(1)
∂ e[t ] ∂ u2j [t ] ∂ u2j [t0 ]
∂ e[t ]
=
∂ρj [t0 ]
∂ u(2jIIR) [t ] ∂ u(2j1) [t0 ] ∂ρj [t0 ]

= aT δ2j(IIR)

∂ u(2j1) [t0 ]
∂ρj [t0 ]

(IIR)



= −aT δ2j(IIR) · sin π f (σj ) − 2ρj [t0 ] .

(31)

bj [t0 ] = Im

exp i ψh [t ] + φj,h

 

n







· sin ϕh [t ] + θj,h

(32)

exp i ψh [t ] + φj,h

 




· sin ϕh [t ] + θj,h




cj [t0 ] = Re

n


exp iφj,h








· cos ϕh [t ] + θj,h

(34)

h=1


dj [t0 ] = Im

n



exp iφj,h · cos ϕh [t ] + θj,h









.

(35)

h=1

The partial derivative terms are given as follows:

According to the division rule of complex numbers,

(36)
(37)
(38)
(39)


n
 h=1
Γ j [t0 ] = Im 


is

(40)

where

Ωj [t0 ] =

[t0 ] +

d2j

[t0 ]

¯ j [t0 ] = aj [t0 ]cj [t0 ] + bj [t0 ]dj [t0 ]
Φ

Φ j [t0 ]

(47)



· sin ϕh [t ] + θj,h


n






exp iφj,h · cos ϕi [t ] + θj,h

exp i ψh [t ] + φj,h

 



h=1

(48)

Ωj [t0 ]

∂ bj [t0 ]
∂ cj [t0 ]
P j [t0 ] = Ωj [t0 ] cj [t0 ]
+ bj [t0 ]
∂θj,h
∂θ
j,h
∂ aj [t0 ]
∂ dj [t0 ]
+ dj [t0 ]
− aj [t0 ]
∂θj,h
∂θj,h


∂ dj [t0 ]
∂ cj [t0 ]
+ dj [t0 ]
.
Q j [t0 ] = 2Φ j [t0 ] cj [t0 ]
∂θj,h
∂θj,h

∆φj,h =

(49)

(50)

∂ e[t ]
∂αj [t − T + 1]
∂αj [t − T + 1]
∂φj,h
T =1

∂ e[t ]
∂ρj [t − T + 1]
+
∂ρj [t − T + 1]
∂φj,h
N +1 


(j = 1, . . . , n0 , h = 1, . . . , n).

(41)
(42)

(51)

Similar with ∆θj,h , we divide the calculation of ∆φj,h into
∂ e[t ]
∂ e[t ]
two steps. First, we calculate ∂α
and ∂ρ
, which are given
[t ]
[t ]
j 0

∂αj [t0 ]
∂θj,h

calculated by

∂αj [t0 ]
1
P¯ j,i [t0 ] − Q¯ j,i [t0 ]
=
·
∂θj,i
Ωj2 [t0 ]
1 + Γ¯ j2 [t0 ]

(46)

The increment of the rotation angles f is given by


 



∂ aj [t0 ]
= Re exp i ψh [t ] + φj,h · cos ϕh [t ] + θj,h
∂θj,h

 



∂ bj [t0 ]
= Im exp i ψh [t ] + φj,h · cos ϕh [t ] + θj,h
∂θj,h

 



∂ cj [t0 ]
= Re − exp i ψh [t ] + φj,h · sin ϕh [t ] + θj,h
∂θj,h

 



∂ dj [t0 ]
= Im − exp i ψh [t ] + φj,h · sin ϕh [t ] + θj,h .
∂θj,h

cj2

(45)

Φ j [t0 ] = bj [t0 ]cj [t0 ] − aj [t0 ]dj [t0 ]

=

(33)

h=1



(44)

P j,i [t0 ] − Q j,i [t0 ]
∂ρj [t0 ]
1
·
=
2
∂θj,i
1 + Γ j [t0 ]
Ωj2 [t0 ]

h=1



(43)

where

∂α [t ]

Next, we calculate ∂θj 0 by utilizing Eq. (14). First, we define
j,h
some summation terms and partial derivative terms. The summation terms are given as follows:

aj [t0 ] = Re

¯ j [t0 ]
Φ
=
Ωj [t0 ]

∂ aj [t0 ]
∂ cj [t0 ]
+ cj [t0 ]
P¯ j [t0 ] = Ωj [t0 ] aj [t0 ]
∂θj,h
∂θj,h
∂ dj [t0 ]
∂ bj [t0 ]
+ bj [t 0 ]
+ dj [t0 ]
∂θj,h
∂θj,h


∂ dj [t0 ]
∂ cj [t0 ]
¯
¯
+ dj [t0 ]
.
Qj [t0 ] = 2Φj [t0 ] cj [t0 ]
∂θj,h
∂θj,h

At the same time, we calculate ∂θj 0 by utilizing Eq. (15), which
j,h
is given by



2

n




∂ρ [t ]

π

= −aT δ2j · 2 sin
f (σj ) − ρj [t0 ]
2

π
f (σj ) − ρj [t0 ]
× cos



 

h=1

2

−aT δ2j(IIR−)1



· sin ϕh [t ] + θj,h


n






exp iφj,h · cos ϕi [t ] + θj,h

exp i ψh [t ] + φj,h

j 0

∂α [t ]

∂ρ [t ]

in Eqs. (31) and (32). Next, we calculate ∂φj 0 and ∂φj 0 . Their
j,h
j,h
∂α [t ]

∂ρ [t ]

calculations are similar to the calculations of ∂θj 0 and ∂θj 0 .
j,h
j,h
What we need is to replace all θj,h by φj,h ; besides, the partial
derivative terms given by Eqs. (37)–(40) should be substituted with
the following:


 



∂ aj [t0 ]
= Re i exp i ψh [t ] + φj,h · sin ϕh [t ] + θj,h
∂φj,h

(52)


 



∂ bj [t0 ]
= Im i exp i ψh [t ] + φj,h · sin ϕh [t ] + θj,h
∂φj,h

(53)

Y. Cui et al. / Neural Networks 71 (2015) 11–26





 
∂ cj [t0 ]
= Re i exp i ψh [t ] + φj,h · cos ϕh [t ] + θj,h
∂φj,h

(54)


 



∂ dj [t0 ]
= Im i exp i ψh [t ] + φj,h · cos ϕh [t ] + θj,h .
∂φj,h

(55)

The increment of the quantum reversal vectors β and σ, is given
by

∆βj =

=

∂ e [t ]
∂βj
N +1


∂ u(2j1−) 1 [t − T + 1]

∂ e[t ]



∂βj
∂ u(2j1−) 1 [t − T + 1]
π

  (IIR)
=
aT δ2j−1 · 2 sin
f (βj ) − αj [t − T + 1]
2
T =1
π
 π

× cos
f (βj ) − αj [t − T + 1] · g (βj )
T =1
N +1

2

=
∆σj =

=

π
2

2

(IIR)
aT δ2j−1 g (βj )

·

N +1




sin π f (βj ) − 2αj [t − T + 1]

(56)

T =1

∂ e [t ]
∂σj
N +1


∂ e[t ]

∂ u(2j1) [t − T + 1]



∂σj
∂ u(2j1) [t − T + 1]

π
  (IIR)
f (σj ) − ρj [t − T + 1]
=
aT δ2j · 2 sin
2
T =1
π
 π

× cos
f (βj ) − ρj [t − T + 1] · g (σj )
T =1
N +1

2

=

π
2

(IIR)

aT δ2j g (σj ) ·

2

N +1


sin π f (σj ) − 2ρj [t − T + 1].





(57)

T =1

References
Abarbanel, H. (1996). Analysis of observed chaotic data. Springer.
Ardalani-Farsa, M., & Zolfaghari, S. (2010). Chaotic time series prediction
with residual analysis method using hybrid Elman–NARX neural networks.
Neurocomputing, 73, 2540–2553.
Ayoubi, M. (1994). Fault diagnosis with dynamic neural structure and application
to a turbo-charger. In International symposium on fault detection, supervision and
safety for technical processes (pp. 13-16). Espoo, Finland.
Back, A. D., & Tsoi, A. C. (1991). FIR and IIR synapses, a new neural network
architecture for time series modeling. Neural Computation, 3, 375–385.
Benioff, P. (1982). Quantum mechanical Hamiltonian models of turing machines.
Journal of Statistical Physics, 29, 515–546.
Chandra, R., & Zhang, M. (2012). Cooperative coevolution of Elman recurrent neural
networks for chaotic time series prediction. Neurocomputing, 86, 116–123.
Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179–211.
Ezhov, A. A., & Ventura, D. (2000). Quantum neural networks. In Future directions
for intelligent systems and information sciences (pp. 213–235). Springer.
Feynman, R. P. (1982). Simulating physics with computers. International journal of
theoretical physics, 21, 467–488.
Frasconi, P., Gori, M., & Soda, G. (1992). Local feedback multilayered networks.
Neural Computation, 4, 120–130.
Gholipour, A., Araabi, B. N., & Lucas, C. (2006). Predicting chaotic time series using
neural and neurofuzzy models: a comparative study. Neural Processing Letters,
24, 217–239.
Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with
bidirectional LSTM and other neural network architectures. Neural Networks,
18, 602–610.
Grover, L. K. (1997). Quantum mechanics helps in searching for a needle in a
haystack. Physical Review Letters, 79, 325.
Gu, J., Lau, D., & Pecht, M. (2009). Health assessment and prognostics of
electronic products. In: Proceedings of 8th international conference on reliability,
maintainability and safety (pp. 21–25).
Han, K.-H., & Kim, J.-H. (2002). Quantum-inspired evolutionary algorithm for a class
of combinatorial optimization. Evolutionary Computation, IEEE Transactions on,
6, 580–593.
Hopfield, J. J. (1982). Neural networks and physical systems with emergent
collective computational abilities. Proceedings of the National Academy of
Sciences, 79, 2554–2558.

25

Jozsa, R., & Linden, N. (2003). On the role of entanglement in quantumcomputational speed-up. Proceedings of the Royal Society of London. Series A:
Mathematical, Physical and Engineering Sciences, 459, 2011–2032.
Kabir, A., Bailey, C., Lu, H., & Stoyanov, S. (2012). A review of data-driven prognostics
in power electronics. In Electronics technology (ISSE), 2012 35th international
spring seminar on (pp. 189–192). IEEE.
Kak, S. C. (1995). Quantum neural computing. Advances in Imaging and Electron
Physics, 94, 259–313.
Kasabov, N. (2010). Integrative probabilistic evolving spiking neural networks utilising quantum inspired evolutionary algorithm: a computational framework.
In J. Koronacki, Z. Raś, S. Wierzchoń, & J. Kacprzyk (Eds.), Advances in Machine
Learning II: Vol. 263 (pp. 415–425). Berlin Heidelberg: Springer.
Kasabov, N., Dhoble, K., Nuntalid, N., & Indiveri, G. (2013). Dynamic evolving spiking
neural networks for on-line spatio- and spectro-temporal pattern recognition.
Neural Networks, 41, 188–201.
Kasabov, N.K. (2007). Evolving connectionist systems: the knowledge engineering
approach. Springer Science & Business Media.
Kim, J. H. (1997). Time-varying two-phase optimization neural network. Journal of
Intelligent and Fuzzy Systems, 5, 85–101.
Koskela, T., Lehtokangas, M., Saarinen, J., & Kaski, K. (1996). Time series prediction
with multilayer perceptron, FIR and Elman neural networks. In: Proceedings of
the world congress on neural networks (pp. 491–496).
Kouda, N., Matsui, N., & Nishimura, H. (2002). Image compression by layered
quantum neural networks. Neural Processing Letters, 16, 67–80.
Kouda, N., Matsui, N., Nishimura, H., & Peper, F. (2005). Qubit neural network and
its learning efficiency. Neural Computing & Applications, 14, 114–121.
Kuk-Hyun, H., & Jong-Hwan, K. (2004). Quantum-inspired evolutionary algorithms
with a new termination criterion, H<sub>&epsi;</sub>gate, and two-phase
scheme. Evolutionary Computation, IEEE Transactions on, 8, 156–169.
Lagaris, I. E., Likas, A., & Fotiadis, D. I. (1997). Artificial neural network methods in
quantum mechanics. Computer Physics Communications, 104, 1–14.
Lebedev, D. V., Steil, J. J., & Ritter, H. J. (2005). The dynamic wave expansion neural
network model for robot motion planning in time-varying environments.
Neural Networks, 18, 267–285.
Li, P., Li, Y., Xiong, Q., Chai, Y., & Zhang, Y. (2014). Application of a hybrid quantized
Elman neural network in short-term load forecasting. International Journal of
Electrical Power & Energy Systems, 55, 749–759.
Li, P., & Xiao, H. (2014). Model and algorithm of quantum-inspired neural network
with sequence input based on controlled rotation gates. Applied Intelligence, 40,
107–126.
Li, P., Xiao, H., Shang, F., Tong, X., Li, X., & Cao, M. (2013). A hybrid quantum-inspired
neural networks with sequence inputs. Neurocomputing, 117, 81–90.
Liu, C.-Y., Chen, C., Chang, C.-T., & Shih, L.-M. (2013). Single-hidden-layer feedforward quantum neural network based on Grover learning. Neural Networks,
45, 144–150.
Lou, X., & Cui, B. (2007). Synchronization of competitive neural networks with
different time scales. Physica A: Statistical Mechanics and its Applications, 380,
563–576.
Mandic, D., & Chambers, J. (2001). Recurrent neural networks for prediction:
Architectures, learning algorithms and stability. New York: Wiley.
Manju, A., & Nigam, M. J. (2014). Applications of quantum inspired computational
intelligence: a survey. Artificial Intelligence Review, 42, 79–156.
Marquardt, D. W. (1963). An algorithm for least-squares estimation of nonlinear
parameters. Journal of the Society for Industrial & Applied Mathematics, 11,
431–441.
Marra, S., & Morabito, F. C. (2006). Solar activity forecasting by incorporating prior
knowledge from nonlinear dynamics into neural networks. In Neural networks,
2006. international joint conference on (pp. 3722–3728). IEEE.
Matsui, N., Nishimura, H., & Peper, F. (2005). An examination of qubit neural
network in controlling an inverted pendulum. Neural Processing Letters, 22,
277–290.
Matsui, N., Takai, M., & Nishimura, H. (2000). A network model based on qubitlike
neuron corresponding to quantum circuit. Electronics and Communications in
Japan (Part III: Fundamental Electronic Science), 83, 67–73.
Menneer, T., & Narayanan, A. (1995). Quantum-inspired neural networks. UK.:
Department of Computer Sciences, University of Exeter.
Menneer, T. S. I. (1999). Quantum artificial neural networks. (Ph. D thesis). University
of Exeter.
Meyer-Bäse, A., Ohl, F., & Scheich, H. (1996). Singular perturbation analysis of
competitive neural networks with different time scales. Neural Computation,
8, 1731–1742.
Pecht, M., & Jaai, R. (2010). A prognostics and health management roadmap
for information and electronics-rich systems. Microelectronics Reliability, 50,
317–323.
Platel, M. D., Schliebs, S., & Kasabov, N. (2009). Quantum-inspired evolutionary
algorithm: a multimodel EDA. Evolutionary Computation. IEEE Transactions on,
13, 1218–1232.
Purushothaman, G., & Karayiannis, N. B. (1997). Quantum neural networks
(QNNs): inherently fuzzy feedforward neural networks. Neural Networks, IEEE
Transactions on, 8, 679–693.
Rigatos, G. G., & Tzafestas, S. G. (2002). Parallelization of a fuzzy control algorithm
using quantum computation. Fuzzy Systems, IEEE Transactions on, 10, 451–460.
Sarathi Vasan, A. S., Long, B., & Pecht, M. (2013). Diagnostics and prognostics
method for analog electronic circuits. Industrial Electronics, IEEE Transactions on,
60, 5277–5291.
Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal
Processing, IEEE Transactions on, 45, 2673–2681.

26

Y. Cui et al. / Neural Networks 71 (2015) 11–26

Seising, R. (2006). Can Fuzzy Sets Be Useful in the (Re) Interpretation of Uncertainty
in Quantum Mechanics?. In Fuzzy information processing society, 2006. Annual
meeting of the North American (pp. 414–419). IEEE.
Sello, S. (2001). Solar cycle forecasting: a nonlinear dynamics approach. Astronomy
and Astrophysics-Berlin-, 377, 312–320.
Shafee, F. (2007). Neural networks with quantum gated nodes. Engineering
Applications of Artificial Intelligence, 20, 429–437.
Shor, P. W. (1997). Polynomial-time algorithms for prime factorization and
discrete logarithms on a quantum computer. SIAM Journal on Computing, 26,
1484–1509.
SIDC. (2015). World Data Center for the Sunspot Index, Monthly Smoothed Sunspot
Data. http://sidc.oma.be.
Svitek, M. (2008). Wave probabilities and quantum entanglement. Neural Network
World, 18, 401–406.

Teo, K. K., Wang, L., & Lin, Z. (2001). Wavelet packet multi-layer perceptron for
chaotic time series prediction: effects of weight initialization. In Computational
science-ICCS 2001 (pp. 310–317). Springer.
Widiputra, H., Pears, R., & Kasabov, N. (2011). Multiple time-series prediction
through multiple time-series relationships profiling and clustered recurring
trends. In Advances in knowledge discovery and data mining (pp. 161–172).
Springer.
Yazdizadeh, A., & Khorasani, K. (2002). Adaptive time delay neural network
structures for nonlinear system identification. Neurocomputing, 47, 207–240.
Zak, M., & Williams, C. P. (1998). Quantum neural nets. International Journal of
Theoretical Physics, 37, 651–684.
Zhang, G., Eddy Patuwo, B., & Y Hu, M. (1998). Forecasting with artificial neural
networks: The state of the art. International Journal of Forecasting, 14, 35–62.

