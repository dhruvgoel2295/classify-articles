Image and Vision Computing 40 (2015) 65–74

Contents lists available at ScienceDirect

Image and Vision Computing
journal homepage: www.elsevier.com/locate/imavis

Developing a contactless palmprint authentication system by
introducing a novel ROI extraction method☆
Murat Aykut ⁎, Murat Ekinci
Computer Vision and Pattern Recognition Lab., Karadeniz Technical University, 61080 Trabzon, Turkey

a r t i c l e

i n f o

Article history:
Received 25 August 2014
Received in revised form 7 May 2015
Accepted 28 May 2015
Available online 24 June 2015
Keywords:
Contactless palmprint authentication system
Model-based ROI extraction
Palm direction estimation by nonlinear
regression
Contactless palm database
AAM-based palm segmentation

a b s t r a c t
In this paper, we propose a novel contactless palmprint authentication system where the system uses a CCD camera to capture the user's hand at a distance without any restrictions and touching the device. Furthermore, a novel
and high performance region of interest (ROI) extraction method which makes use of nonlinear regression and
palm model to extract the ROIs with high success is proposed. Comparative results indicate that the proposed ROI
extraction method gives superior performance as compared to the previously proposed point-based approaches.
To show the performance of the proposed system, a novel contactless database has also been created. This database includes images captured from the users who present their hands with various hand positions and orientations in cluttered backgrounds. Furthermore, experiments show that the proposed system has achieved a
recognition rate of 99.488% and equal error rate of 0.277% on the contactless database of 145 people containing
1752 hand images.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
The recent research attention focused on the improvement of
palmprint recognition systems can be ascribed to its commercialization
potentials in real-world biometric applications. As a result, most of the
end-users of this technology seek to deploy secure, accurate, hygienic
biometric identiﬁcation solutions. A contactless palmprint acquisition
device can be the ultimate solution to alleviate such hygiene concerns.
For this reason, the trend has been toward the contactless acquisition
devices and developing robust approaches for contactless hand-based
biometrics [1–4].
This paper presents a novel contactless palmprint recognition system with a novel region of interest (ROI) extraction method. The users
present their hands without touching any guidance peripheral. Furthermore, there is no limitation about the scene behind the users' hand.
1.1. Related works
In the literature, the ﬁrst contactless palmprint image acquisition
and recognition system was developed by Han et al. in 2007. This system [5] consists of a standard PC and an image capturing device that
captures two images for each hand by two cameras. One of them is a
NIR (near infrared) camera that captures NIR images with NIR lighting
☆ This paper has been recommended for acceptance by Jianjiang Feng.
⁎ Corresponding author. Tel.: +90 462 3773672.
E-mail addresses: murat_aykut@ktu.edu.tr (M. Aykut), ekinci@ktu.edu.tr (M. Ekinci).

http://dx.doi.org/10.1016/j.imavis.2015.05.002
0262-8856/© 2015 Elsevier B.V. All rights reserved.

to segment hand regions under unconstrained scenes by hardware.
The other one is a CMOS web camera used for capturing color hand
images. In this system, users have to open their hands and place them
toward the camera at a distance from 35 cm to 50 cm.
Michael et al. developed three different contactless palmprint recognition systems [6,3,7]. Their ﬁrst developed system [6] involves a web
camera, a ﬂuorescent light and an enclosure. The segmentation of the
hands was achieved by a simple algorithm (skin-color thresholding).
In Ref. [3] where a multi-modal palmprint and knuckle print system
was presented, the lighting source was replaced by a bulb emitted yellowish light. They also included a dimmer to adjust the brightness of
the light bulb. The third system [7] uses additionally two biometric
models: hand vein and hand geometry. To obtain the vein images the
system involves a second camera, infrared (IR) ﬁlter, a number of infrared LEDs, and a diffuser paper.
In the recent work [8], Sato et al. have been proposed to normalize
the deformation arising from the different hand postures, for reliable
palm image matching. Their contactless system consists of a diffraction
grating laser and a high-speed camera. The geometric correction of the
image has been achieved by the measured 3D points with the use of
diffraction grating laser and grid pattern.
Other recent studies also include the mobile contactless biometrics
[1] and the simultaneous use of 3D and 2D hand information in the
contactless systems [2]. Although the mobile contactless palmprint systems allow ease of use, their recognition performances are not satisfactory at present. On the contrary, the proposed system in Ref. [2] gives
superior recognition performance in unrestricted schemes as compared

66

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

to the mobile contactless systems, but the cost and computation complexity of the system are very high, and the 3D digitizer can be affected
by the ambient noise.
One of the important considerations in the contactless setting is how
the palm can be segmented from hands that have pose and orientation
variations within unrestricted background. Some of the recently proposed contactless palmprint systems [1,6] use monotone backgrounds
for palm capturing to eliminate the complex segmentation problems.
The works performed in Refs. [3,6], for instance, use skin color based
segmentation and variations. But, this approach gives poor segmentation results for the images that have skin-colored backgrounds.
Because the use of pixel-based techniques leads to errors and in
some scenarios they are not adequate for correct segmentation, Doublet
et al. [9] proposed to improve the skin color modeling approach with active shape model (ASM). In our earlier work [10], we have proposed to
use Active Appearance Model for palm segmentation within unrestricted backgrounds which is also used in this study. This method makes use
of a combined statistical model of shape and texture together to segment the palm images.
ROI extraction is one of the most important and inﬂuential steps for
the palmprint recognition/veriﬁcation which is not emphasized as required in the papers although it can directly affect the results. Almost
all the methods presented in the literature have extracted the ROIs
with point wise manner by using the valley points of the gaps between
the ﬁngers [11,12]. The major steps of the most common method proposed by Zhang et al. [11] are as follows: (i) Convert the grayscale
image to a binary image. (ii) Obtain the boundaries of the gaps between
the ﬁngers. (iii) Calculate the tangent of the upper and lower gaps,
which represents the y axis. (iv) Draw a line (x axis) passing through
the midpoint of two valley points which is perpendicular to these
points. (v) Extract a ﬁxed size region based on the coordinate system.
In Ref. [12], Connie et al. proposed to utilize from the gap between
middle and ring ﬁnger. They drew two lines passing through the valley
points of the gaps and assigned the two corner points of the square region as the midpoint of the index ﬁnger and little ﬁnger on these lines.
Similarly, Badrinath and Gupta [13] developed a new method inspired
by the works of Connie et al. and Zhang et al. This method uses the tangent of the two gaps (reference line) between the ﬁngers. And the two
corner points of the square region are assigned as the midpoints of the
ﬁngers on the lines drawn at speciﬁc angle with respect to the reference
line.
The researchers in Ref. [6] have introduced an enhancement to ROI
extraction which utilizes from the pixel neighborhoods. According to
this approach, a pixel can be considered as a valley point if some of
the neighboring points lie in the non-hand region while the most of
the neighbors are the points of the hand region. In the work presented
by Leng et al. [14], the regions of valleys were determined after some
morphological operations. And centroids of the regions of the candidate
valley points that satisfy all conditions (4, 8 and 16 point check) were
chosen as the ﬁnal valley points. In another paper [15], the valley points
of the gaps between ﬁngers were selected by the modiﬁed Harris Corner
Detection algorithm.
Unlike the others, Li et al. [16] presented a ROI alignment reﬁnement
method based on the principal lines to improve the veriﬁcation accuracy. They applied the iterative closest point method to estimate the
transformation parameters of the images. The transformation was
performed and the new aligned ROI was then produced.
In the literature, there are many types of palmprint recognition algorithms: Local feature-based [11], holistic-based [17], and hybrid [18].
Local feature-based approaches extract local features from palmprint
images and use a matcher to compare them [11,4,19]. Holistic-based approaches extract the features by regarding a palmprint image as a whole
image, or a high dimensional vector. Appearance-based methods also
called subspace methods [17,20] are involved in this group. In many
subspace methods, the feature extraction methods (such as PCA, LDA
or LPP) are combined with the spectral representations. The Gabor

feature based kernel Fisher Discriminant Analysis which gives superior
results than other feature extraction methods has been proposed in
our recent study [21]. Hybrid methods [18], which utilize from more
than one feature extraction approaches were proposed to improve the
recognition accuracies [22].
1.2. Contributions of this paper
We have endeavored to develop a novel contactless palmprint recognition system. Our contactless system consist of a CCD camera, a DC
auto iris lens, LED light sources, a standard PC, a touch screen and an
enclosure to acquire palmprint images, as shown in Fig. 1. The main
contributions are as follows:
• A novel contactless device for high quality palmprint image acquisition in real environment is developed. The trade-off between quality
and ﬂexibility (free hand placement in unrestricted environment) is
optimized for high recognition results.
• As a novelty, the usage of ﬁtted palm model and nonlinear regression
(Least Squares Support Vector Regression) to extract the region of interests (ROI) of the palms is proposed. This model-based approach
provides reliable ROI extraction that is robust to the small segmentation errors. Whereas point wise approaches are substantially affected
from these errors.
• One of the most important stages in developing a contactless
palmprint recognition system is to accurately segment freely positioned hands under unconstrained scenes. For that consideration, it
is proposed to use Active Appearance Model [23] method for palm
segmentation. In this study, the AAM method is conﬁgured speciﬁcally to achieve palm segmentation accurately for the contactless
palmprint authentication system.
• A novel database for contactless system is constituted by collecting
1752 images from 145 different subjects who present their hands in
various directions, positions under unrestricted environment.
2. Materials and methods
2.1. Contactless palm image acquisition system
In this work, our earlier restricted palmprint identity veriﬁcation
system [21] has been further developed to achieve an unrestricted
contactless palmprint veriﬁcation system. External view of the system
is shown in Fig. 1. This system involves a standard PC with Intel Core2
Quad CPU, 4GB RAM and 500GB HD to execute authentication program
written in C++ programming language.
The capturing device is a low-cost CCD camera with DC auto iris lens
which has 28 mm focal length, maximum aperture of f/2.8 and wide

Fig. 1. External view of our palm image acquisition system and a hand presented to the
system.

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

angle of vision for viewing hand region completely. Shutter speed, focal
aperture and focus adjustments of the camera have been made for the
best image quality. Although the images taken with a low f-number
gives focused hand images at a distance and out of focused background
(leads to better segmentation), it is not suitable for the contactless biometric systems. The reason is that the distance between the hand and
camera can be varied in different sessions which results in unfocused
palm images. Furthermore, faster shutter speed can provide sharpened
images for shaken hands, but it needs more illumination which removes
the details of the palm texture.
To reveal the details in the palm image and to reduce the negative effect of the ambient lighting variations, LED-based light sources were located at a speciﬁc location and with a certain angle (as shown in Fig. 1).
Furthermore, the intensity of the LED light was adjusted to an optimal
value. The bottom light source was located to remove the side effects
of the 3D posture variations, and the upper light source was located at
the right side of the enclosure with 45 degrees to make the wrinkles
and ridges visible.
The subjects interact with the system by using a touch screen. In this
way, the veriﬁcation errors caused by the erroneous usage of the system
are decreased. The camera and the light sources are controlled by an
electronic interface which communicates with PC via a serial port interface. The upper surface shape of the enclosure is formed as a trapezoid
hole due to the concordance with the hand shape. Furthermore, wooden plates on two sides of the enclosure were added for ensuring the
proper presentation of the hands to the system. Users' hands do not
touch these wooden plates. They are included to the system in order
to guide users to present their palms correctly with appropriate handcamera distances. Thus, the probable presentation errors which lead
to unfocused palmprint images can be prevented as much as possible.
The acquisition of the palm image is realized in such a way that the
user presents his/her hand about 20 cm above the CCD camera. There is
no other restriction to the users and to the background scenes.
2.2. Hand segmentation
In our study, the segmentation of the hand was realized by an advanced model based segmentation method named as AAM method.
The construction of the appearance model was done by using our earlier
proposed palm model with 25 landmark points [10].
2.2.1. Active appearance model (AAM)
The AAM method [23] consists of two stages; model construction
and model search. In both stages the shape vector, s, is represented by
the coordinates of the ‘ landmark points, s = (x1, y1,..., x‘, y‘)T. Then, a
shape variation, modeled based on PCA, is deﬁned by a linear combination of a mean shape, s, and n shape base vectors, Ps:
s ¼ s þ Ps bs

between parameter changes, δc, and pixel differences, δg, for facilitation
of the computation.
δc ¼ Aδg

2.2.2. Palm modeling
In the palm modeling, 25 anatomical landmark points (as shown in
Fig. 2), corresponding to the borders of the shape, are manually selected,
as in Ref. [10]. Ten of them describe the end points of the palm model
and can be located by sliding their positions forward or backward
along the border of the hand shape. This ﬂexibility makes the presentation of the whole ﬁngers or upper/lower palm arcs unnecessary
(presenting approximately 50% of the upper and lower palm arcs is
sufﬁcient).
The rest 15 landmark points are also used to model the gaps
between the ﬁngers, the upper and lower palm arcs. Each gap in the
model is represented by three points (valley, left and right corner
points).
2.2.3. Implementation of the AAM
In our studies, to implement the AAM method, we used the open
source AAM API library [24], coded in C++ programming language by
M. Stegmann. Use of this library, allows the users to change many
parameters of the improved version of the AAM method. The applied
parameter adjustments determined as optimum are listed below:
• In the model construction stage, the eigenvectors which are associated with the smallest eigenvalues were truncated (98% of the sum of
the eigenvalues were involved).
• The optimization of the model was realized by estimating the Jacobian
over the training set.
• The initialization of the model search was performed by scanning a
5 × 5 mesh on the test image for all model parameters. Likewise,
orientation and scale of the models were set to the optimal values
(ﬁve scalings (0.75, 0.85, 1.0, 1.15, 1.25) and ﬁve orientations (−20,
−10, 0, 10, 20)).
• The model was ﬁtted in a two-stage procedure. In the ﬁrst stage, the
optimum 15 models which have the minimum model reconstruction

ð1Þ

ð2Þ

where g is the synthesized shape free texture, g is the mean texture
vector, Pg is the eigenvector of texture and bg is the texture parameter
vector. Combined appearance model parameters are obtained by
concatenating weighted shape parameters and texture parameters.
In the model search, the texture error (E) between the model texture
(gm) and the image texture (gs) is evaluated with the Euclidean distance
and minimized by tuning the model and pose parameters. The optimization of the model is realized by assuming a linear relationship

ð3Þ

where the matrix A is estimated by the regression at the training stage.
For large models, regression has been replaced by estimating the
Jacobian, ∂(δg)/∂c, over the training set. This gives better results and
far less computation overhead. More details on the AAM method can
be found in Ref. [23].

where bs is the shape parameter vector that controls the synthesized
shape. After the mean shape is calculated, the training textures are
warped onto the mean shape, s, in order to obtain shape free textures.
Then, the texture model is given by:
g ¼ g þ P g bg

67

Fig. 2. Illustration of the proposed region of interest (ROI) extraction method.

68

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

errors were determined and saved. In the second stage, ﬁne grain
search was performed in these 15 models only. The ﬁne grain
search was achieved by pattern search approximation. The numbers of the iterations were selected as 15 and 25 for the ﬁrst and
second stages, respectively. The convergence level was set to
0.00001.
• Damping factors [25] were used to improve the ﬁtting performance
and determined as 1.0, 0.5, 0.25, 0.12, 1.25, and 1.5.

2.3. Region of interest extraction
After palm image has been segmented, palmprint region, called region of interest (ROI), is extracted for subsequent processes. This region
generally has a squared shape and involves sufﬁcient information to
represent the palm.
A novel regression-based approach is proposed for the extraction of
ROI which utilizes the hand shape model. Almost all the other approaches in the literature are pixel-based, and hence a possible inaccurate selection of the hand valley points will lead to vast erroneous
extraction of ROI. The model-based approach proposed in this study
prevents the weakness of the pixel-based approaches by using the
palm model as a whole in the regression. The major performance difference between the model and pixel-based approaches arises from the
accurate determination of the hand orientation.
Although the proposed approach is suitable for model-based
methods, it can be easily adapted to the traditional segmentation
methods by determining landmark points.
The four major steps in the extraction of the ROI are as follows:
Step 1. In our method, the orientation of each palm is characterized
by the slope (m) of a line (y = mx + n) which shows the main direction of the palm. As a ﬁrst step, a training set that contains hand images is constituted from different shaped hands. Then the slope of
each palm in this set is manually determined by a supervisor and enrolled into a palm slope database. These slopes are used to estimate
the orientation of each palm as a function of landmark points in the
following steps.
Step 2. The orientation of each palm is associated with the positions
of the landmark points which constitute the palm model, and
trained with a nonlinear regression method. Thus, the small errors
at the determination of the landmark points do not affect the palm
orientation. In our study, we have used Least Squares Support Vector
Regression (LS-SVR) [26] method with RBF kernel. This method is
selected due to the following reasons: low computational cost,
performance, and enabling non-linearity for complex structures.
The method is given by the following optimization problem:
minw;b;e J P ðw; eÞ ¼

1 T
γ XN 2
w wþ
e
i¼1 i
2
2

subject to yi ¼ wT φðsi Þ þ b þ ei ; i ¼ 1; :::;N

ð4Þ
ð5Þ

where yi and si are considered as the orientation of a palm (m), and a
vector formed by the positions of the landmark points (si =[P1x, P1y,
P2x, P2y,…,P25x, P25y], respectively. φð⋅Þ : ℝn →ℝnh is a mapping to a
high dimensional feature space. ek and b are the error and bias terms
in the formulation. N is the number of samples and γ represents the
error tolerance. This optimization problem means that the distances
between the separating hyperplane and the training samples are
precisely “1” unit (with the allowed error which is adjusted by the
squared loss function).
The kernel form of the above problem is obtained by constructing
the Lagrangian, eliminating w, e, and applying kernel trick. The

ﬁnal formula is represented by the linear Karush–Kuhn–Tucker
system:


0
1v

1tv
K þ γ −1 IN



b
α


¼

 
0
y

ð6Þ

where IN ∈ ℝN × N is an identity matrix, K is the kernel matrix, and
y = [y1; …; yN], 1v = [1; …; 1], α = [α1; …; αN].
Step 3. For the test sample, a palm direction (see Fig. 2) which is
searched for the ROI extraction is then estimated by using the
palm model at the regression. This is achieved by the following
formula in LS-SVR:
yðsÞ ¼

N
X
α i kðs; si Þ þ b

ð7Þ

i¼1

where k is the kernel function, and s is the test sample.
Step 4. The upper and lower lines are then obtained as parallel lines
to the palm direction and passing tangentially to the curves of the
upper and lower palm arcs (see Fig. 2). The width of the palm can
be determined by measuring the distance between these two lines.
A line which is perpendicular to the upper and lower lines and tangent to one of the gaps between the ﬁngers that does not intersect
with the other gaps at two points is deﬁned as a reference line.
This line can be visualized as the innermost gap between the ﬁngers.
The distance between the upper and lower lines, L, is then used to
extract the ROI of the palm, and is shown in Fig. 2. To form the ROI,
the upper, lower and reference lines have to be shifted toward the
center of palm by 0.07L, 0.07L and 0.02L, respectively. The resultant
square shaped ROI with edge length 0.86L is formed between the intersections of these lines. The shifting amount and the size of the ROI
are determined empirically with the aim of reducing the negative
effects caused by the various hand postures. This approach also ascertains to retain the maximum palmprint information. The position
of the lines and the extracted ROI are shown in Fig. 2.
Finally, since the ROI sizes can be varied due to the distance between
the hand and the camera, the extracted ROI images are resized to a
128 × 128 pixel resolution. Furthermore, the hands can be also in different sizes.
2.4. Palm feature extraction
In our earlier work [21], the Gabor-based kernel Fisher discriminant
analysis method for online palmprint veriﬁcation system has been
employed to control the entries of the PC laboratory of our department
since October 2009 [27] with a great success. In this paper, this method is
implemented to achieve contactless palmprint recognition/veriﬁcation
experiments. The method consists of two stages: Gabor wavelet feature
representation of the palmprint images, and kernel Fisher discriminants
for meaningful feature extraction (which has more discriminative
power).
2.4.1. Gabor wavelet feature representation
In Gabor wavelets, basis function is Gabor kernel which is Gaussian
modulated by a complex sinusoidal:
!
#
 "
1
1 x2
y2
þ
exp −
þ
2πjWx
2πσ x σ y
2 σ 2x σ 2y


g ðx; yÞ ¼

ð8Þ

where σx and σy are the horizontal and vertical standard deviations of
Gaussian envelope, and W is central frequency. Gabor wavelets can be
obtained with dilations and rotations of the Gabor kernel.

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

Because Gabor functions form a non-orthogonal basis set, ﬁltered
images has redundant information. Manjunath and Ma [28] developed
a strategy to reduce this redundancy. In our experiments, to reduce
the computational cost, this scheme was used with ﬁve different scales
and four orientations of Gabor wavelets. Parameters of the Gabor function were selected as in Ref. [21].
2.4.2. Kernel Fisher discriminants (KFD)
Fisher linear discriminants (FLD) aim at ﬁnding the best separation
hyperplane via maximizing the between-class variances (SB) and minimizing the within-class variances (SW). Nonlinear version of FLD, called
KFD, can be implemented in a similar manner by adding the kernel trick
[29]. Alternatively, Mika et al. [30] proposed to use convex quadratic
programming (optimization) problem for large number of samples:
2

min kξk þ CP ðα Þ

ð9Þ

α;b;ξ

subject to:
Kα þ 1b ¼ y þ ξ

ð10Þ

1Ti ξ ¼ 0; i ¼ 1; 2

ð11Þ

for α, ξ ∈ ℝ‘, and b, C ∈ ℝ, C ≥ 0. Besides, deﬁne 1 ∈ ℝ‘ as the vector of all
ones. Here, K is the kernel matrix, b is the bias term, C and P are regularization function and the regularization parameter. The term ‖ξ‖2 minimizes the variance of the error. The ﬁrst constraint pulls the outputs
of the each sample to its class and the second ensures that the average
outputs for each class is the label. Then, the KFD features for the test
sample x is computed by:
ðw Á ΦðxÞÞ ¼

‘
X
α i kðxi ; xÞ

ð12Þ

i¼1

where k() is the kernel function and ‘ is the number of training samples.
The KFD explained above can be easily extended to multi-class problem with the output coding techniques. In our experiments we used RBF
kernel and one-versus-all (1vsA) output coding techniques due to the
achieved superior performance compared to other approaches.
2.5. Matching and classiﬁcation
In this study, weighted Euclidean distance (WED) [31] has been selected for the similarity measurement which gives superior results than
other metrics (such as Euclidean, Manhattan and cosine). It is deﬁned as
follows:
dk ¼

2
d
X
ð f ðiÞ− f k ðiÞÞ
i¼1

ðsi Þ2

!1=2
ð13Þ

where f and fk are the feature vectors of the test sample and k-th training
sample, respectively; si is the standard deviation of the i-th feature; and
d denotes the feature length of the samples.
Nearest neighbor (NN) classiﬁer, which provides high performance
with less computation overhead, was used to perform the classiﬁcation
of the test patterns.

69

system. Another one is the most publicly used contactless palm DB: IIT
Delhi Palmprint Image Database version 1.0 (brieﬂy IITD database) [32].
3.1.1. KTU-Contactless Palm DB
In this work, a contactless palm database1 has been constituted
for the performance measurement of the proposed system. By the
novel palm image acquisition system a total of 1752 images of size
768 × 576 were collected for the palm database. These images belong
to the right hand of the 145 individuals, of whom 105 are male and
the rest are female. The minimum, maximum and average numbers of
images per palm are 6, 30 and 12, respectively.
To evaluate the performance of the proposed method more realistic,
and to simulate the various difﬁculties that can be encountered, the
hand images in the database were collected by performing various
scenarios. They are as follows: (1) The environment brightness was
changed by decreasing/increasing the light intensity in the room to reveal the invariability on the external brightness. Some hand image samples with the estimated palm model for this type of scenario are shown
in Fig. 3.i,k. (2) The users are asked to present their hands toward the
capturing device by changing the hand-camera distance to construct a
database with different shaped hand images. They are also asked to
present their hands in various pose and translations (strongly bending
their hands downward, changing the space between the ﬁngers, excessively moving their hands in the forward/backward, and the left/right
directions, different hand rotations to right and left sides). Some of the
hand images in our database and their segmentation results (palm
models) are also given in Fig. 3.a, c, e–g, i, k–l. (3) The objects that
have similar textures and colors such as faces, other hands and complex
textured objects were also added to the background to evaluate the robustness of the algorithms on the complex backgrounds. The sample
images and their palm models are shown in Fig. 3.b, d, j. In the meantime, we captured the images at different times during the day so as
to acquire natural differences on the external lighting conditions. Furthermore, the users did not remove the rings or other ornaments from
their ﬁngers while the hand images were taken (see Fig. 3.j). The KTUContactless Palm DB also includes hand images which have partly
viewed palms or occlusions to the palmprint regions.
3.1.2. IITD database
This publicly available contactless palm image database [32] consists
of the hand images collected from the students and staff at IIT Delhi,
India. The images in the database (with pixel resolutions of 800 × 600
pixels) have been acquired in the Biometrics Research Laboratory during January 2006 to July 2007 using a very simple contactless imaging
setup. This database divided into two sub-data sets: left and right
hand image data sets. The left hand image data set consists of 1393
palm images from 234 individuals, while the right hand image data
set consists of 1376 palm images from 233 individuals. The minimum
number of images for each hand is 5. The images are grabbed in a
semi-closed enclosure and so there is no background complexity as in
our database. Additionally, the images of this database are not taken
in different environment brightnesses, and the spaces between the
ﬁngers are not changed excessively. However, the hand images have
different pose variations arisen from the contact-free structure of the
system.
3.2. Experimental results and discussions

3. Experiments
3.1. Databases
The performance of the proposed approach is evaluated on two different contactless palmprint databases. One of them (KTU-Contactless
Palm DB) is a new and realistic database which is constituted by the images captured from the proposed contactless palmprint authentication

In the experiments, for the model construction in AAM, a training set
with 73 images was constituted. The images in the training set were selected due to the representation ability of the variations on the shape
and texture of the palm images. Furthermore, the number of the samples in the training set is determined according to the trade-off between
1

The details can be found in http://ceng2.ktu.edu.tr/~cvpr/contactlessPalmDB.htm.

70

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

Fig. 3. Segmentation results for twelve samples of KTU-Contactless Palm DB with different scenarios. There is a hand (a), a face (b) or a book (c) at the background; the hand is moved
excessively to the right (d)/forward (e)/backward (f), rotated to the left direction (g) or strongly bent downward (h); the ﬁngers are downwardly-inclined (i), spaces between them
are reduced (f) or increased (j); thumb ﬁnger is out of the scene (k); the ring is not removed from a ﬁnger (c); a light source is present on the scene (l).

the computational complexity arising from large data sets and incompatibility occurred from small data sets [25]. The ROI selection results
of the proposed method are given visually with the sample images in
a comparative manner. In addition to these results, recognition rate,
ROC graph and equal error rate (EER) values were computed to quantify
the performance of the recognition and veriﬁcation systems.
3.2.1. Experiments on KTU-Contactless Palm DB
In this section, the ROI images extracted by the proposed method
were analyzed in comparison with the images of a point-based method
[12] introduced by Connie et al. The most commonly used ROI extraction method [11], described by Zhang et al., was not taken into account
in the comparisons. Because, there is an uncertainty in the determination of the ROI size (the extracted ROIs have a ﬁxed size which are suitable for their palm images only), and the location of a ROI is determined
with the visible part of the palm (extracting the central part as ROI)
which can lead to misleading results. Therefore, another commonly
used method, which is similar to the Zhang et al.'s method, was selected
for the comparisons. In Ref. [12], the midpoints on the index and little
ﬁngers are used to locate the ROI of the palm. The midpoints are also estimated by using the gaps between the ﬁngers and the contour of the
hand shape. Henceforth, we called this technique as classical ROI. To obtain the ROIs with this approach, the gaps between the ﬁngers and the
contour of the hand shape were extracted from the ﬁtted palm model.
Generally, it has been observed from the visual comparisons that the
proposed method provides more accurate selection of the ROIs as compared to the classical method. Sample palmprint images extracted by
these methods are given in Fig. 4.
In Fig. 4, for each item, the images in the left and right are extracted
with the classical and proposed ROI extraction methods, respectively.

The samples which are linked with the arrows are obtained from the
same user with different hand postures. Although the selection of the
ROIs are translated and scaled with the use of the classical method,
the proposed method results are not affected with different postures.
Besides, the valley point near the thumb ﬁnger can be seen and the
ROI can be misplaced substantially in some images for the classical
ROI approach.
There are two main reasons for the general performance improvement obtained by the proposed ROI extraction approach. First of them
is the removal of the occurred errors due to the faulty detection of the
valley points of the gaps between the ﬁngers. These errors are the common problems for the point-based approaches which are very sensitive
to valley point selection. Incorrect valley point selection may cause such
methods to obtain rotated, translated and scaled ROI images. In the proposed method, orientation of the hand and ROI are evaluated with
regressing all landmark points in the training set, which are annotated
by a supervisor. Furthermore, the major changes occurred from small
errors can be tolerated by a regularization coefﬁcient in LS-SVR. Therefore, the results are not greatly affected by the errors originated from
faulty landmark point selection in our method. The second reason that
affects the performance negatively is the existence of the wrinkles in
the palmprint region which are caused by decreasing the distance between the thumb and the index ﬁnger. This negative effect is observed
in a more pronounced manner in classical ROI methods as compared
to the proposed method since the ROI is extracted more closely to the
thumb ﬁnger. Besides, in some hand shapes, the valley point between
the thumb and index ﬁnger appears on the ROIs extracted by the classical approach. In our approach, the position of the ROI (the distances to
the edges of the palm) is empirically determined to prevent such errors.
These problems are illustrated in Fig. 4.

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

71

Fig. 4. A visual comparison of the ROI images extracted from two different methods. It shows that there are some problems for the classical method: The images are translated, scaled
(a), and rotated (b); wrinkles of the thumb ﬁnger are seen (c), and the ROI is misplaced (d).

number is 675,072. The false acceptance rate (FAR) and false reject
rate (FRR) curves are estimated by correct and incorrect matchings
and illustrated for the proposed ROI extraction method in Fig. 5.
From the FAR-FRR graph, it can be seen that, FAR and FRR curves are
separated from each other sufﬁciently and EER of the unrestricted
contactless palmprint veriﬁcation system is considerably low (only
0.277%) with the proposed method. The classical ROI extraction method
leads to more than twice veriﬁcation errors.
The performance of a biometric system is mostly illustrated by the
ROC curve, which is frequently used in the assessment of genuine

FAR-FRR Graph for Proposed Method

100

Percentage

To draw attention on the importance of the ROI extraction techniques with quantitative results, the recognition and veriﬁcation experiments were performed in a comparative manner. In the experiments,
the ﬁrst four samples for each palm belonging to an individual are
selected for training set, and the rest are selected for the test set. The
results, extracted by using both ROI approaches, are summarized in
Table 1.
In the recognition experiments the proposed ROI extraction method
gives better results as compared to the classical ROI extraction method.
We have observed a performance improvement of 0.854% as compared
to the method given in Ref. [12] and we have achieved an overall recognition rate of 99.488%. The achieved 99.488% recognition rate means
that only 6 images were classiﬁed incorrectly when the palmprints
were extracted by using the proposed ROI method.
Veriﬁcation results of the system were obtained by matching each
test sample with all of the training samples. Correct and incorrect
matchings are deﬁned by whether two samples are from the same
palm or not, respectively. The total number of matchings is 679,760.
The correct matching number is 4688, and the incorrect matching

10

1
Table 1
Recognition and veriﬁcation results (%) of the Gabor-based KFD method on KTUContactless Palm DB by using the classical and the proposed methods.

Recognition rate
GAR (when FAR = 1E-4%)
EER

Classical ROI

Proposed ROI

98.634
92.726
0.621

99.488
97.056
0.277

FAR
FRR

0.1
0

50

100

150

200
Distance

250

300

350

400

Fig. 5. Logarithmic FAR-FRR graph for the proposed method on KTU-Contactless Palm DB.

72

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74
ROC Graph

Table 3
Recognition and veriﬁcation results (%) of the Gabor-based KFD method on IITD DB by
using the classical and the proposed methods.

100
98

GAR (%)

96

Data set

Criterion

Classical ROI

Proposed ROI

Left hand

Recognition rate
GAR (when FAR = 1E-3%)
EER
Recognition rate
GAR (when FAR = 1E-3%)
EER

98.842
85.112
0.145
98.523
91.581
0.267

99.421
92.708
0.033
99.409
95.900
0.149

94
92

Right hand

90
88
Classical ROI
Proposed ROI

86
0.0001

0.001

0.01

0.1
FAR (%)

1

10

100

Fig. 6. Comparative ROC curves for different ROI extraction methods on KTU-Contactless
Palm DB.

acceptance rate against false acceptance rate. The ROC curves for the
proposed ROI and classical ROI extraction methods are shown comparatively in Fig. 6.
It is clear from the ROC curves that our ROI extraction approach gives
superior GAR results than the point-based approach in all of the FAR
values. For the false acceptance rate of 1E-4%, 97.056% genuine acceptance rate is achieved with the proposed method, while with the use
of classical ROI method only 92.726% GAR can be achieved.
As another experiment, to evaluate the developed contactless
system's general performance and reveal the effect of the increase in
the number of the training samples on the recognition/veriﬁcation performance, we used the ﬁrst 2, 3, 4 and 5 samples for each subject as
training samples. The recognition rates and equal error rates of the system are listed in Table 2.
Table 2 shows that, for the increasing number of the training samples, the recognition rate (RR) and EER are logarithmically increased
and decreased, respectively. Besides, the performance of the ﬁve training samples per palm is higher than others, in which 99.708% RR and
0.117% EER are achieved. However, it is known from the practical experience that, the users are mostly unwilling to present their hands
repeatedly. Therefore, the use of a few number of training samples can
be preferred in a real life application. If the system was trained with
only two samples per palm, the results were obtained as: 97.606% RR
and 0.855% EER. In our studies given above, four training samples
scheme was preferred for the optimality and practicability. This scheme
also used in the performance comparisons with the literature.

than the classical method. While the recognition rates of the proposed
method are approximately 99.4% for both data sets, the recognition
rate of the classical method for the left and right hand data sets are
98.8% and 98.5%, respectively. This means that the proposed method
gives a higher recognition rate of 0.579% than the classical method in
the worst case.
Veriﬁcation results also support the superiority of the proposed
method. In the experiments, for the left and right hand data sets, the
correct matching numbers are 2073 and 2031; and the incorrect
matching numbers are 483,009 and 471,192, respectively. The obtained
EERs by the proposed method are much lower than the EERs of the classical method. The proposed methods reduce the errors by 75% as compared to the classical method for the left hand data set and by about
50% for the right hand data set. The ROC curves for the proposed ROI
and classical ROI extraction methods on IITD DB are shown comparatively in Fig. 7.
The ROC curves show that the proposed ROI extraction method gives
superior ROC curves than the classical point-based method. For the left
hand data set, with the false acceptance rate of 1E-3%, 95.900% genuine
acceptance rate is achieved by the proposed method, while with the use
of classical ROI method only 91.581% GAR can be achieved. This distinction can be more visible for the right hand data set (92.708% to
85.112%). All of the recognition/veriﬁcation results achieved on the
IITD DBs are consistent with the results achieved on the KTUContactless Palm DB: the proposed method gives higher GAR values
for the all FAR values in the ROC curves, it reduces the errors (EERs)
by 50% minimally, and it has 99.4% recognition rates for both databases.
The EER values of the proposed method are 0.277% and 0.149% for the
KTU and IITD DBs, respectively. And it gives 0.854% and 0.579% higher
recognition rates for the KTU and IITD DBs, respectively. Hereby, it is
conﬁrmed that the proposed method gives better recognition and
veriﬁcation results than the classical point-based approaches.
3.3. Comparisons

3.2.2. Experiments on IITD DB
The performance of the proposed ROI extraction method is also evaluated on the public IIT Delhi Palmprint Image Database version 1.0 [32]
through the recognition and veriﬁcation experiments. For the training
of AAM model, 73 sample hand images are selected. These samples
are also used for training the direction of the palm. In the experiments
three samples for each palm were selected for the training set, and the
rest were chosen for the test set due to the limitations on the samples
per individual. The veriﬁcation and recognition results are summarized
in Table 3 for the left and right hand data sets separately.
The recognition results of the left and right hand data sets are very
close to each other and better for the proposed ROI extraction method

Table 2
Recognition and veriﬁcation results (%) of the proposed method on KTU-Contactless Palm
DB, with increasing number of training samples.
No. of training samples per subject

2

3

4

5

Recognition rate (%)
EER (%)

97.606
0.855

98.709
0.487

99.488
0.277

99.708
0.117

In this section, a comparison on the recognition/veriﬁcation accuracies has been given for our contactless palmprint system and the other
recently proposed contactless palmprint systems. Quantitative performance results of the contactless recognition/veriﬁcation systems were
given in Table 4 with the number of images, a number of training samples per subjects and a number of individuals which the experiments
performed on. Although some of the contactless systems include other
biometrics or multi-spectral structures2 we only consider 2-D palmprint
parts of them. In the comparisons, the papers utilized CASIA palmprint
database were not taken into account despite they have lower performances (for example in Ref. [34] the EER is 0.9%). Because, CASIA database consists of images acquired from a peg-free but not a contact-free
device.
Most of the papers in the literature have usually presented either
veriﬁcation or recognition results of their systems. However, in our
work both experiments were performed. Hence, we have selected ﬁve

2
For example in Ref. [33] the 3-D and 2-D palmprint features, in Ref. [3] palmprint and
knuckle-print biometrics were combined.

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74
ROC Graph

a 100
98

GAR (%)

96
94
92
90
88
Classical ROI
Proposed ROI

86
0.001

0.01

0.1

1

10

100

FAR (%)
ROC Graph

b 100
98

GAR (%)

96
94
92
90
88
Classical ROI
Proposed ROI

86
0.001

0.01

0.1

1

10

100

FAR (%)

Fig. 7. Comparative ROC curves for different ROI extraction methods on IITD DB (a) for the
left hand data set and (b) for the right hand data set.

high quality studies for the veriﬁcation and three studies for the recognition comparisons, which were recently published.
From the comparisons in Table 4, it can be seen that our contactless
palm database is one of the biggest database in the literature. Most of
the contactless palm databases (ﬁve of eight analyzed databases) have
a smaller number of samples and individuals than our database has
(e.g. in Ref. [38] there are only 49 individuals and 490 samples). Furthermore, in Ref. [33] the database involves right and left hand images, and
so it provides a signiﬁcant advantage for the veriﬁcation of the
palmprints. From the database comparisons we can say that, the results
of the experiments in this paper are more reliable and realistic than the
others. Furthermore, the complexities of the images in the databases are
satisfactory from the point of representing the real-world conditions.
Some of the studies emphasized on the cluttered backgrounds (e.g. in
Refs. [3,38]) and the others emphasized on the pose, translation and
scale variations in semi-closed background (e.g. in Refs. [2,35]) to simulate the difﬁculties on the real-world conditions. Our database also
includes images which were collected with pose, translation and rotation variations on different cluttered backgrounds.
In the proposed work, Gabor wavelets based kernel Fisher discriminant analysis was performed to extract the palmprint features. Most of

73

the palmrint veriﬁcation studies such as Refs. [2,3,33,36] used the coding approaches for the same purpose. In Ref. [38], a novel robust decomposition method was suggested. Poinsot et al. [37] utilized the Gabor
ﬁlter and binarization to extract the features. Unlike the others, in
Refs. [4,35], SIFT-based local descriptors were used as the palmprint
features. For matching and classiﬁcation, Weighted Euclidean Distance
and Nearest Neighbor methods were used in our work, respectively.
On the other hand, all of the papers, mentioned above, used the Hamming distance for matching (in Refs. [4,35] also Euclidean distance
were used for SIFT feature matching). The classiﬁcations of the palms
in the recognition experiments were achieved by Support Vector
Machines in Ref. [38], Rank-Level Fusion in Ref. [36], and Nearest
Neighbor in Ref. [37].
The number of training samples per subject is an important criterion
for the subspace methods in which the features are extracted from these
training samples. However, for the other methods (e.g. coding-based
methods, SIFT-based methods), the training set/test set separation is
not essential. In these methods, since it does not inﬂuence the extracted
features directly, each sample can be matched with all the other samples in the data set, and this leads to greater number of matchings. In
other words, a sample is matched to all the remaining samples of that
subject repeatedly for all of the test samples to generate genuine
match scores. So, we refer to this approach as “leave-one-out (LOO)”.
The works in Refs. [2–4,35,36] used LOO, and cannot be compared
with the proposed method in this respect. Although, Kanhangad et al.
used competitive coding in Ref. [33], they separate the samples to training set/test set and used ﬁve samples for training set. The other works
[37,38] which are selected for recognition comparisons used three and
seven samples for training, respectively. When compared with the results of the proposed method for three training samples per subject it
can be seen that, Poinsot et al. [37] gives a lower performance.
Table 4 also shows that, the performance of the proposed contactless
palmprint authentication system is encouraging and superior to the
others (0.277% EER and 99.488% RR). The number of veriﬁcation errors
occurred in Ref. [3] is higher by a factor of seven as compared to the proposed system. Besides, the system which has the highest veriﬁcation
performance in the literature [35] gives 50% more errors as compared
to the proposed method. Furthermore, in some papers it is stated that
the matching was performed in a leave-one-out manner, and the images in their databases were not separated as training and test sets.
This selection scheme on the database can affect their results positively.
Similarly, the highest recognition rate given in the papers referenced
above is 0.568% lower than the proposed system. In other words, the
number of errors given in those papers is higher than the errors
occurred in this study at least by a factor of two for the recognition
experiments.
Another comparison can be made by discussing the results for the
IITD touchless palmprint database. In Ref. [35], 0.43% and 0.49% EER, in
Ref. [4] 0.20% and 0.21% EER were achieved for the left and right palm
data sets, respectively. However, with the proposed method superior
results can be achieved as highlighted by the EER results of 0.033%
and 0.149% for the left and right palm data sets. Also, the recognition
rate given in Ref. [36] (97.55%) for the right palm data set is lower
than the recognition rate of the proposed method for this data set
(99.41%).

Table 4
Comparison of the proposed and other contactless palmprint recognition/veriﬁcation systems.

EER (%)
RR (%)
No. of individuals
No. of samples
No. of training samples per subject

Proposed method

In Ref. [35]
2014

In Ref. [4]
2011

In Ref. [33]
2011

In Ref. [2]
2011

In Ref. [3]
2010

In Ref. [36]
2011

In Ref. [37]
2009

In Ref. [38]
2007

0.277
99.488
145
1752
4

0.43
–
235
1400
LOO

0.57
–
110
1540
LOO

1.22
–
177
3540
5

1.10
–
114
570
LOO

1.97
–
100
1000
LOO

–
98.92
233
1376
LOO

–
97.55
130
1170
3

–
98.5
49
490
7

74

M. Aykut, M. Ekinci / Image and Vision Computing 40 (2015) 65–74

Consequently, it has been revealed that the robustness of the evaluated system is very competitive and promising as a whole with all of its
parts including image acquisition device, segmentation, ROI extraction
and recognition. This observation is conﬁrmed by extensive evaluation
studies carried on an unrestricted contactless palmprint database.
4. Conclusion
This paper presents a novel contactless palmprint image capturing
and recognition system. In this system, the users were asked to introduce their hands above the CCD camera with various hand pose and orientations under unconstrained environment. Due to the presence of
these challenges where the pixel-based segmentation methods cannot
give acceptable results, we utilized AAM-based palm modeling to accurately segment the palms.
For the extraction of a central area from a palm image (called ROI), a
novel ROI extraction approach that utilizes the palm model and nonlinear regression has been proposed. Because it removes weaknesses of the
other approaches, a great deal of improvement has been achieved by the
usage of this method. The performance of the method was evaluated
with recognition rate, equal error rate, and ROC graph. The possible
reasons of the success are also illustrated with ROI image samples.
In our contactless palmprint database that contains 1752 images collected from 145 subjects, we have achieved high genuine acceptance
rate (97.056% GAR) at low false acceptance (0.0001% FAR) rate, 0.277%
EER, and 99.488% recognition rate, which are clearly encouraging for
the contactless palmprint system under unconstrained scenes. This
high performance is also supported by the results of the recognition/
veriﬁcation tests on the public IIT Delhi Palmprint Image Database. In future works, the proposed system should be transformed to an online
contactless palmprint identiﬁcation system and more users should be
included into the database to test its feasibility in large organizations.
Acknowledgment
The work described in this paper has been supported by National
Science Foundation under Project No. 107E212.
Portions of the work were tested on the IITD Touchless Palmprint
Database version 1.0.
The authors would like to thank the associative editor and the
anonymous reviewers for their valuable comments that resulted in an
improved manuscript, and Dr. Sedat Gormus from the Karadeniz Technical University, for his helpful discussions on the technical sections of
the paper.
References
[1] M. Choras, R. Kozik, Contactless palmprint and knuckle biometrics for mobile
devices, Pattern. Anal. Applic. 15 (2012) 73–85.
[2] V. Kanhangad, A. Kumar, D. Zhang, Contactless and pose invariant biometric identiﬁcation using hand surface, IEEE Trans. Image Process. 20 (5) (2011) 1415–1424.
[3] G.K.O. Michael, T. Connie, A.T.B. Jin, An innovative contactless palm print and knuckle print recognition system, Pattern Recogn. Lett. 31 (12) (2010) 1708–1719.
[4] M. Morales, A. Ferrer, A. Kumar, Towards contactless palmprint authentication, IET
Comput. Vis. 5 (6) (2011) 407–416.
[5] Y. Han, Z. Sun, F. Wang, T. Tan, Palmprint recognition under unconstrained scenes,
Proceedings of the 8th Asian conference on Computer vision - Volume Part II,
ACCV'07, Berlin, Heidelberg 2007, pp. 1–11.
[6] G. Michael, T. Connie, A. Teoh, Touch-less palmprint biometrics: novel design and
implementation, Image Vis. Comput. 26 (12) (2008) 1551–1560.
[7] G.K.O. Michael, T. Connie, A.B.J. Teoh, A contactless biometric system using multiple
hand features, J. Vis. Commun. Image Represent. 23 (7) (2012) 1068–1084.
[8] T. Sato, S. Aoyama, S. Sakai, S. Yusa, K. Ito, T. Aoki, A contactless palm recognition
system using simple active 3d measurement with diffraction grating laser, 2nd
IAPR Asian Conference on Pattern Recognition (ACPR) 2013, pp. 542–546.

[9] J. Doublet, O. Lepetit, M. Revenu, Contactless hand recognition using shape and
texture features, 8th International Conference on Signal Processing, Vol. 3 2006,
pp. 1–4.
[10] M. Aykut, M. Ekinci, Aam-based palm segmentation in unrestricted backgrounds
and various postures for palmprint recognition, Pattern Recogn. Lett. 34 (9)
(2013) 955–962.
[11] D. Zhang, W.-K. Kong, J. You, M. Wong, Online palmprint identiﬁcation, IEEE Trans.
Pattern Anal. Mach. Intell. 25 (9) (2003) 1041–1050.
[12] T. Connie, A.T.B. Jin, M.G.K. Ong, D.N.C. Ling, An automated palmprint recognition
system, Image Vis. Comput. 23 (5) (2005) 501–515.
[13] G. Badrinath, P. Gupta, Palmprint based recognition system using phase-difference
information, Futur. Gener. Comput. Syst. 28 (1) (2012) 287–305.
[14] L. Leng, G. Liu, M. Li, M.K. Khan, A.M. Al-Khouri, Logical conjunction of tripleperpendicular-directional translation residual for contactless palmprint preprocessing, Proceedings of the 11th International Conference on Information Technology:
New Generations, ITNG'14, IEEE Computer Society, Washington, DC, USA 2014,
pp. 523–528.
[15] L. Shang, J. Chen, P.-G. Su, Y. Zhou, Roi extraction of palmprint images using modiﬁed harris corner point detection algorithm, Proceedings of the 8th International
Conference on Intelligent Computing Theories and Applications, ICIC'12, Berlin,
Heidelberg 2012, pp. 479–486.
[16] W. Li, B. Zhang, L. Zhang, J. Yan, Principal line-based alignment reﬁnement for
palmprint recognition, IEEE Trans. Syst. Man Cybern. Part C Appl. Rev. 42 (6)
(2012) 1491–1499.
[17] M. Ekinci, M. Aykut, Gabor-based kernel pca for palmprint recognition, Electron.
Lett. 43 (20) (2007) 1077–1079.
[18] S. Zhang, X. Gu, Palmprint recognition method based on score level fusion, Optik Int. J. Light Electron. Optics 124 (18) (2013) 3340–3344.
[19] L. Zhang, H. Li, Encoding local image patterns using riesz transforms: with applications to palmprint and ﬁnger-knuckle-print recognition, Image Vis. Comput. 30
(12) (2012) 1043–1051.
[20] M. Aykut, M. Ekinci, An application of gabor based kernel Fisher discriminants to the
online palmprint veriﬁcation system, IEEE 19th Conference on Signal Processing and
Communications Applications (SIU) 2011, pp. 303–306.
[21] M. Ekinci, M. Aykut, Kernel Fisher discriminant analysis of gabor features for online
palmprint veriﬁcation, Turk. J. Electr. Eng. Comput. Sci. (2013) (in press).
[22] D. Zhang, W. Zuo, F. Yue, A comparative study of palmprint recognition algorithms,
ACM Comput. Surv. 44 (1) (2012) 2:1–2:37.
[23] T. Cootes, G. Edwards, C. Taylor, Active appearance models, IEEE Trans. Pattern Anal.
Mach. Intell. 23 (6) (2001) 681–685.
[24] The aam-api, http://www2.imm.dtu.dk/ aam/aamapi/, accessed: 2013–05-01.
[25] M. Stegmann, Active Appearance Models: Theory, Extensions and CasesMaster's
thesis Department of Mathematical Modelling, Technical University of Denmark,
Lyngby, Denmark, 2000.
[26] T. Van Gestel, J. Suykens, J. De Brabanter, B. De Moor, J. Vandewalle, Least squares
support vector machine regression for discriminant analysis, International Joint
Conference on Neural Networks - IJCNN'01, Vol. 4 2001, pp. 2445–2450.
[27] M. Ekinci, Automated personal recognition system based on the palmprint biometric feature, Tech. Rep. 107E212, The Scientiﬁc and Technological Research Council of
TurkeyAugust 2010.
[28] B. Manjunath, W. Ma, Texture features for browsing and retrieval of image data,
IEEE Trans. Pattern Anal. Mach. Intell. 18 (8) (1996) 837–842.
[29] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, K.R. Mullers, Fisher discriminant analysis
with kernels, Neural Networks for Signal Processing IX: Proceedings of the 1999
IEEE Signal Processing Society Workshop, IEEE 1999, pp. 41–48.
[30] S. Mika, G. Rätsch, K.-R. Müller, A Mathematical Programming Approach to the
Kernel Fisher Algorithm, NIPS 2000, pp. 591–597.
[31] Y. Zhu, T. Tan, Y. Wang, Biometric personal identiﬁcation based on handwriting,
15th International Conference on Pattern Recognition, Vol. 2 of ICPR'00 2000,
pp. 797–800.
[32] A. Kumar, Incorporating cohort information for reliable palmprint authentication,
Sixth Indian Conference on Computer Vision, Graphics Image Processing - ICVGIP'08
2008, pp. 583–590.
[33] V. Kanhangad, A. Kumar, D. Zhang, A uniﬁed framework for contactless hand
veriﬁcation, IEEE Trans. Inf. Forensics Secur. 6 (3-2) (2011) 1014–1027.
[34] J. Chen, Y.-S. Moon, M.-F. Wong, G. Su, Palmprint authentication using a symbolic
representation of images, Image Vis. Comput. 28 (3) (2010) 343–351.
[35] X. Wu, Q. Zhao, W. Bu, A sift-based contactless palmprint veriﬁcation approach
using iterative {RANSAC} and local palmprint descriptors, Pattern Recogn. 47 (10)
(2014) 3314–3326.
[36] A. Kumar, S. Shekhar, Personal identiﬁcation using multibiometrics rank-level
fusion, IEEE Trans. Syst. Man Cybern. Part C Appl. Rev. 41 (5) (2011) 743–752.
[37] A. Poinsot, F. Yang, M. Paindavoine, Small sample biometric recognition based on
palmprint and face fusion, Fourth International Multi-Conference on Computing in
the Global Information Technology - ICCGI'09 2009, pp. 118–122.
[38] J. Doublet, M. Revenu, O. Lepetit, Robust grayscale distribution estimation for
contactless palmprint recognition, First IEEE International Conference on Biometrics: Theory, Applications, and Systems - BTAS 2007 2007, pp. 1–6.

